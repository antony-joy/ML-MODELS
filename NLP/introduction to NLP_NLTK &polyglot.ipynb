{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Segment 5 - Introduction to NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"On Wednesday, the Association for Computing Machinery, the worldâ€™s largest society of computing professionals, announced that Hinton, LeCun and Bengio had won this yearâ€™s Turing Award for their work on neural networks. The Turing Award, which was introduced in 1966, is often called the Nobel Prize of computing, and it includes a $1 million prize, which the three scientists will share.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ANTHONY\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Sentence Tokenizer</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence tokenizing the text: \n",
      "\n",
      "['On Wednesday, the Association for Computing Machinery, the worldâ€™s largest society of computing professionals, announced that Hinton, LeCun and Bengio had won this yearâ€™s Turing Award for their work on neural networks.', 'The Turing Award, which was introduced in 1966, is often called the Nobel Prize of computing, and it includes a $1 million prize, which the three scientists will share.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "sent_tk = sent_tokenize(text)\n",
    "print(\"Sentence tokenizing the text: \\n\")\n",
    "print(sent_tk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word tokenizing the text: \n",
      "\n",
      "['On', 'Wednesday', ',', 'the', 'Association', 'for', 'Computing', 'Machinery', ',', 'the', 'world', 'â€™', 's', 'largest', 'society', 'of', 'computing', 'professionals', ',', 'announced', 'that', 'Hinton', ',', 'LeCun', 'and', 'Bengio', 'had', 'won', 'this', 'year', 'â€™', 's', 'Turing', 'Award', 'for', 'their', 'work', 'on', 'neural', 'networks', '.', 'The', 'Turing', 'Award', ',', 'which', 'was', 'introduced', 'in', '1966', ',', 'is', 'often', 'called', 'the', 'Nobel', 'Prize', 'of', 'computing', ',', 'and', 'it', 'includes', 'a', '$', '1', 'million', 'prize', ',', 'which', 'the', 'three', 'scientists', 'will', 'share', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "word_tk = word_tokenize(text)      # OR word_tokenize(\" hii  there or text\")\n",
    "print(\"Word tokenizing the text: \\n\")\n",
    "print(word_tk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### REGEX TOKENIZER\n",
    "The regexp_tokenize uses regular expressions to tokenize the string, giving you more granular control over the process. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['#nlp', '#python', '#NLP', '#learning', '@datacamp', '#nlp', '#python']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import regexp_tokenize\n",
    "tweets=\"'This is the best #nlp exercise ive found online! #python','#NLP is super fun! <3 #learning','Thanks @datacamp :) #nlp #python'\"\n",
    "# Define a regex pattern to find hashtags: pattern1\n",
    "pattern1 = r\"([@#]\\w+)\"\n",
    "# Use the pattern on the first tweet in the tweets list\n",
    "hashtags = regexp_tokenize(tweets, pattern1)\n",
    "hashtags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EMOJI TOKENIZING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Wann', 'gehen', 'wir', 'Pizza', 'essen', '?', 'ðŸ•', 'Und', 'fÃ¤hrst', 'du', 'mit', 'Ãœber', '?', 'ðŸš•']\n",
      "['Wann', 'Pizza', 'Und', 'Ãœber']\n",
      "['ðŸ•', 'ðŸš•']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import regexp_tokenize\n",
    "\n",
    "german_text=\"Wann gehen wir Pizza essen? ðŸ• Und fÃ¤hrst du mit Ãœber? ðŸš•\"\n",
    "# Tokenize and print all words in german_text\n",
    "all_words = word_tokenize(german_text)\n",
    "print(all_words)\n",
    "\n",
    "# Tokenize and print only capital words\n",
    "capital_words = r\"[A-ZÃœ]\\w+\"\n",
    "print(regexp_tokenize(german_text, capital_words))\n",
    "\n",
    "# Tokenize and print only emoji\n",
    "emoji = \"['\\U0001F300-\\U0001F5FF'|'\\U0001F600-\\U0001F64F'|'\\U0001F680-\\U0001F6FF'|'\\u2600-\\u26FF\\u2700-\\u27BF']\"\n",
    "print(regexp_tokenize(german_text, emoji))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TWEEET TOKENIZER\n",
    "And the tweettokenizer does neat things like recognize hashtags, mentions and when you have too many punctuation symbols following a sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'is', 'the', 'best', '#', 'nlp', 'exercise', 'ive', 'found', 'online', '!', '#', 'python', \"'\", ',', \"'\", '#', 'NLP', 'is', 'super', 'fun', '!', '<', '3', '#', 'learning', \"'\", ',', \"'Thanks\", '@', 'datacamp', ':', ')', '#', 'nlp', '#', 'python']\n",
      "[['This'], ['is'], ['the'], ['best'], ['#'], ['nlp'], ['exercise'], ['ive'], ['found'], ['online'], ['!'], ['#'], ['python'], [\"'\"], [','], [\"'\"], ['#'], ['NLP'], ['is'], ['super'], ['fun'], ['!'], ['<'], ['3'], ['#'], ['learning'], [\"'\"], [','], [\"'\", 'Thanks'], ['@'], ['datacamp'], [':'], [')'], ['#'], ['nlp'], ['#'], ['python']]\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "tweets=\"This is the best #nlp exercise ive found online! #python','#NLP is super fun! <3 #learning','Thanks @datacamp :) #nlp #python\"\n",
    "word_tokenized = word_tokenize(tweets)  \n",
    "print(word_tokenized)\n",
    "tknzr = TweetTokenizer()\n",
    "\n",
    "# Use the TweetTokenizer to tokenize all tweets into one list\n",
    "all_tokens = [tknzr.tokenize(t) for t in word_tokenized]\n",
    "print(all_tokens)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This',\n",
       " 'is',\n",
       " 'the',\n",
       " 'best',\n",
       " '#nlp',\n",
       " 'exercise',\n",
       " 'ive',\n",
       " 'found',\n",
       " 'online',\n",
       " '!',\n",
       " '#python',\n",
       " \"'\",\n",
       " ',',\n",
       " \"'\",\n",
       " '#NLP',\n",
       " 'is',\n",
       " 'super',\n",
       " 'fun',\n",
       " '!',\n",
       " '<3',\n",
       " '#learning',\n",
       " \"'\",\n",
       " ',',\n",
       " \"'\",\n",
       " 'Thanks',\n",
       " '@datacamp',\n",
       " ':)',\n",
       " '#nlp',\n",
       " '#python']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "######################################  OR #####################\n",
    "\n",
    "twee = tknzr.tokenize(tweets)\n",
    "twee"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#nlp\n",
      "#python\n",
      "#NLP\n",
      "#learning\n",
      "#nlp\n",
      "#python\n"
     ]
    }
   ],
   "source": [
    "for i in twee:\n",
    "    if i[0]=='#':\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FILTERING THE TOKENS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['@', 'VirginAmerica', 'it', \"'s\", 'really', 'aggressive', 'to', 'blast', 'obnoxious', \"'entertainment\", \"'\", 'in', 'your', 'guests', \"'\", 'faces', '&', 'amp', ';', 'they', 'have', 'little', 'recourse'], ['@', 'VirginAmerica', 'Hey', ',', 'first', 'time', 'flyer', 'next', 'week', '-', 'excited', '!', 'But', 'I', \"'m\", 'having', 'a', 'hard', 'time', 'getting', 'my', 'flights', 'added', 'to', 'my', 'Elevate', 'account', '.', 'Help', '?'], ['@', 'united', 'Change', 'made', 'in', 'just', 'over', '3', 'hours', '.', 'For', 'something', 'that', 'should', 'have', 'taken', 'seconds', 'online', ',', 'I', 'am', 'not', 'thrilled', '.', 'Loved', 'the', 'agent', ',', 'though', '.']]\n",
      "\n",
      "item in alphabetic list: \t [['VirginAmerica', 'it', 'really', 'aggressive', 'to', 'blast', 'obnoxious', 'in', 'your', 'guests', 'faces', 'amp', 'they', 'have', 'little', 'recourse'], ['VirginAmerica', 'Hey', 'first', 'time', 'flyer', 'next', 'week', 'excited', 'But', 'I', 'having', 'a', 'hard', 'time', 'getting', 'my', 'flights', 'added', 'to', 'my', 'Elevate', 'account', 'Help'], ['united', 'Change', 'made', 'in', 'just', 'over', 'hours', 'For', 'something', 'that', 'should', 'have', 'taken', 'seconds', 'online', 'I', 'am', 'not', 'thrilled', 'Loved', 'the', 'agent', 'though']]\n",
      "\n",
      " item in list of alphanumerics:\t  [['VirginAmerica', 'it', 'really', 'aggressive', 'to', 'blast', 'obnoxious', 'in', 'your', 'guests', 'faces', 'amp', 'they', 'have', 'little', 'recourse'], ['VirginAmerica', 'Hey', 'first', 'time', 'flyer', 'next', 'week', 'excited', 'But', 'I', 'having', 'a', 'hard', 'time', 'getting', 'my', 'flights', 'added', 'to', 'my', 'Elevate', 'account', 'Help'], ['united', 'Change', 'made', 'in', 'just', 'over', '3', 'hours', 'For', 'something', 'that', 'should', 'have', 'taken', 'seconds', 'online', 'I', 'am', 'not', 'thrilled', 'Loved', 'the', 'agent', 'though']]\n",
      "\n",
      "item in the list of digits:  [[], [], ['3']]\n"
     ]
    }
   ],
   "source": [
    "## tweets_list contain 3 string in a list\n",
    "tweets_list=[\"@VirginAmerica it's really aggressive to blast obnoxious 'entertainment' in your guests' faces &amp; they have little recourse\",\n",
    " \"@VirginAmerica Hey, first time flyer next week - excited! But I'm having a hard time getting my flights added to my Elevate account. Help?\",\n",
    " '@united Change made in just over 3 hours. For something that should have taken seconds online, I am not thrilled. Loved the agent, though.']\n",
    "\n",
    "# Create a list of lists, containing the tokens from list_tweets\n",
    "tokens = [word_tokenize(item) for item in tweets_list]\n",
    "\n",
    "print(tokens)\n",
    "\n",
    "# Remove characters and digits , i.e. retain only letters\n",
    "letters = [[word for word in item if word.isalpha()] for item in tokens]\n",
    "# Remove characters, i.e. retain only letters and digits\n",
    "let_digits = [[word for word in item if word.isalnum()] for item in tokens]\n",
    "# Remove letters and characters, retain only digits\n",
    "digits = [[word for word in item if word.isdigit()] for item in tokens]\n",
    "\n",
    "# Print the last item in each list\n",
    "print('\\nitem in alphabetic list: \\t', letters)\n",
    "print('\\n item in list of alphanumerics:\\t ', let_digits)\n",
    "print('\\nitem in the list of digits: ', digits)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'isalpha'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-32-43ed85d13125>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0malpha_only\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mt\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtokens\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misalpha\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0malpha_only\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# this will only work when word_tk is a list....ie word_tk is tokenized from a single string...\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m#  if more than that have to follow the above example....\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-32-43ed85d13125>\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0malpha_only\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mt\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtokens\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misalpha\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0malpha_only\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# this will only work when word_tk is a list....ie word_tk is tokenized from a single string...\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m#  if more than that have to follow the above example....\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'isalpha'"
     ]
    }
   ],
   "source": [
    "alpha_only = [t for t in tokens if t.isalpha()]\n",
    "alpha_only\n",
    "# this will only work when word_tk is a list....ie word_tk is tokenized from a single string...\n",
    "#  if more than that have to follow the above example...."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizing a panda df...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ANTHONY\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stop words in English language are: \n",
      "\n",
      "{'just', 'about', \"she's\", 'over', 'their', 'his', 'or', \"couldn't\", 'some', 'we', 'again', 'has', 'its', 'only', 'own', \"aren't\", 'mightn', 'where', 'how', 'when', 'here', 'out', 'more', 'shouldn', 'from', 'to', 'she', \"needn't\", 'been', \"don't\", 'hadn', \"mustn't\", 'did', 'with', 'through', \"you're\", \"you've\", 'those', 'him', \"wouldn't\", 'no', 'won', 'the', 'all', 'few', 'such', 'same', 'there', 'too', 'it', 'does', 'ourselves', 'that', 'them', \"shan't\", 'why', 'our', 'do', \"didn't\", 'than', 'they', 'after', 'ours', 'ain', \"you'd\", 'further', 'needn', 'of', 'which', 'hasn', 'as', 'up', 'is', 'down', 'isn', 's', 'himself', 'on', 'nor', 'be', 'can', 'while', 'couldn', 'so', 'then', 'was', 'you', 'are', 'below', \"weren't\", 'having', 'by', \"haven't\", 'themselves', 'd', \"shouldn't\", 'a', 'yourself', \"should've\", 'under', 'shan', 'have', 'he', 'once', 'weren', 'these', 'what', 'aren', 'at', 'each', 'any', 'mustn', 'her', \"hadn't\", 'itself', 'into', 'wasn', 'this', 'had', 'both', 'now', 't', 'didn', 'herself', 'but', 'll', 'between', 'y', 'during', \"won't\", 'don', 'before', \"mightn't\", 've', \"doesn't\", 'other', 'who', 'myself', 'i', \"wasn't\", 'and', \"that'll\", 'theirs', 'until', 'will', 'o', 'above', 'very', \"isn't\", 'doesn', 'for', 'because', 'off', 'an', 'yours', \"hasn't\", 'should', 'against', 'in', 'yourselves', 'most', 'my', 'am', 'haven', \"you'll\", 'hers', 'not', 'being', 'wouldn', 'whom', 'were', 're', \"it's\", 'm', 'ma', 'me', 'your', 'doing', 'if'}\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "sw = set(stopwords.words(\"english\"))\n",
    "print(\"Stop words in English language are: \\n\")\n",
    "print(sw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['On',\n",
       " 'Wednesday',\n",
       " ',',\n",
       " 'the',\n",
       " 'Association',\n",
       " 'for',\n",
       " 'Computing',\n",
       " 'Machinery',\n",
       " ',',\n",
       " 'the',\n",
       " 'world',\n",
       " 'â€™',\n",
       " 's',\n",
       " 'largest',\n",
       " 'society',\n",
       " 'of',\n",
       " 'computing',\n",
       " 'professionals',\n",
       " ',',\n",
       " 'announced',\n",
       " 'that',\n",
       " 'Hinton',\n",
       " ',',\n",
       " 'LeCun',\n",
       " 'and',\n",
       " 'Bengio',\n",
       " 'had',\n",
       " 'won',\n",
       " 'this',\n",
       " 'year',\n",
       " 'â€™',\n",
       " 's',\n",
       " 'Turing',\n",
       " 'Award',\n",
       " 'for',\n",
       " 'their',\n",
       " 'work',\n",
       " 'on',\n",
       " 'neural',\n",
       " 'networks',\n",
       " '.',\n",
       " 'The',\n",
       " 'Turing',\n",
       " 'Award',\n",
       " ',',\n",
       " 'which',\n",
       " 'was',\n",
       " 'introduced',\n",
       " 'in',\n",
       " '1966',\n",
       " ',',\n",
       " 'is',\n",
       " 'often',\n",
       " 'called',\n",
       " 'the',\n",
       " 'Nobel',\n",
       " 'Prize',\n",
       " 'of',\n",
       " 'computing',\n",
       " ',',\n",
       " 'and',\n",
       " 'it',\n",
       " 'includes',\n",
       " 'a',\n",
       " '$',\n",
       " '1',\n",
       " 'million',\n",
       " 'prize',\n",
       " ',',\n",
       " 'which',\n",
       " 'the',\n",
       " 'three',\n",
       " 'scientists',\n",
       " 'will',\n",
       " 'share',\n",
       " '.']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The text after removing stop words \n",
      "\n",
      "['On', 'Wednesday', ',', 'Association', 'Computing', 'Machinery', ',', 'world', 'â€™', 'largest', 'society', 'computing', 'professionals', ',', 'announced', 'Hinton', ',', 'LeCun', 'Bengio', 'year', 'â€™', 'Turing', 'Award', 'work', 'neural', 'networks', '.', 'The', 'Turing', 'Award', ',', 'introduced', '1966', ',', 'often', 'called', 'Nobel', 'Prize', 'computing', ',', 'includes', '$', '1', 'million', 'prize', ',', 'three', 'scientists', 'share', '.']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['On',\n",
       " 'Wednesday',\n",
       " ',',\n",
       " 'the',\n",
       " 'Association',\n",
       " 'for',\n",
       " 'Computing',\n",
       " 'Machinery',\n",
       " ',',\n",
       " 'the',\n",
       " 'world',\n",
       " 'â€™',\n",
       " 's',\n",
       " 'largest',\n",
       " 'society',\n",
       " 'of',\n",
       " 'computing',\n",
       " 'professionals',\n",
       " ',',\n",
       " 'announced',\n",
       " 'that',\n",
       " 'Hinton',\n",
       " ',',\n",
       " 'LeCun',\n",
       " 'and',\n",
       " 'Bengio',\n",
       " 'had',\n",
       " 'won',\n",
       " 'this',\n",
       " 'year',\n",
       " 'â€™',\n",
       " 's',\n",
       " 'Turing',\n",
       " 'Award',\n",
       " 'for',\n",
       " 'their',\n",
       " 'work',\n",
       " 'on',\n",
       " 'neural',\n",
       " 'networks',\n",
       " '.',\n",
       " 'The',\n",
       " 'Turing',\n",
       " 'Award',\n",
       " ',',\n",
       " 'which',\n",
       " 'was',\n",
       " 'introduced',\n",
       " 'in',\n",
       " '1966',\n",
       " ',',\n",
       " 'is',\n",
       " 'often',\n",
       " 'called',\n",
       " 'the',\n",
       " 'Nobel',\n",
       " 'Prize',\n",
       " 'of',\n",
       " 'computing',\n",
       " ',',\n",
       " 'and',\n",
       " 'it',\n",
       " 'includes',\n",
       " 'a',\n",
       " '$',\n",
       " '1',\n",
       " 'million',\n",
       " 'prize',\n",
       " ',',\n",
       " 'which',\n",
       " 'the',\n",
       " 'three',\n",
       " 'scientists',\n",
       " 'will',\n",
       " 'share',\n",
       " '.']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_words = [w for w in word_tk if not w in sw]\n",
    "\n",
    "print(\"The text after removing stop words \\n\")\n",
    "print(filtered_words)\n",
    "\n",
    "### but if we see here we are also having , . '  etc in the filtered text... so we will remove that too\n",
    "word_tk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['On', 'Wednesday', 'the', 'Association', 'for', 'Computing', 'Machinery', 'the', 'world', 's', 'largest', 'society', 'of', 'computing', 'professionals', 'announced', 'that', 'Hinton', 'LeCun', 'and', 'Bengio', 'had', 'won', 'this', 'year', 's', 'Turing', 'Award', 'for', 'their', 'work', 'on', 'neural', 'networks', 'The', 'Turing', 'Award', 'which', 'was', 'introduced', 'in', 'is', 'often', 'called', 'the', 'Nobel', 'Prize', 'of', 'computing', 'and', 'it', 'includes', 'a', 'million', 'prize', 'which', 'the', 'three', 'scientists', 'will', 'share']\n",
      "\n",
      "The text after removing stop words and unwanted punctuations\n",
      "\n",
      "['On', 'Wednesday', 'Association', 'Computing', 'Machinery', 'world', 'largest', 'society', 'computing', 'professionals', 'announced', 'Hinton', 'LeCun', 'Bengio', 'year', 'Turing', 'Award', 'work', 'neural', 'networks', 'The', 'Turing', 'Award', 'introduced', 'often', 'called', 'Nobel', 'Prize', 'computing', 'includes', 'million', 'prize', 'three', 'scientists', 'share']\n"
     ]
    }
   ],
   "source": [
    "# Retain alphabetic words: alpha_only\n",
    "alpha_only = [t for t in word_tk if t.isalpha()]\n",
    "\n",
    "print(alpha_only)\n",
    "\n",
    "again_filtered_words = [w for w in alpha_only if not w in sw]\n",
    "\n",
    "print(\"\\nThe text after removing stop words and unwanted punctuations\\n\")\n",
    "print(again_filtered_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "EOL while scanning string literal (<ipython-input-31-d15e4ea134cb>, line 6)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-31-d15e4ea134cb>\"\u001b[1;36m, line \u001b[1;32m6\u001b[0m\n\u001b[1;33m    alp.append(t)''''\u001b[0m\n\u001b[1;37m                     \n^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m EOL while scanning string literal\n"
     ]
    }
   ],
   "source": [
    "#or below can also be used\n",
    "''''\n",
    "alp=[]\n",
    "for t in word_tk:\n",
    "        if t.isalpha():\n",
    "              alp.append(t)''''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['@', 'VirginAmerica', 'it', \"'s\", 'really', 'aggressive', 'to', 'blast', 'obnoxious', \"'entertainment\", \"'\", 'in', 'your', 'guests', \"'\", 'faces', '&', 'amp', ';', 'they', 'have', 'little', 'recourse'], ['@', 'VirginAmerica', 'Hey', ',', 'first', 'time', 'flyer', 'next', 'week', '-', 'excited', '!', 'But', 'I', \"'m\", 'having', 'a', 'hard', 'time', 'getting', 'my', 'flights', 'added', 'to', 'my', 'Elevate', 'account', '.', 'Help', '?'], ['@', 'united', 'Change', 'made', 'in', 'just', 'over', '3', 'hours', '.', 'For', 'something', 'that', 'should', 'have', 'taken', 'seconds', 'online', ',', 'I', 'am', 'not', 'thrilled', '.', 'Loved', 'the', 'agent', ',', 'though', '.']]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[['VirginAmerica',\n",
       "  'really',\n",
       "  'aggressive',\n",
       "  'blast',\n",
       "  'obnoxious',\n",
       "  'guests',\n",
       "  'faces',\n",
       "  'amp',\n",
       "  'little',\n",
       "  'recourse'],\n",
       " ['VirginAmerica',\n",
       "  'Hey',\n",
       "  'first',\n",
       "  'time',\n",
       "  'flyer',\n",
       "  'next',\n",
       "  'week',\n",
       "  'excited',\n",
       "  'But',\n",
       "  'I',\n",
       "  'hard',\n",
       "  'time',\n",
       "  'getting',\n",
       "  'flights',\n",
       "  'added',\n",
       "  'Elevate',\n",
       "  'account',\n",
       "  'Help'],\n",
       " ['united',\n",
       "  'Change',\n",
       "  'made',\n",
       "  'hours',\n",
       "  'For',\n",
       "  'something',\n",
       "  'taken',\n",
       "  'seconds',\n",
       "  'online',\n",
       "  'I',\n",
       "  'thrilled',\n",
       "  'Loved',\n",
       "  'agent',\n",
       "  'though']]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "###############  REMOVING STOP WORDS FROM THIS LIST  ###########################\n",
    "\n",
    "tweets_list=[\"@VirginAmerica it's really aggressive to blast obnoxious 'entertainment' in your guests' faces &amp; they have little recourse\",\n",
    " \"@VirginAmerica Hey, first time flyer next week - excited! But I'm having a hard time getting my flights added to my Elevate account. Help?\",\n",
    " '@united Change made in just over 3 hours. For something that should have taken seconds online, I am not thrilled. Loved the agent, though.']\n",
    "\n",
    "# Create a list of lists, containing the tokens from list_tweets\n",
    "tokens = [word_tokenize(item) for item in tweets_list]\n",
    "\n",
    "print(tokens)\n",
    "\n",
    "# Remove characters and digits , i.e. retain only letters\n",
    "letters = [[word for word in item if word.isalpha()] for item in tokens]\n",
    "\n",
    "again_filtered_ = [[w for w in alpha_w if not w in sw] for alpha_w in letters]   ###########\n",
    "\n",
    "\n",
    "\n",
    "again_filtered_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Stemming</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "port_stem = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered Sentence(only aphanumeric): \n",
      " ['On', 'Wednesday', 'Association', 'Computing', 'Machinery', 'world', 'largest', 'society', 'computing', 'professionals', 'announced', 'Hinton', 'LeCun', 'Bengio', 'year', 'Turing', 'Award', 'work', 'neural', 'networks', 'The', 'Turing', 'Award', 'introduced', 'often', 'called', 'Nobel', 'Prize', 'computing', 'includes', 'million', 'prize', 'three', 'scientists', 'share'] \n",
      "\n",
      "Stemmed Sentence: \n",
      " ['On', 'wednesday', 'associ', 'comput', 'machineri', 'world', 'largest', 'societi', 'comput', 'profession', 'announc', 'hinton', 'lecun', 'bengio', 'year', 'ture', 'award', 'work', 'neural', 'network', 'the', 'ture', 'award', 'introduc', 'often', 'call', 'nobel', 'prize', 'comput', 'includ', 'million', 'prize', 'three', 'scientist', 'share']\n"
     ]
    }
   ],
   "source": [
    "stemmed_words = []\n",
    "\n",
    "for w in again_filtered_words:\n",
    "    stemmed_words.append(port_stem.stem(w))\n",
    "    \n",
    "print(\"Filtered Sentence(only aphanumeric): \\n\", again_filtered_words, \"\\n\")\n",
    "print(\"Stemmed Sentence: \\n\", stemmed_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Non English Stemming\n",
    "Stemming is possible using other languages as well, such as Danish, Dutch, French, Spanish, German, etc. To use foreign language stemmers we need to use the SnowballStemmer package. We can specify in the stemmer the foreign language we want to use. Then we apply the stem function on our string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stemmed spanish word is: com\n",
      "['on', 'wednesday', 'association', 'computing', 'machinery', 'world', 'largest', 'society', 'computing', 'professionals', 'announc', 'hinton', 'lecun', 'bengi', 'year', 'turing', 'award', 'work', 'neural', 'networks', 'the', 'turing', 'award', 'introduc', 'often', 'call', 'nobel', 'priz', 'computing', 'includ', 'million', 'priz', 'thre', 'scientists', 'shar']\n"
     ]
    }
   ],
   "source": [
    "# Import the required packages\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk import word_tokenize\n",
    "\n",
    "# Import the Spanish SnowballStemmer\n",
    "SpanishStemmer = SnowballStemmer(\"spanish\")\n",
    "\n",
    "spanish_stemmed_words = []\n",
    "\n",
    "print(\"stemmed spanish word is:\",SpanishStemmer.stem(\"comiendo\"))\n",
    "\n",
    "for w in again_filtered_words:\n",
    "    spanish_stemmed_words.append(SpanishStemmer.stem(w))\n",
    "\n",
    "# Print the first item of the stemmed tokenss\n",
    "print(spanish_stemmed_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lemmatizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\ANTHONY\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stemmed Sentence: \n",
      " ['On', 'wednesday', 'associ', 'comput', 'machineri', 'world', 'largest', 'societi', 'comput', 'profession', 'announc', 'hinton', 'lecun', 'bengio', 'year', 'ture', 'award', 'work', 'neural', 'network', 'the', 'ture', 'award', 'introduc', 'often', 'call', 'nobel', 'prize', 'comput', 'includ', 'million', 'prize', 'three', 'scientist', 'share']\n",
      "Lemmatized sentence:\n",
      " ['On', 'Wednesday', 'Association', 'Computing', 'Machinery', 'world', 'largest', 'society', 'computing', 'professional', 'announced', 'Hinton', 'LeCun', 'Bengio', 'year', 'Turing', 'Award', 'work', 'neural', 'network', 'The', 'Turing', 'Award', 'introduced', 'often', 'called', 'Nobel', 'Prize', 'computing', 'includes', 'million', 'prize', 'three', 'scientist', 'share']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "lem = WordNetLemmatizer()\n",
    "\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "stem = PorterStemmer()\n",
    "\n",
    "lemm_words = []\n",
    "\n",
    "for i in range(len(again_filtered_words)):\n",
    "    lemm_words.append(lem.lemmatize(again_filtered_words[i]))\n",
    "print(\"Stemmed Sentence: \\n\", stemmed_words)    \n",
    "print(\"Lemmatized sentence:\\n\",lemm_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['On',\n",
       " 'wednesday',\n",
       " 'associ',\n",
       " 'comput',\n",
       " 'machineri',\n",
       " 'world',\n",
       " 'largest',\n",
       " 'societi',\n",
       " 'comput',\n",
       " 'profession',\n",
       " 'announc',\n",
       " 'hinton',\n",
       " 'lecun',\n",
       " 'bengio',\n",
       " 'year',\n",
       " 'ture',\n",
       " 'award',\n",
       " 'work',\n",
       " 'neural',\n",
       " 'network',\n",
       " 'the',\n",
       " 'ture',\n",
       " 'award',\n",
       " 'introduc',\n",
       " 'often',\n",
       " 'call',\n",
       " 'nobel',\n",
       " 'prize',\n",
       " 'comput',\n",
       " 'includ',\n",
       " 'million',\n",
       " 'prize',\n",
       " 'three',\n",
       " 'scientist',\n",
       " 'share']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmm=[port_stem.stem(i) for i in again_filtered_words]\n",
    "stemmm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your task is to clean this text into a more machine friendly format. This will involve converting to lowercase, lemmatization and removing stopwords, punctuations and non-alphabetic characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "twenty first century politic have witness an alarm rise of populism in the and Europe the first warning sign come with the UK Brexit Referendum vote in swinge in the way of Leave this be follow by a stupendous victory by billionaire Donald Trump to become the President of the United States in November since then Europe have see a steady rise in populist and far right party that have capitalize on Europe Immigration Crisis to raise nationalist and anti europe sentiment some instance include Alternative for Germany AfD win of all seat and enter the Bundestag thus upset Germany political order for the first time since the Second World War the success of the five Star Movement in Italy and the surge in popularity of neo nazism and neo fascism in country such as Hungary Czech Republic Poland and Austria\n"
     ]
    }
   ],
   "source": [
    "blog=\"'\\nTwenty-first-century politics has witnessed an alarming rise of populism in the U.S. and Europe. The first warning signs came with the UK Brexit Referendum vote in 2016 swinging in the way of Leave. This was followed by a stupendous victory by billionaire Donald Trump to become the 45th President of the United States in November 2016. Since then, Europe has seen a steady rise in populist and far-right parties that have capitalized on Europeâ€™s Immigration Crisis to raise nationalist and anti-Europe sentiments. Some instances include Alternative for Germany (AfD) winning 12.6% of all seats and entering the Bundestag, thus upsetting Germanyâ€™s political order for the first time since the Second World War, the success of the Five Star Movement in Italy and the surge in popularity of neo-nazism and neo-fascism in countries such as Hungary, Czech Republic, Poland and Austria.\\n'\"\n",
    "# Load model and create Doc object\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "doc = nlp(blog)\n",
    "\n",
    "# Generate lemmatized tokens\n",
    "lemmas = [token.lemma_ for token in doc]\n",
    "\n",
    "# Remove stopwords and non-alphabetic tokens\n",
    "a_lemmas = [lemma for lemma in lemmas if lemma.isalpha()]\n",
    "\n",
    "# Print string after text cleaning\n",
    "print(' '.join(a_lemmas))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Parts of Speech Tagging</h3>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\ANTHONY\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('On', 'IN'), ('Wednesday', 'NNP'), (',', ','), ('the', 'DT'), ('Association', 'NNP'), ('for', 'IN'), ('Computing', 'VBG'), ('Machinery', 'NNP'), (',', ','), ('the', 'DT'), ('world', 'NN'), ('â€™', 'NNP'), ('s', 'RB'), ('largest', 'JJS'), ('society', 'NN'), ('of', 'IN'), ('computing', 'VBG'), ('professionals', 'NNS'), (',', ','), ('announced', 'VBD'), ('that', 'IN'), ('Hinton', 'NNP'), (',', ','), ('LeCun', 'NNP'), ('and', 'CC'), ('Bengio', 'NNP'), ('had', 'VBD'), ('won', 'VBN'), ('this', 'DT'), ('year', 'NN'), ('â€™', 'VBZ'), ('s', 'JJ'), ('Turing', 'NNP'), ('Award', 'NNP'), ('for', 'IN'), ('their', 'PRP$'), ('work', 'NN'), ('on', 'IN'), ('neural', 'JJ'), ('networks', 'NNS'), ('.', '.'), ('The', 'DT'), ('Turing', 'NNP'), ('Award', 'NNP'), (',', ','), ('which', 'WDT'), ('was', 'VBD'), ('introduced', 'VBN'), ('in', 'IN'), ('1966', 'CD'), (',', ','), ('is', 'VBZ'), ('often', 'RB'), ('called', 'VBN'), ('the', 'DT'), ('Nobel', 'NNP'), ('Prize', 'NNP'), ('of', 'IN'), ('computing', 'NN'), (',', ','), ('and', 'CC'), ('it', 'PRP'), ('includes', 'VBZ'), ('a', 'DT'), ('$', '$'), ('1', 'CD'), ('million', 'CD'), ('prize', 'NN'), (',', ','), ('which', 'WDT'), ('the', 'DT'), ('three', 'CD'), ('scientists', 'NNS'), ('will', 'MD'), ('share', 'NN'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "from nltk import pos_tag\n",
    "pos_tagged_words = pos_tag(word_tk)\n",
    "\n",
    "print(pos_tagged_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Frequency Distribution Plots</h3>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<FreqDist with 56 samples and 76 outcomes>\n"
     ]
    }
   ],
   "source": [
    "from nltk.probability import FreqDist\n",
    "fd = FreqDist(word_tk)\n",
    "print(fd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "fd.plot(30, cumulative=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for getting distribution of alphabets..............."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'On Wednesday, the Association for Computing Machinery, the worldâ€™s largest society of computing professionals, announced that Hinton, LeCun and Bengio had won this yearâ€™s Turing Award for their work on neural networks. The Turing Award, which was introduced in 1966, is often called the Nobel Prize of computing, and it includes a $1 million prize, which the three scientists will share.'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<FreqDist with 41 samples and 387 outcomes>\n"
     ]
    }
   ],
   "source": [
    "fd_alpha = FreqDist(text)    ######## we will just use the sentence\n",
    "print(fd_alpha)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX4AAAEGCAYAAABiq/5QAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8vihELAAAACXBIWXMAAAsTAAALEwEAmpwYAAAp2UlEQVR4nO3deXxU5dn/8c+VjRAIS4AsBQQVZDFQNUGl7huFqj9t61JbrW31oau19WmrfWpr25d9ajdbn9Zat1rq1lpbi6igFBdEBU0QJRAQRFEwC2ENhBAC1++PcwJjCMkQMpmQ832/Xuc1c87cc51rBnLNPfeccx9zd0REJDpSkp2AiIh0LhV+EZGIUeEXEYkYFX4RkYhR4RcRiRgVfhGRiElLdgLxGDhwoA8fPrxdz92+fTs9e/bs0LaKqZiKqZhdLWZLSktLa9x90D4PuHuXX4qKiry9SkpKOrytYiqmYipmV4vZEqDEW6ipGuoREYkYFX4RkYhR4RcRiRgVfhGRiElo4Tezfmb2qJktM7NyM5toZjlmNtvMVoS3/ROZg4iIfFiie/y3AbPcfTTwUaAcuAGY4+4jgTnhuoiIdJKEFX4z6wOcCtwL4O4N7r4JuACYFjabBlyYqBzmlFfx1Mpt1DU0JmoXIiKHnET2+I8A1gH3mdnrZnaPmfUC8ty9AiC8zU1UAjc/Wc69r9fywabtidqFiMghxzxBF2Ixs2JgPnCSuy8ws9uALcA17t4vpt1Gd99nnN/MpgJTAQoKCopmzJhxwDn88Ln1LK3ZyY9P68+43B5ttq+rqyMrK6vD2immYiqmYnZWzJYUFxeXunvxPg+0dFZXRyxAPvBuzPopwJPAcqAg3FYALG8rVnvP3P3GQwt92PVP+GML18TVvrudyaeYiqmY0YnZEjr7zF13rwTeN7NR4aazgKXA48CV4bYrgemJyiE3O+jlV22pT9QuREQOOYmepO0a4EEzywBWAV8k+F3hETO7CngPuDhRO8/rExT+6todidqFiMghJ6GF390XAfuOLwW9/4TLzc4E1OMXEYnVrc/czVWPX0RkH9278Ic9/mr1+EVE9ujehT+mx+8JOmxVRORQ060Lf3aPNHqkGnUNu9i6Q2fviohANy/8Zkb/zOAlapxfRCTQrQs/QP+eYeHfosIvIgJRKPyZqQBU1+oHXhERiEDhz1GPX0TkQ7p94W8a49dJXCIigW5f+Pv1bBrqUY9fRAQiUPhz1OMXEfmQbl/4+4c9/nXq8YuIABEo/Orxi4h8WLcv/FnpRo+0FLY17GKbzt4VEen+hd/MyOsTTtam4R4Rke5f+EFX4hIRiRWNwq95+UVE9ohG4de8/CIie0Sj8KvHLyKyRyQKf56uvSsiskckCv+eHr8mahMRiUbh33s4p3r8IiKRKPxNh3Oqxy8iEpHC37dnOhlpKdTuaKSuQWfviki0RaLwm5l6/SIioYQWfjN718wWm9kiMysJt+WY2WwzWxHe9k9kDk32FH4d0ikiEdcZPf4z3P0Ydy8O128A5rj7SGBOuJ5wTT/w6pBOEYm6ZAz1XABMC+9PAy7sjJ2qxy8iEkh04XfgGTMrNbOp4bY8d68ACG9zE5wDALl9NG2DiAiAuXvigpt9xN0/MLNcYDZwDfC4u/eLabPR3fcZ5w8/KKYCFBQUFM2YMaNdOdTV1ZGVlcWz79Zx+2tbOPWwTK49oV+rbeONeSD7V0zFVEzFTGTMlhQXF5fGDLPv5e6dsgA/Br4DLAcKwm0FwPK2nltUVOTtVVJS4u7uzy+v9mHXP+GfvfuVNtvGG7Mj2yqmYiqmYh5s2+aAEm+hpiZsqMfMeplZdtN9YBJQBjwOXBk2uxKYnqgcYuX1aZqTX2P8IhJtaQmMnQc8ZmZN+3nI3WeZ2WvAI2Z2FfAecHECc9hDUzOLiAQSVvjdfRXw0Ra2rwfOStR+96d/VjrpqcaW+kbqd+4iMz21s1MQEekSInHmLjSdvdvU69dwj4hEV2QKP+ydnrlKs3SKSIRFq/Brvh4RkagVfs3LLyISqcKvQzpFRCJW+NXjFxGJWuHXtXdFRCJW+NXjFxGJVuHXGL+ISMQKf/+sDNJSjM3bd1K/c1ey0xERSYpIFf6UFGNQeCz/Ol2QRUQiKlKFH2IuyKJxfhGJqOgVfp29KyIRF7nCv/cHXvX4RSSaIlf49x7SqR6/iERT5Aq/DukUkaiLXOHXSVwiEnWRK/yD9OOuiERc5Ap/ng7nFJGIi1zhH9Arg9QUY2PdTnY06uxdEYmeyBX+lBRjUG+dvSsi0RW5wg8x0zOr8ItIBEWz8Dcd2aOTuEQkgqJZ+NXjF5EIi2bhz9a0DSISXQkv/GaWamavm9kT4XqOmc02sxXhbf9E59DcnkM6dSy/iERQZ/T4rwXKY9ZvAOa4+0hgTrjeqfb0+DXUIyIRlNDCb2ZDgHOBe2I2XwBMC+9PAy5MZA4t2dvj11CPiERPonv8vwO+B+yO2Zbn7hUA4W1ugnPYR66uwiUiEWbunpjAZucBn3D3r5nZ6cB33P08M9vk7v1i2m10933G+c1sKjAVoKCgoGjGjBntyqOuro6srKwPbdvlzmcerWI38LdP55GeYvttG2/Mg22rmIqpmIp5sG2bKy4uLnX34n0ecPeELMDPgTXAu0AlUAc8ACwHCsI2BcDytmIVFRV5e5WUlLS4fcLNs33Y9U/42o11bbaNN+bBtFVMxVRMxTzYts0BJd5CTU3YUI+7f9/dh7j7cOAzwLPufjnwOHBl2OxKYHqicmhNrq7EJSIRlYzj+G8BzjGzFcA54Xqny9OVuEQkotI6Yyfu/jzwfHh/PXBWZ+y3NXvO3lWPX0QiJpJn7oKuvSsi0RXdwt9HV+ISkWiKbOFvGuOv0pW4RCRiIlv41eMXkaiKbuHP1rV3RSSaIlv4B/bOwAzWb2tg567dbT9BRKSbiGzhT0tNYUCvHrhDzVYN94hIdES28APkaZxfRCIo0oVfV+ISkSiKdOHfMy+/TuISkQg54MJvZv3NbHwikulsTT1+FX4RiZK4Cr+ZPW9mfcwsB3gDuM/Mbk1saok3SFfiEpEIirfH39fdtwCfAu5z9yLg7MSl1Tny1OMXkQiKt/CnmVkBcAnwRALz6VS5YY9fP+6KSJTEW/h/AjwNrHT318zsCGBF4tLqHHsO51SPX0QiJN75+Cvcfc8Puu6+qjuM8Q/s3QOz4ASuRp29KyIREW+P//dxbjukpKemMKBXBu7B1A0iIlHQao/fzCYCHwMGmdl1MQ/1AVITmVhnGZSdSc3WBo3zi0hktNXjzwB6E3xAZMcsW4CLEpta59hzLL+mbRCRiGi1x+/uLwAvmNlf3H11J+XUqWJ/4O3fKVcgFhFJrnhLXQ8zuwsYHvscdz8zEUl1pqZ5+au21DMqJ8nJiIh0gngL/z+APwH3ALsSl07n+9AhnSr8IhIB8Rb+Rne/I6GZJMmg7NhpGyI9Z52IRES8lW6GmX3NzArMLKdpSWhmnUQncYlI1MTb478yvP1uzDYHjujYdDrfh6dt6JncZEREOkFchd/dDz/QwGaWCcwFeoT7edTdbwq/Kfyd4Ifid4FL3H3jgcbvKIN6Bz3+mq072OWerDRERDpNXIXfzD7f0nZ3/2srT9sBnOnuW80sHZhnZjMJZvic4+63mNkNwA3A9QeYd4fJSEshp1cGG7Y1sGWHpm0Qke4v3qGeCTH3M4GzgIXAfgu/uzuwNVxNDxcHLgBOD7dPA54niYUfgpO4NmxrYON2FX4R6f7iHeq5JnbdzPoC97f1PDNLBUqBEcDt7r7AzPLcvSKMW2FmuQeedsfK7ZPJsspaNtZ3qyNVRURaZN6Oce1w6OZNdx8TZ/t+wGPANcA8d+8X89hGd+/fwnOmAlMBCgoKimbMmHHAeQLU1dWRlZXVaps/vLaZ597dzpfGZXLu6H6tto035oG2VUzFVEzFPNi2zRUXF5e6e/E+D7h7mwswA3g8XJ4EVgG3xPPcmBg3Ad8BlgMF4bYCYHlbzy0qKvL2KikpabPNL2eV+7Drn/Dv/fX5Dot5oG0VUzEVUzEPtm1zQIm3UFPjHeP/dcz9RmC1u69p7QlmNgjY6e6bzKwnwaUafxF+eFwJ3BLeTo8zh4RpmrZhzqo6Lr9nQZvtfcdWLkldy5mjc8nOTE90eiIiHSreMf4XzCyPvT/yxnP1rQJgWjjOnwI84u5PmNkrwCNmdhXwHnBxO/LuUKPzswGo2b6beStr4nrOS39bREZqCqeMHMjkwnzOGZtHv6yMRKYpItIh4j2c8xLgVwRH4BjwezP7rrs/ur/nuPubwLEtbF9PcFRQl3H84Tk8+c2TWbBoCSNHjmyz/bOl5SzZlM5rqzcwZ1k1c5ZVk5ZiTDxyAJML85k0Np9B4XTPIiJdTbxDPT8AJrh7NewZxvkPsN/CfygxM47+SF/qK3pQNHJQm+2ztrzHTUVFVNfW88ySKmaVVfLKqvW8uKKGF1fU8MN/lzFheA5TCvMZmaZDREWka4m38Kc0Ff3QejSjGbnZmVx+4jAuP3EYG7c1MHtpFTPLKpi3soYF72xgwTsbGDMwnceLd5OeGvm3S0S6iHgL/ywzexp4OFy/FHgqMSkdmvr3yuCSCUO5ZMJQttTv5Nnyan4+s5zymh38ctYyfnDu2GSnKCICtNFrN7MRZnaSu38XuBMYD3wUeAW4qxPyOyT1yUznwmMHc/tnjyPV4O4X32FWWUWy0xIRAdoervkdUAvg7v9y9+vc/dsEvf3fJTa1Q1/x8ByuGB8cMfTdf7zJOzXbkpyRiEjbhX94eHTOh7h7CcHsmtKG80Zm8Ylx+dTuaOSrD5SyvUHTQohIcrVV+DNbeUyT18fBzPjFp8dzxMBeLKus5cZ/lzWdySwikhRtFf7XzOy/mm8MT74qTUxK3U92Zjp/vPw4MtNT+OfCNfzttfeTnZKIRFhbhf9bwBfN7Hkz+024vABcDVyb8Oy6kdH5ffjfT44D4KbHl1C2dnOSMxKRqGq18Lt7lbt/DPgJwdWy3gV+4u4T3b0y8el1L586bgifO+EwGhp385UHStlctzPZKYlIBMV1VpG7P+fuvw+XZxOdVHf2o/PHMn5IX9Zs3M51jyxi926N94tI59LppJ2sR1oqt3/2OPr2TGfOsmrueOHtZKckIhGjwp8EQ3Oy+N2lxwDwm2eW81KcM4KKiHQEFf4kOWN0LtecOYLdDt98+HXWb9fx/SLSOVT4k+hbZx/FySMGsn5bA79+eRPrt+5IdkoiEgEq/EmUmmLc9pljyO+TyVsbdnL2rS/w79fX6gQvEUkoFf4kG9C7B//4ykTG52awsW4n3/r7Iq6aVsIHm7YnOzUR6aZU+LuAoTlZ/OjU/vzy0+PJzkzj2WXVTPrtXB6Yv1qHe4pIh1Ph7yLMjEsmDOU/153GpLF5bN3RyI3/LuOyu+drVk8R6VAq/F1MXp9M7ryiiNs/exwDe2ew4J0NTP7dXO584W0ad+kyjiJy8FT4uyAz49zxBcz+9ml86rjB7Gjczc9nLuOTf3yZpR9sSXZ6InKIU+Hvwvr3yuDWS47hL1+cwOB+PVm8djP/7w/zeLislh2NOu5fRNpHhf8QcPqoXJ7+9ql8fuIwGnc7j5Zv49z/m0fp6o3JTk1EDkEq/IeI3j3S+OkFhTzy5Yl8pHcqK6u3ctGfXuYnM5ZQ19CY7PRE5BCiwn+IOf7wHH4zaSBfPf1IUsy476V3mfTbucxbofl+RCQ+CSv8ZjbUzJ4zs3IzW2Jm14bbc8xstpmtCG/7JyqH7ioj1bh+8mimf/0kxhb0Yc3G7Vx+7wK+9+gbbN6uOf5FpHWJ7PE3Av/t7mOAE4Gvm9lY4AZgjruPBOaE69IOhYP7Mv0bJ/Hdj48iIzWFR0rWcM6tL/D0El0jR0T2L2GF390r3H1heL8WKAcGAxcA08Jm04ALE5VDFKSnpvD1M0bw1LWnUDSsP9W1O/jy/aV8/cGFbKrXkT8isq9OGeM3s+HAscACIM/dKyD4cAByOyOH7m5Ebm/+8eWJ/Pj8sWRlpPLk4gq+9lQNX3uwlMff+ICtO/QDsIgELNEzQZpZb+AF4Gfu/i8z2+Tu/WIe3+ju+4zzm9lUYCpAQUFB0YwZM9q1/7q6OrKysjq0bVePWb2tkbsX1rKwcu80z+kpcEx+D04cnEnxR3rQO+PDn/nd5bUrpmJGNWZLiouLS929eJ8H3D1hC5AOPA1cF7NtOVAQ3i8AlrcVp6ioyNurpKSkw9seKjFnzl3g97y4yi+64yUffsMTPuz6YDny+0/6Ffcu8IcWrPaa2vqk56mYiqmYHdO2OaDEW6ipae36GImDmRlwL1Du7rfGPPQ4cCVwS3g7PVE5RN2grFQmFx3OVScfTvWWep5eWsXMxRUseGcDc99ax9y31vGDxxZz/OE5HN13J0NG1pPXJzPZaYtIgiWs8AMnAVcAi81sUbjtfwgK/iNmdhXwHnBxAnOQUG6fTK44cRhXnDiMDdsamL20kplllby0sob5qzYwH7j39TkUDevPlMJ8Pn50PkNz2vf1UkS6toQVfnefB9h+Hj4rUfuVtuX0yuDSCYdx6YTD2Lx9J88uq+LhF5fxRvVOSldvpHT1Rm5+spxxg/syuTCfKYX5HDGod7LTFpEOksgevxwC+vZM55PHDuGw3VWMLvwozy2vZmZZJc8tq2bx2s0sXruZXz29nFF52UwuzOfI9EaKkp20iBwUFX7Zo1ePNM4b/xHOG/8R6nfuYu5b65hVVsns8iqWV9WyvKqWFKCsrpxvn30UPTNSk52yiLSDCr+0KDM9lUlH5zPp6HwaGnfz0ts1PPFGBY8tXMNdc1fx9JJKbvnUeCYeOSDZqYrIAVLhlzZlpKVwxqhczhiVy4T+ddxXtpPlVbVcdvd8Ljv+ML7/idH0yUxPdpoiEifNzikHZGROBjOuOZlvn30U6anGw6++x6Rb5zKnvCrZqYlInFT45YBlpKVw7dkjefKbp3DM0H5UbqnnqmklfPPh11m/dUfbAUQkqVT4pd2Oysvmn1/9GDeeO4bM9BQef+MDzvntXKYvWtt0lraIdEEq/HJQUlOMq085gme+dRofO3IAG7Y1cO3fFnH1tBI2btfsoCJdkQq/dIjDBmTx4NUncMunxpHdI405y6r5wXMbWLOxLtmpiUgzKvzSYcyMzxx/GLOvO43xQ/pStW0Xl945n/fWq/iLdCUq/NLh8vtm8sDVJ3BUTjprN23nkjtfYdW6rclOS0RCKvySEH0y0/nRqf2ZMLw/lVvqufSu+aysrk12WiKCCr8kUM/0FKZ96XgmHjGAdbU7uPTO+Syr3JLstEQiT4VfEiorI40/f2ECp4wcyPptDVx213zK1m5OdloikabCLwnXMyOVuz9fzJmjc9lYt5PP3j2fRe9vSnZaIpGlwi+dIjM9lT9dXsSksXlsqW/k8nsWULp6Q7LTEokkFX7pNBlpKdz+ueM4d3wBW3c0csW9r7Jg1fpkpyUSOSr80qnSU1O47dJjuPCYj1DXsIsr73uVN6o0v49IZ9K0zNLp0lJT+M0lx5CWmsKjpWv46dyN/HPli0wpzGdyYQEjcnWZR5FEUuGXpEhNMX756fEM7N2Dv7y0iiUfbGHJB1v49TNvMTK3N1MK85kyroDR+dmY7e/SzSLSHir8kjQpKcYNU0Zz2oCtbOs9lJlllcxeWsmK6q2seHYl//fsSoYPyGJyYQFTCvMZP6RvslMW6RZU+CXpMlKNiWPzOHtsHg2N43hl1XpmlVXwzJIq3l1fx59eeJs/vfA2BX0z6ZWyi16vzGszZm/bwS2H1zE0J6sTXoHIoUWFX7qUjLQUTjtqEKcdNYibL3RefWcDs8oqmLWkkorN9UGjjfGdADbpt3P53uRRfH7icFJTNFwk0kSFX7qs1BRj4pEDmHjkAG46/2hWVG/l9cVljB49ptXn7drt3PrEQl56v56fzFjKjDc+4JcXjWdEbnYnZS7StanwyyEhJcUYlZ/N1rUZHDO0X5vtrzuxH184Ywg3/nsxC9/bxCdum8c1Z47gK6cfSXqqjmKWaEvYX4CZ/dnMqs2sLGZbjpnNNrMV4W3/RO1f5JyxeTzz7dO47PihNOzazW9mv8X5v5/H4jWaK0iiLZFdn78Ak5ttuwGY4+4jgTnhukjC9O2Zzs8/NZ6Hrj6Bw3KyWFZZywW3z+PnM8up36lLQ0o0Jazwu/tcoPlkLBcA08L704ALE7V/kVgfGzGQp791KleffDgAd76wiim3vagpIySSOnuMP8/dKwDcvcLMcjt5/xJhPTNSufG8sZw7voDr//kmb1Vt5dK75pOWAin/mtnm83OzjFv61HDyyIGdkK1I4pi7Jy642XDgCXcvDNc3uXu/mMc3unuL4/xmNhWYClBQUFA0Y8aMduVQV1dHVlZ8x3LH21YxD/2YO3c7j5Vv47HlW2k4wBGfM4f35AsfzaZXxv6/MHfl166Y3TNmS4qLi0vdvXifB9w9YQswHCiLWV8OFIT3C4Dl8cQpKiry9iopKenwtorZfWLubNzlryx4zet3Nra6bNux079///M+8n+e8mHXP+ETbp7ts8oqOi1PxVTM9gBKvIWa2tnHtT0OXBnevxKY3sn7F/mQtNQU0lONHmmprS5ZGWl8ekxvnrr2ZIqG9ae6dgdfvr+Urz+4kHW1ml1UDi2JPJzzYeAVYJSZrTGzq4BbgHPMbAVwTrgucsgYkZvNP748kR+fP5asjFSeXFzBOb99gX8tXNP0rVaky0vYj7vuftl+HjorUfsU6QwpKcYXTjqcs8bk8T+PLebFFTVc98gbPP7GB/zsk+MY3K9nslMUaZVOYRRpp6E5Wfz1S8fz64s/St+e6Ty/fB2Tbn2B+195l13q/UsXpikbRA6CmXFR0RBOPWogN01fwsyySn44fQn9eqTwifcX84nCAk44IkfTREiXosIv0gFyszO54/IiZpVVcMvMZby7vo6HFrzHQwveo19WOueMyWPKuHxOGjGQHmmpyU5XIk6FX6QDTS4s4ONH5/PPZxeweld/ZpZVsrJ6K/8oXcM/SteQ3SONM8fkMqUwn9OO0vmLkhwq/CIdzMw4vF86FxWN4r8njWJldS0zF1fyVFkl5RVbmL7oA6Yv+oCe6al8NDeNy1LXcuboXLIz05OdukSECr9Igo3Izeaas7K55qyRrF6/jZlllcwsq+SN9zcxf+0u5v9tERlpKZw6ciCTCws4Z0wefbP0ISCJo8Iv0omGDejFV047kq+cdiRrN23nnlmvUbYpjZLVG/lPeTX/Ka8mLbwAzZTCAiYdncfA3j2SnbZ0Myr8IkkyuF9PzhvZi5uKiqiurefpJVXMKqtg/qoNvLiihhdX1HDjvxczYXgOUwrz+YhrGmnpGCr8Il1AbnYmV5w4jCtOHMaGbQ38Z2kVM8sqmLeyhgXvbGDBO8EM58e9+RJTCguYXJivC8lLu6nwi3QxOb0yuGTCUC6ZMJQt9Tt5tryamWUVPFdexcL3NrHwvU387KlyCgf32fMhcOSg3slOWw4hKvwiXVifzHQuPHYwFx47mHkLXmNzzyHBh8CyasrWbqFs7RZ+9fRyRuVlM7kwnynj8jVnkLRJhV/kENEzLYWTxxdw7vgC6nfu4sUVNcxcXMHs8iqWV9WyvKqW2+asoKB3KhfWLGNKYT7jBvfFzJKdunQxKvwih6DM9FTOGZvHOWPzaGjczctv1zCrrJJnllZRsbWBO55/mzuef5vB/XoyJfwmcOzQ/qSk6ENAVPhFDnkZaSmcPiqX00flcvOFu3ngmfmsaujDrLLK4JDRee9wz7x3yOvTg48fnc/kwnyOH55DmuYPiiwVfpFuJC01hXG5PfhCUSE/Pv9oFr63kZlllXs+BP76ymr++spqcnplcPaYXBq31jJn3bI249Zv2kbeEXUM6a8jiboDFX6RbiolxSgenkPx8BxuPHcMi9du3vMh8E7NNh4pWRM0XPZ2XPH+vOg5xg/py5TCAqYU5jN8YK8EZi+JpMIvEgFmxvgh/Rg/pB/f+/gollfVMm9FDatWv8/gwYNbfa6789LS1SyqauTNNZt5c81mfjFrGaPzs4MPgXH5jMztrR+RDyEq/CIRY2aMzu/D6Pw+lGZtpKhoRJvPmdh3C2PHHcMLb61jVlkFc8qrWVZZy7LKWn77n7c4YlAvphTmM2lsPuvqdrF20/Y2Y9Y37u6IlyPtoMIvInHpmZHK5MLgx+Edjbt4eeV6ZpZV8MzSKlat28btz73N7c+Fw0ZPPttmvLQUOLX8NSYX5nPOmDz698pI8CuQJir8InLAeqSlcsboXM4YncvPdu1mwaoNzCyr4KWVNWytqycjo/Ui7kDV5nqeXVbNs8uqSU0xJh4xgMmF+Xz86HwGZWtiukRS4ReRg5KemsLJIwdy8siBAJSWllJUVNTm8+a89CqVaXnMKqvk5bfXM29lDfNW1vDD6WV7JqabXJhPQV9dvL6jqfCLSFL0y0zlrKJhfO6EYWyqa2D20ipmlVXy4ooaXn1nA6++s4GfzFjK+CF9SWusp++br7YZc/PmLXG1O5C2yY45NnsHcXyOHhAVfhFJun5ZGVxcPJSLi4dSW7+TZ5dVM3NxJc+/Vc2bazYHjSrXxRcs3naHSMy+ozr+sFkVfhHpUrIz07ngmMFccMxg6hoaWbh6E0uXL2fEiLaPPlq5cmVc7Q6kbbJj1la8G1e7A6HCLyJdVlZGGiePHEjPLaspGp3XZvu+29bE1e5A2iY7Zum2NXG1OxCarENEJGKSUvjNbLKZLTezlWZ2QzJyEBGJqk4v/GaWCtwOTAHGApeZ2djOzkNEJKqS0eM/Hljp7qvcvQH4G3BBEvIQEYkk6+zLtJnZRcBkd786XL8COMHdv9Gs3VRgKkBBQUHRjBkz2rW/uro6srLim0o23raKqZiKqZhdLWZLiouLS929eJ8H3L1TF+Bi4J6Y9SuA37f2nKKiIm+vkpKSDm+rmIqpmIrZ1WK2BCjxFmpqMoZ61gBDY9aHAB8kIQ8RkUhKxlBPGvAWcBawFngN+Ky7L2nlOeuA1e3c5UCgpoPbKqZiKqZidrWYLRnm7oP22drS14BEL8AnCIr/28APEryvFr/qHExbxVRMxVTMrhbzQJaknLnr7k8BTyVj3yIiUaczd0VEIiYKhf+uBLRVTMVUTMXsajHj1uk/7oqISHJFoccvIiIxVPhFJKHCQ7ilC1Hh70YsMLTtltKVmdn94e21yc6lg8R3jcEEM7N9LmBoZucnI5dki/wYv5n1AD4NDCfmwjTu/tNm7Qz4HHCEu//UzA4D8t39oP5Tm9lHgVPC1Rfd/Y0W2vzC3a9va1u4vdTd27xCp5ldDMxy91ozuxE4DrjZ3Rfup31/YCSQ2bTN3ee20K7V99PMrmstL3e/tVm8acC17r4pJo/fuPuX2nqN+2NmP9rPvn/aQtv7gbkE/zbLDmAfBcAGd9/RwmMtvQebgVJ3X2RmSwlmr30cOB2wZnluaCFmJvA14GTAgXnAHe5e36zdN4AH3X1jHK+hGPgBMIzg39KC3fv4Np53EsFJmV8P119392Pb2l/YdgSQ5+4vNdt+CvCBu78ds+0Ed18QT9yw/ULgSndfHK5fBnzL3U+IN0Z7mNkJQLm7bzGznsANBH9vS4H/dffNzdrPIfg//lTMtrvcfWpH5aQeP0wnmB20EdgWszT3R2AicFm4XkswvXS7hT26B4HccHnAzK5poek5LWybsp+w881sQhy7/2FY9E8GPg5MA+7YT55XExS/p4GfhLc/3k/ctt7P7HApBr4KDA6XrxBM093c+KaiDxAWrLiKSCti89pF8F4O30/b+4AC4Pdm9raZ/TPOnvj9wDIz+3ULjxUTvN6m1z6VoMDfbWbfA/4EzAJGA6XNlpL97O+vwNHA74E/AGPCHJrLB14zs0fC62JYC22aPEjw+j8NnA+cF97uw8yOMbNfmtm7wM1A7IfkIDO7bn9Ls1C/I/jbam57+FisX5nZC2aW08priHURMM3MxpjZfxF8UE6K87mYWf4BtP1izOqfgbrw/m1AX+AX4bb7Wnj64cD1ZnZTzLZ9J1o7GB19RtihtgBlcbZbGN6+HrPtjWZt5oW3tcCWmKUW2NJCzDeBXjHrvYA3Y9a/CiwmKFBvxizvAA/sJ8+lBEX37bDt4tiYMe1eD29/TtA7+9Bra9Z2MUFPf1G4Phr4+0G+n88A2THr2QTfQJq3ewPoH7OeAyxu1qb5+93q+97CPnoAT7fyeCpwIvB9gqlDlsX5Gg04uoXtTwO9Y9Z7ExT6nsDSmO13HMD/4zfi2RaT18cJpkRfCfwvcGQL7ea1sc+jgB8B5QTfMK4BVrfQriJsd1NLS7z/f5r/u4fb0oC0A3ifjgr/Rp4Gesb7vPC5Tx5A2/di7pfH3F/YrN2iFp67MHxdfwRmEHxQLDyQXNta9KMLvGxm4zz8+teKneFFZIK/HLNBwO7YBu5+cnibHee+jaDH2WQXH/5a/xAwk6A4x16prNZb+Lof2t83gebWmtmdwNnAL8Ihmv19A6x393ozw8x6uPsyMxu1n7bxvp+HAQ0x6w203Ov+TRjzUYL3/hLgZ7ENDuD93p8s4IiWHgi/dvcCXgFeBCa4e3U8QT34K25pDqrmr30nwZwq281sz9CQu381vvQBeN3MTnT3+WHeJwAvtdTQ3d3MKoFKgk5Cf+BRM5vt7t+LaXqTmd0DzAFi8/pXeHcZwXtyvruvDPf77RZ2WeEtDKPtR2Yrj/Vs4bU0thXQzBYT/t2Gcgg+zBeYGd7G0FXMvs5tFvfN/e0SiL2gbpmZfdHd7wPeMLNidy8xs6MI/u33eX74ur5mZl8g+FDtH0+O8VLhD8ZEv2Bm7xD8597fOOb/AY8BuWb2M4KvjTce5L7vI/jP91i4fiFwb9ODHoz9bWbv8FKb3D3eyewuASYDv3b3TeGY9Hf303aNmfUD/g3MNrONNJtRNeaPKw34opmtovX3837g1fC1O/BJguGm5q/nr2ZWApwZxvqUuy+N8zW2qFkhSAUGAfsrTG8CRUAhwb/FJjN7xd23H0QKDxEMyU0P188HHjazXgS90fY4Afi8mb0Xrh8GlDe91qb338y+CVxJMOnXPcB33X2nmaUAK4DYwv9Fgm936ezt5DjQVPg/DXwGeM7MZhF8g2hp6Ki14aTmXjOz/3L3uz8UwOwqgqGu9jivnc9rSx7BN6fmv5cY8HLM+tXAbeFvaTXAK2b2PvB++Fhzf2q64+5/Cf8Nv96RievHXbNhLW1vqYCa2WiCWUUNmOPu5R2w/+MIPnwMmOvurx9szEQys9MIvnrO8uAKak3bW3wfm+zn/TyOvT9sd9prb5ZrI1DVVs/RzHoTFMLvEPyo3+Mgcyhi77/7PHff39h9vPHiev/N7KfAvfv59xgT+3/azBa7+7g49t2LoNNyGcEH9DTgMXd/Jnw8p5VvqM1j5RF0sBrYW+iLgQzgk+5eGU+czmBm9wL3ufu8Fh57yN0/22xbNsE3yzRgjbtXdU6m+4p84RdpTXgUzCkEvf7V7D3C59mkJtYJzOxu4LcH8g0r/KH1YuBSdz/zIPZ9BsG3LIAlUXi/O5MKv0grzOy7BMW+NJ7x5O7EzMqBIwkOJmht2E4OMSr8ItKiAxkGlUOLCr+ISMToBC4RkYhR4RcRiRgVfokcM/uBmS0xszfNbFF4slOi9vW8BXPeiHQZOoFLIsXMJhKc0HOcu+8ws4EEx4iLRIZ6/BI1BUCNh7NmunuNu39gZj8ys9fMrMzM7mqavCzssf/WzOaaWbmZTTCzf5nZCjO7OWwz3MyWmdm08FvEo2aW1XzHZjbJzF4xs4Vm9o/wpDDM7BYzWxo+t6VJ3UQ6lAq/RM0zwFAze8vM/hieiQzwB3ef4O6FBHPCxJ7m3+DupxKcSj+d4PT5QoKpPgaEbUYBd4XHuG8hmPlxj/CbxY3A2e5+HMEsm9eFJzx9kmAyt/EEM1uKJJQKv0SKu28lOAt3KrAO+Hs4EdYZZrYgnBflTIIpjps8Ht4uJjiLtCL8xrAKaLrwzfu+dw75BwimY4h1IsG00y+Z2SKC+XKGEXxI1AP3mNmn2Dt9r0jCaIxfIsfddwHPA8+Hhf7LwHig2N3fN7Mf8+FZIptmptwdc79pvelvqPkJMc3XDZjt7vtMuGdmxxPMAfUZ4BsEHzwiCaMev0SKmY0ys5Exm44Blof3a8Jx94vaEfqw8IdjCCYraz5x13zgJAuuMIWZZZnZUeH++npwtaVvhfmIJJR6/BI1vQmuptWPYGbOlQTDPpsIhnLeBV5rR9xy4EoLrnGwgmZXM3P3deGQ0sMWXPsAgjH/WmC6BZdONKCl+exFOpSmbBA5SGY2HHgi/GFYpMvTUI+ISMSoxy8iEjHq8YuIRIwKv4hIxKjwi4hEjAq/iEjEqPCLiESMCr+ISMT8f+jC9q9Lq3ooAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='Samples', ylabel='Counts'>"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fd_alpha.plot(30, cumulative=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###INSTRUCTIONS\n",
    "\n",
    "Split the script holy_grail into lines using the newline ('\\n') character.\n",
    "Use re.sub() inside a list comprehension to replace the prompts such as ARTHUR: and SOLDIER #1. The pattern has been written for you.\n",
    "Use a list comprehension to tokenize lines with regexp_tokenize(), keeping only words. Recall that the pattern for words is \"\\w+\".\n",
    "Use a list comprehension to create a list of line lengths called line_num_words.\n",
    "Use t_line as your iterator variable to iterate over tokenized_lines, and then len() function to compute line lengths.\n",
    "Plot a histogram of line_num_words using plt.hist(). Don't forgot to use plt.show() as well to display the plot.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ZOOT: Oh, you must see the doctors immediately!  No, no, please!  Lie down. [clap clap] ', 'PIGLET: Well, what seems to be the trouble?', \"GALAHAD: They're doctors?!\", 'ZOOT: Uh, they have a basic medical training, yes.', 'GALAHAD: B-- but--', 'ZOOT: Oh, come, come.  You must try to rest.  Doctor Piglet!  Doctor Winston!  Practice your art.', 'WINSTON: Try to relax.', \"GALAHAD: Are you sure that's absolutely necessary?\", 'PIGLET: We must examine you.', \"GALAHAD: There's nothing wrong with that!\", 'PIGLET: Please.  We are doctors.', 'GALAHAD: Look!  This cannot be.  I am sworn to chastity.', 'PIGLET: Back to your bed!  At once!', 'GALAHAD: Torment me no longer.  I have seen the Grail!', \"PIGLET: There's no grail here.\", 'GALAHAD: I have seen it!  I have seen it! [clank] I have seen--', 'GIRLS: Hello.', 'GALAHAD: Oh.', 'GIRLS: Hello.  Hello.  Hello.  Hello.  Hello.  Hello.  Hello.  Hello.  Hello.  Hello.  Hello.  Hello.  Hello.  Hello.  Hello.  Hello.  Hello.  Hello.  Hello.  Hello.  Hello.  Hello.  Hello.', 'GALAHAD: Zoot!', \"DINGO: No, I am Zoot's identical twin sister, Dingo.\", 'GALAHAD: Oh, well, excuse me, I--', 'DINGO: Where are you going?', 'GALAHAD: I seek the Grail!  I have seen it, here in this castle!', 'DINGO: Oh no.  Oh, no!  Bad, bad Zoot!', 'GALAHAD: Well, what is it?', \"DINGO: Oh, wicked, bad, naughty Zoot!  She has been setting alight to our beacon, which, I have just remembered, is grail-shaped.  It's not the first time we've had this problem.\", \"GALAHAD: It's not the real Grail?\", \"DINGO: Oh, wicked, bad, naughty, evil Zoot!  She is a bad person and must pay the penalty.  Do you think this scene should have been cut?  We were so worried when the boys were writing it, but now, we're glad.  It's better than some of the previous scenes, I think.\", 'LEFT HEAD: At least ours was better visually.', \"DENNIS: Well, at least ours was committed.  It wasn't just a string of pussy jokes.\", 'OLD MAN: Get on with it.', 'TIM THE ENCHANTER: Yes, get on with it!', 'ARMY OF KNIGHTS: Yes, get on with it!', 'DINGO: Oh, I am enjoying this scene.', 'GOD: Get on with it!', 'DINGO: [sigh] Oh, wicked, wicked Zoot.  Oh, she is a naughty person, and she must pay the penalty.  And here in Castle Anthrax, we have but one punishment for setting alight the grail-shaped beacon.  You must tie her down on a bed and spank her.', 'GIRLS: A spanking!  A spanking!', 'DINGO: You must spank her well.  And after you have spanked her, you may deal with her as you like.  And then, spank me.', 'AMAZING: And spank me.', 'STUNNER: And me.', 'LOVELY: And me.', 'DINGO: Yes, yes, you must give us all a good spanking!', 'GIRLS: A spanking!  A spanking!  There is going to be a spanking tonight!', 'DINGO: And after the spanking, the oral sex.', 'GIRLS: The oral sex!  The oral sex!', 'GALAHAD: Well, I could stay a bit longer.', 'LAUNCELOT: Sir Galahad!', 'GALAHAD: Oh, hello.', 'LAUNCELOT: Quick!', 'GALAHAD: What?', 'LAUNCELOT: Quick!', 'GALAHAD: Why?', 'LAUNCELOT: You are in great peril!', \"DINGO: No he isn't.\", 'LAUNCELOT: Silence, foul temptress!', \"GALAHAD: You know, she's got a point.\", 'LAUNCELOT: Come on!  We will cover your escape!', \"GALAHAD: Look, I'm fine!\", 'LAUNCELOT: Come on!', 'GIRLS: Sir Galahad!', 'GALAHAD: No.  Look, I can tackle this lot single-handed!', 'DINGO: Yes!  Let him tackle us single-handed!', 'GIRLS: Yes!  Let him tackle us single-handed!', 'LAUNCELOT: No, Sir Galahad.  Come on!', 'GALAHAD: No!  Really!  Honestly, I can cope.  I can handle this lot easily.', 'DINGO: Oh, yes.  Let him handle us easily.', 'GIRLS: Yes.  Let him handle us easily.', 'LAUNCELOT: No.  Quick!  Quick!', \"GALAHAD: Please!  I can defeat them!  There's only a hundred-and-fifty of them!\", \"DINGO: Yes, yes!  He will beat us easily!  We haven't a chance.\", \"GIRLS: We haven't a chance.  He will beat us easily... [boom] \", 'DINGO: Oh, shit.', 'LAUNCELOT: We were in the nick of time.  You were in great peril.', \"GALAHAD: I don't think I was.\", 'LAUNCELOT: Yes you were.  You were in terrible peril.', 'GALAHAD: Look, let me go back in there and face the peril.', \"LAUNCELOT: No, it's too perilous.\", \"GALAHAD: Look, it's my duty as a knight to sample as much peril as I can.\", \"LAUNCELOT: No, we've got to find the Holy Grail.  Come on!\", 'GALAHAD: Oh, let me have just a little bit of peril?', \"LAUNCELOT: No.  It's unhealthy.\", \"GALAHAD: I bet you're gay.\", \"LAUNCELOT: No I'm not\", \"NARRATOR: Sir Launcelot had saved Sir Galahad from almost certain temptation, but they were still no nearer the Grail.  Meanwhile, King Arthur and Sir Bedevere, not more than a swallow's flight away, had discovered something.  Oh, that's an unladen swallow's flight, obviously.  I mean, they were more than two laden swallows' flights away-- four, really, if they had a coconut on a line between them.  I mean, if the birds were walking and dragging--\", 'CROWD: Get on with it!', \"NARRATOR: Oh, anyway.  On to scene twenty-four, which is a smashing scene with some lovely acting, in which Arthur discovers a vital clue, and in which there aren't any swallows, although I think you can hear a starling-- oooh\", 'SCENE 12:', 'OLD MAN: Heh, hee ha ha hee hee!  Hee hee hee ha ha ha...', 'ARTHUR: And this enchanter of whom you speak, he has seen the Grail?', 'OLD MAN: ... Ha ha ha ha!  Heh, hee ha ha hee!  Ha hee ha!  Ha ha ha ha...', 'ARTHUR: Where does he live?', 'OLD MAN: ... Heh heh heh heh...', 'ARTHUR: Old man, where does he live?', 'OLD MAN: ... Hee ha ha ha.  He knows of a cave, a cave which no man has entered.', 'ARTHUR: And the Grail.  The Grail is there?', 'OLD MAN: There is much danger, for beyond the cave lies the Gorge of Eternal Peril, which no man has ever crossed.', 'ARTHUR: But the Grail!  Where is the Grail?!', 'OLD MAN: Seek you the Bridge of Death.', 'ARTHUR: The Bridge of Death, which leads to the Grail?', 'OLD MAN: Heh, hee hee hee hee!  Ha ha ha ha ha!  Hee ha ha..', 'SCENE 13: [spooky music] [music stops] ', 'HEAD KNIGHT OF NI: Ni!', 'KNIGHTS OF NI: Ni!  Ni!  Ni!  Ni!  Ni!', 'ARTHUR: Who are you?', \"HEAD KNIGHT: We are the Knights Who Say...  'Ni'!\", 'RANDOM: Ni!', \"ARTHUR: No!  Not the Knights Who Say 'Ni'!\", 'HEAD KNIGHT: The same!', 'BEDEVERE: Who are they?', 'HEAD KNIGHT: We are the keepers of the sacred words: Ni, Peng, and Neee-wom!', 'RANDOM: Neee-wom!', 'ARTHUR: Those who hear them seldom live to tell the tale!', \"HEAD KNIGHT: The Knights Who Say 'Ni' demand a sacrifice!\", 'ARTHUR: Knights of Ni, we are but simple travellers who seek the enchanter who lives beyond these woods.', 'HEAD KNIGHT: Ni!', 'KNIGHTS OF NI: Ni!  Ni!  Ni!  Ni!  Ni! ...', 'ARTHUR: Ow!  Ow!  Ow!  Agh!', \"HEAD KNIGHT: We shall say 'ni' again to you if you do not appease us.\", 'ARTHUR: Well, what is it you want?', 'HEAD KNIGHT: We want...  a shrubbery! [dramatic chord] ', 'ARTHUR: A what?', 'KNIGHTS OF NI: Ni!  Ni!  Ni!  Ni!', 'ARTHUR and PARTY: Ow!  Oh!', 'ARTHUR: Please, please!  No more!  We will find you a shrubbery.', 'HEAD KNIGHT: You must return here with a shrubbery or else you will never pass through this wood alive!', 'ARTHUR: O Knights of Ni, you are just and fair, and we will return with a shrubbery.', 'HEAD KNIGHT: One that looks nice.', 'ARTHUR: Of course.', 'HEAD KNIGHT: And not too expensive.', 'ARTHUR: Yes.', 'HEAD KNIGHT: Now...  go [trumpets] ']\n",
      "\n",
      "\n",
      "lines:\n",
      " [' Oh, you must see the doctors immediately!  No, no, please!  Lie down. [clap clap] ', ' Well, what seems to be the trouble?', \" They're doctors?!\", ' Uh, they have a basic medical training, yes.', ' B-- but--', ' Oh, come, come.  You must try to rest.  Doctor Piglet!  Doctor Winston!  Practice your art.', ' Try to relax.', \" Are you sure that's absolutely necessary?\", ' We must examine you.', \" There's nothing wrong with that!\", ' Please.  We are doctors.', ' Look!  This cannot be.  I am sworn to chastity.', ' Back to your bed!  At once!', ' Torment me no longer.  I have seen the Grail!', \" There's no grail here.\", ' I have seen it!  I have seen it! [clank] I have seen--', ' Hello.', ' Oh.', ' Hello.  Hello.  Hello.  Hello.  Hello.  Hello.  Hello.  Hello.  Hello.  Hello.  Hello.  Hello.  Hello.  Hello.  Hello.  Hello.  Hello.  Hello.  Hello.  Hello.  Hello.  Hello.  Hello.', ' Zoot!', \" No, I am Zoot's identical twin sister, Dingo.\", ' Oh, well, excuse me, I--', ' Where are you going?', ' I seek the Grail!  I have seen it, here in this castle!', ' Oh no.  Oh, no!  Bad, bad Zoot!', ' Well, what is it?', \" Oh, wicked, bad, naughty Zoot!  She has been setting alight to our beacon, which, I have just remembered, is grail-shaped.  It's not the first time we've had this problem.\", \" It's not the real Grail?\", \" Oh, wicked, bad, naughty, evil Zoot!  She is a bad person and must pay the penalty.  Do you think this scene should have been cut?  We were so worried when the boys were writing it, but now, we're glad.  It's better than some of the previous scenes, I think.\", ' At least ours was better visually.', \" Well, at least ours was committed.  It wasn't just a string of pussy jokes.\", ' Get on with it.', 'TIM  Yes, get on with it!', 'ARMY  Yes, get on with it!', ' Oh, I am enjoying this scene.', ' Get on with it!', ' [sigh] Oh, wicked, wicked Zoot.  Oh, she is a naughty person, and she must pay the penalty.  And here in Castle Anthrax, we have but one punishment for setting alight the grail-shaped beacon.  You must tie her down on a bed and spank her.', ' A spanking!  A spanking!', ' You must spank her well.  And after you have spanked her, you may deal with her as you like.  And then, spank me.', ' And spank me.', ' And me.', ' And me.', ' Yes, yes, you must give us all a good spanking!', ' A spanking!  A spanking!  There is going to be a spanking tonight!', ' And after the spanking, the oral sex.', ' The oral sex!  The oral sex!', ' Well, I could stay a bit longer.', ' Sir Galahad!', ' Oh, hello.', ' Quick!', ' What?', ' Quick!', ' Why?', ' You are in great peril!', \" No he isn't.\", ' Silence, foul temptress!', \" You know, she's got a point.\", ' Come on!  We will cover your escape!', \" Look, I'm fine!\", ' Come on!', ' Sir Galahad!', ' No.  Look, I can tackle this lot single-handed!', ' Yes!  Let him tackle us single-handed!', ' Yes!  Let him tackle us single-handed!', ' No, Sir Galahad.  Come on!', ' No!  Really!  Honestly, I can cope.  I can handle this lot easily.', ' Oh, yes.  Let him handle us easily.', ' Yes.  Let him handle us easily.', ' No.  Quick!  Quick!', \" Please!  I can defeat them!  There's only a hundred-and-fifty of them!\", \" Yes, yes!  He will beat us easily!  We haven't a chance.\", \" We haven't a chance.  He will beat us easily... [boom] \", ' Oh, shit.', ' We were in the nick of time.  You were in great peril.', \" I don't think I was.\", ' Yes you were.  You were in terrible peril.', ' Look, let me go back in there and face the peril.', \" No, it's too perilous.\", \" Look, it's my duty as a knight to sample as much peril as I can.\", \" No, we've got to find the Holy Grail.  Come on!\", ' Oh, let me have just a little bit of peril?', \" No.  It's unhealthy.\", \" I bet you're gay.\", \" No I'm not\", \" Sir Launcelot had saved Sir Galahad from almost certain temptation, but they were still no nearer the Grail.  Meanwhile, King Arthur and Sir Bedevere, not more than a swallow's flight away, had discovered something.  Oh, that's an unladen swallow's flight, obviously.  I mean, they were more than two laden swallows' flights away-- four, really, if they had a coconut on a line between them.  I mean, if the birds were walking and dragging--\", ' Get on with it!', \" Oh, anyway.  On to scene twenty-four, which is a smashing scene with some lovely acting, in which Arthur discovers a vital clue, and in which there aren't any swallows, although I think you can hear a starling-- oooh\", 'SCENE 12:', ' Heh, hee ha ha hee hee!  Hee hee hee ha ha ha...', ' And this enchanter of whom you speak, he has seen the Grail?', ' ... Ha ha ha ha!  Heh, hee ha ha hee!  Ha hee ha!  Ha ha ha ha...', ' Where does he live?', ' ... Heh heh heh heh...', ' Old man, where does he live?', ' ... Hee ha ha ha.  He knows of a cave, a cave which no man has entered.', ' And the Grail.  The Grail is there?', ' There is much danger, for beyond the cave lies the Gorge of Eternal Peril, which no man has ever crossed.', ' But the Grail!  Where is the Grail?!', ' Seek you the Bridge of Death.', ' The Bridge of Death, which leads to the Grail?', ' Heh, hee hee hee hee!  Ha ha ha ha ha!  Hee ha ha..', 'SCENE 13: [spooky music] [music stops] ', 'HEAD KNIGHT  Ni!', 'KNIGHTS  Ni!  Ni!  Ni!  Ni!  Ni!', ' Who are you?', \" We are the Knights Who Say...  'Ni'!\", ' Ni!', \" No!  Not the Knights Who Say 'Ni'!\", ' The same!', ' Who are they?', ' We are the keepers of the sacred words: Ni, Peng, and Neee-wom!', ' Neee-wom!', ' Those who hear them seldom live to tell the tale!', \" The Knights Who Say 'Ni' demand a sacrifice!\", ' Knights of Ni, we are but simple travellers who seek the enchanter who lives beyond these woods.', ' Ni!', 'KNIGHTS  Ni!  Ni!  Ni!  Ni!  Ni! ...', ' Ow!  Ow!  Ow!  Agh!', \" We shall say 'ni' again to you if you do not appease us.\", ' Well, what is it you want?', ' We want...  a shrubbery! [dramatic chord] ', ' A what?', 'KNIGHTS  Ni!  Ni!  Ni!  Ni!', 'ARTHUR and  Ow!  Oh!', ' Please, please!  No more!  We will find you a shrubbery.', ' You must return here with a shrubbery or else you will never pass through this wood alive!', ' O Knights of Ni, you are just and fair, and we will return with a shrubbery.', ' One that looks nice.', ' Of course.', ' And not too expensive.', ' Yes.', ' Now...  go [trumpets] ']\n",
      "\n",
      "\n",
      "tokenized lines:\n",
      " [['Oh', 'you', 'must', 'see', 'the', 'doctors', 'immediately', 'No', 'no', 'please', 'Lie', 'down', 'clap', 'clap'], ['Well', 'what', 'seems', 'to', 'be', 'the', 'trouble'], ['They', 're', 'doctors'], ['Uh', 'they', 'have', 'a', 'basic', 'medical', 'training', 'yes'], ['B', 'but'], ['Oh', 'come', 'come', 'You', 'must', 'try', 'to', 'rest', 'Doctor', 'Piglet', 'Doctor', 'Winston', 'Practice', 'your', 'art'], ['Try', 'to', 'relax'], ['Are', 'you', 'sure', 'that', 's', 'absolutely', 'necessary'], ['We', 'must', 'examine', 'you'], ['There', 's', 'nothing', 'wrong', 'with', 'that'], ['Please', 'We', 'are', 'doctors'], ['Look', 'This', 'cannot', 'be', 'I', 'am', 'sworn', 'to', 'chastity'], ['Back', 'to', 'your', 'bed', 'At', 'once'], ['Torment', 'me', 'no', 'longer', 'I', 'have', 'seen', 'the', 'Grail'], ['There', 's', 'no', 'grail', 'here'], ['I', 'have', 'seen', 'it', 'I', 'have', 'seen', 'it', 'clank', 'I', 'have', 'seen'], ['Hello'], ['Oh'], ['Hello', 'Hello', 'Hello', 'Hello', 'Hello', 'Hello', 'Hello', 'Hello', 'Hello', 'Hello', 'Hello', 'Hello', 'Hello', 'Hello', 'Hello', 'Hello', 'Hello', 'Hello', 'Hello', 'Hello', 'Hello', 'Hello', 'Hello'], ['Zoot'], ['No', 'I', 'am', 'Zoot', 's', 'identical', 'twin', 'sister', 'Dingo'], ['Oh', 'well', 'excuse', 'me', 'I'], ['Where', 'are', 'you', 'going'], ['I', 'seek', 'the', 'Grail', 'I', 'have', 'seen', 'it', 'here', 'in', 'this', 'castle'], ['Oh', 'no', 'Oh', 'no', 'Bad', 'bad', 'Zoot'], ['Well', 'what', 'is', 'it'], ['Oh', 'wicked', 'bad', 'naughty', 'Zoot', 'She', 'has', 'been', 'setting', 'alight', 'to', 'our', 'beacon', 'which', 'I', 'have', 'just', 'remembered', 'is', 'grail', 'shaped', 'It', 's', 'not', 'the', 'first', 'time', 'we', 've', 'had', 'this', 'problem'], ['It', 's', 'not', 'the', 'real', 'Grail'], ['Oh', 'wicked', 'bad', 'naughty', 'evil', 'Zoot', 'She', 'is', 'a', 'bad', 'person', 'and', 'must', 'pay', 'the', 'penalty', 'Do', 'you', 'think', 'this', 'scene', 'should', 'have', 'been', 'cut', 'We', 'were', 'so', 'worried', 'when', 'the', 'boys', 'were', 'writing', 'it', 'but', 'now', 'we', 're', 'glad', 'It', 's', 'better', 'than', 'some', 'of', 'the', 'previous', 'scenes', 'I', 'think'], ['At', 'least', 'ours', 'was', 'better', 'visually'], ['Well', 'at', 'least', 'ours', 'was', 'committed', 'It', 'wasn', 't', 'just', 'a', 'string', 'of', 'pussy', 'jokes'], ['Get', 'on', 'with', 'it'], ['TIM', 'Yes', 'get', 'on', 'with', 'it'], ['ARMY', 'Yes', 'get', 'on', 'with', 'it'], ['Oh', 'I', 'am', 'enjoying', 'this', 'scene'], ['Get', 'on', 'with', 'it'], ['sigh', 'Oh', 'wicked', 'wicked', 'Zoot', 'Oh', 'she', 'is', 'a', 'naughty', 'person', 'and', 'she', 'must', 'pay', 'the', 'penalty', 'And', 'here', 'in', 'Castle', 'Anthrax', 'we', 'have', 'but', 'one', 'punishment', 'for', 'setting', 'alight', 'the', 'grail', 'shaped', 'beacon', 'You', 'must', 'tie', 'her', 'down', 'on', 'a', 'bed', 'and', 'spank', 'her'], ['A', 'spanking', 'A', 'spanking'], ['You', 'must', 'spank', 'her', 'well', 'And', 'after', 'you', 'have', 'spanked', 'her', 'you', 'may', 'deal', 'with', 'her', 'as', 'you', 'like', 'And', 'then', 'spank', 'me'], ['And', 'spank', 'me'], ['And', 'me'], ['And', 'me'], ['Yes', 'yes', 'you', 'must', 'give', 'us', 'all', 'a', 'good', 'spanking'], ['A', 'spanking', 'A', 'spanking', 'There', 'is', 'going', 'to', 'be', 'a', 'spanking', 'tonight'], ['And', 'after', 'the', 'spanking', 'the', 'oral', 'sex'], ['The', 'oral', 'sex', 'The', 'oral', 'sex'], ['Well', 'I', 'could', 'stay', 'a', 'bit', 'longer'], ['Sir', 'Galahad'], ['Oh', 'hello'], ['Quick'], ['What'], ['Quick'], ['Why'], ['You', 'are', 'in', 'great', 'peril'], ['No', 'he', 'isn', 't'], ['Silence', 'foul', 'temptress'], ['You', 'know', 'she', 's', 'got', 'a', 'point'], ['Come', 'on', 'We', 'will', 'cover', 'your', 'escape'], ['Look', 'I', 'm', 'fine'], ['Come', 'on'], ['Sir', 'Galahad'], ['No', 'Look', 'I', 'can', 'tackle', 'this', 'lot', 'single', 'handed'], ['Yes', 'Let', 'him', 'tackle', 'us', 'single', 'handed'], ['Yes', 'Let', 'him', 'tackle', 'us', 'single', 'handed'], ['No', 'Sir', 'Galahad', 'Come', 'on'], ['No', 'Really', 'Honestly', 'I', 'can', 'cope', 'I', 'can', 'handle', 'this', 'lot', 'easily'], ['Oh', 'yes', 'Let', 'him', 'handle', 'us', 'easily'], ['Yes', 'Let', 'him', 'handle', 'us', 'easily'], ['No', 'Quick', 'Quick'], ['Please', 'I', 'can', 'defeat', 'them', 'There', 's', 'only', 'a', 'hundred', 'and', 'fifty', 'of', 'them'], ['Yes', 'yes', 'He', 'will', 'beat', 'us', 'easily', 'We', 'haven', 't', 'a', 'chance'], ['We', 'haven', 't', 'a', 'chance', 'He', 'will', 'beat', 'us', 'easily', 'boom'], ['Oh', 'shit'], ['We', 'were', 'in', 'the', 'nick', 'of', 'time', 'You', 'were', 'in', 'great', 'peril'], ['I', 'don', 't', 'think', 'I', 'was'], ['Yes', 'you', 'were', 'You', 'were', 'in', 'terrible', 'peril'], ['Look', 'let', 'me', 'go', 'back', 'in', 'there', 'and', 'face', 'the', 'peril'], ['No', 'it', 's', 'too', 'perilous'], ['Look', 'it', 's', 'my', 'duty', 'as', 'a', 'knight', 'to', 'sample', 'as', 'much', 'peril', 'as', 'I', 'can'], ['No', 'we', 've', 'got', 'to', 'find', 'the', 'Holy', 'Grail', 'Come', 'on'], ['Oh', 'let', 'me', 'have', 'just', 'a', 'little', 'bit', 'of', 'peril'], ['No', 'It', 's', 'unhealthy'], ['I', 'bet', 'you', 're', 'gay'], ['No', 'I', 'm', 'not'], ['Sir', 'Launcelot', 'had', 'saved', 'Sir', 'Galahad', 'from', 'almost', 'certain', 'temptation', 'but', 'they', 'were', 'still', 'no', 'nearer', 'the', 'Grail', 'Meanwhile', 'King', 'Arthur', 'and', 'Sir', 'Bedevere', 'not', 'more', 'than', 'a', 'swallow', 's', 'flight', 'away', 'had', 'discovered', 'something', 'Oh', 'that', 's', 'an', 'unladen', 'swallow', 's', 'flight', 'obviously', 'I', 'mean', 'they', 'were', 'more', 'than', 'two', 'laden', 'swallows', 'flights', 'away', 'four', 'really', 'if', 'they', 'had', 'a', 'coconut', 'on', 'a', 'line', 'between', 'them', 'I', 'mean', 'if', 'the', 'birds', 'were', 'walking', 'and', 'dragging'], ['Get', 'on', 'with', 'it'], ['Oh', 'anyway', 'On', 'to', 'scene', 'twenty', 'four', 'which', 'is', 'a', 'smashing', 'scene', 'with', 'some', 'lovely', 'acting', 'in', 'which', 'Arthur', 'discovers', 'a', 'vital', 'clue', 'and', 'in', 'which', 'there', 'aren', 't', 'any', 'swallows', 'although', 'I', 'think', 'you', 'can', 'hear', 'a', 'starling', 'oooh'], ['SCENE', '12'], ['Heh', 'hee', 'ha', 'ha', 'hee', 'hee', 'Hee', 'hee', 'hee', 'ha', 'ha', 'ha'], ['And', 'this', 'enchanter', 'of', 'whom', 'you', 'speak', 'he', 'has', 'seen', 'the', 'Grail'], ['Ha', 'ha', 'ha', 'ha', 'Heh', 'hee', 'ha', 'ha', 'hee', 'Ha', 'hee', 'ha', 'Ha', 'ha', 'ha', 'ha'], ['Where', 'does', 'he', 'live'], ['Heh', 'heh', 'heh', 'heh'], ['Old', 'man', 'where', 'does', 'he', 'live'], ['Hee', 'ha', 'ha', 'ha', 'He', 'knows', 'of', 'a', 'cave', 'a', 'cave', 'which', 'no', 'man', 'has', 'entered'], ['And', 'the', 'Grail', 'The', 'Grail', 'is', 'there'], ['There', 'is', 'much', 'danger', 'for', 'beyond', 'the', 'cave', 'lies', 'the', 'Gorge', 'of', 'Eternal', 'Peril', 'which', 'no', 'man', 'has', 'ever', 'crossed'], ['But', 'the', 'Grail', 'Where', 'is', 'the', 'Grail'], ['Seek', 'you', 'the', 'Bridge', 'of', 'Death'], ['The', 'Bridge', 'of', 'Death', 'which', 'leads', 'to', 'the', 'Grail'], ['Heh', 'hee', 'hee', 'hee', 'hee', 'Ha', 'ha', 'ha', 'ha', 'ha', 'Hee', 'ha', 'ha'], ['SCENE', '13', 'spooky', 'music', 'music', 'stops'], ['HEAD', 'KNIGHT', 'Ni'], ['KNIGHTS', 'Ni', 'Ni', 'Ni', 'Ni', 'Ni'], ['Who', 'are', 'you'], ['We', 'are', 'the', 'Knights', 'Who', 'Say', 'Ni'], ['Ni'], ['No', 'Not', 'the', 'Knights', 'Who', 'Say', 'Ni'], ['The', 'same'], ['Who', 'are', 'they'], ['We', 'are', 'the', 'keepers', 'of', 'the', 'sacred', 'words', 'Ni', 'Peng', 'and', 'Neee', 'wom'], ['Neee', 'wom'], ['Those', 'who', 'hear', 'them', 'seldom', 'live', 'to', 'tell', 'the', 'tale'], ['The', 'Knights', 'Who', 'Say', 'Ni', 'demand', 'a', 'sacrifice'], ['Knights', 'of', 'Ni', 'we', 'are', 'but', 'simple', 'travellers', 'who', 'seek', 'the', 'enchanter', 'who', 'lives', 'beyond', 'these', 'woods'], ['Ni'], ['KNIGHTS', 'Ni', 'Ni', 'Ni', 'Ni', 'Ni'], ['Ow', 'Ow', 'Ow', 'Agh'], ['We', 'shall', 'say', 'ni', 'again', 'to', 'you', 'if', 'you', 'do', 'not', 'appease', 'us'], ['Well', 'what', 'is', 'it', 'you', 'want'], ['We', 'want', 'a', 'shrubbery', 'dramatic', 'chord'], ['A', 'what'], ['KNIGHTS', 'Ni', 'Ni', 'Ni', 'Ni'], ['ARTHUR', 'and', 'Ow', 'Oh'], ['Please', 'please', 'No', 'more', 'We', 'will', 'find', 'you', 'a', 'shrubbery'], ['You', 'must', 'return', 'here', 'with', 'a', 'shrubbery', 'or', 'else', 'you', 'will', 'never', 'pass', 'through', 'this', 'wood', 'alive'], ['O', 'Knights', 'of', 'Ni', 'you', 'are', 'just', 'and', 'fair', 'and', 'we', 'will', 'return', 'with', 'a', 'shrubbery'], ['One', 'that', 'looks', 'nice'], ['Of', 'course'], ['And', 'not', 'too', 'expensive'], ['Yes'], ['Now', 'go', 'trumpets']]\n",
      "\n",
      "\\ line_num_words:\n",
      " [14, 7, 3, 8, 2, 15, 3, 7, 4, 6, 4, 9, 6, 9, 5, 12, 1, 1, 23, 1, 9, 5, 4, 12, 7, 4, 32, 6, 51, 6, 15, 4, 6, 6, 6, 4, 45, 4, 23, 3, 2, 2, 10, 12, 7, 6, 7, 2, 2, 1, 1, 1, 1, 5, 4, 3, 7, 7, 4, 2, 2, 9, 7, 7, 5, 12, 7, 6, 3, 14, 12, 11, 2, 12, 6, 8, 11, 5, 16, 11, 10, 4, 5, 4, 76, 4, 40, 2, 12, 12, 16, 4, 4, 6, 16, 7, 20, 7, 6, 9, 13, 6, 3, 6, 3, 7, 1, 7, 2, 3, 13, 2, 10, 8, 17, 1, 6, 4, 13, 6, 6, 2, 5, 4, 10, 17, 16, 4, 2, 4, 1, 3]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import regex as re\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "holy_grail = \"\"\"ZOOT: Oh, you must see the doctors immediately!  No, no, please!  Lie down. [clap clap] \n",
    "PIGLET: Well, what seems to be the trouble?\n",
    "GALAHAD: They're doctors?!\n",
    "ZOOT: Uh, they have a basic medical training, yes.\n",
    "GALAHAD: B-- but--\n",
    "ZOOT: Oh, come, come.  You must try to rest.  Doctor Piglet!  Doctor Winston!  Practice your art.\n",
    "WINSTON: Try to relax.\n",
    "GALAHAD: Are you sure that's absolutely necessary?\n",
    "PIGLET: We must examine you.\n",
    "GALAHAD: There's nothing wrong with that!\n",
    "PIGLET: Please.  We are doctors.\n",
    "GALAHAD: Look!  This cannot be.  I am sworn to chastity.\n",
    "PIGLET: Back to your bed!  At once!\n",
    "GALAHAD: Torment me no longer.  I have seen the Grail!\n",
    "PIGLET: There's no grail here.\n",
    "GALAHAD: I have seen it!  I have seen it! [clank] I have seen--\n",
    "GIRLS: Hello.\n",
    "GALAHAD: Oh.\n",
    "GIRLS: Hello.  Hello.  Hello.  Hello.  Hello.  Hello.  Hello.  Hello.  Hello.  Hello.  Hello.  Hello.  Hello.  Hello.  Hello.  Hello.  Hello.  Hello.  Hello.  Hello.  Hello.  Hello.  Hello.\n",
    "GALAHAD: Zoot!\n",
    "DINGO: No, I am Zoot's identical twin sister, Dingo.\n",
    "GALAHAD: Oh, well, excuse me, I--\n",
    "DINGO: Where are you going?\n",
    "GALAHAD: I seek the Grail!  I have seen it, here in this castle!\n",
    "DINGO: Oh no.  Oh, no!  Bad, bad Zoot!\n",
    "GALAHAD: Well, what is it?\n",
    "DINGO: Oh, wicked, bad, naughty Zoot!  She has been setting alight to our beacon, which, I have just remembered, is grail-shaped.  It's not the first time we've had this problem.\n",
    "GALAHAD: It's not the real Grail?\n",
    "DINGO: Oh, wicked, bad, naughty, evil Zoot!  She is a bad person and must pay the penalty.  Do you think this scene should have been cut?  We were so worried when the boys were writing it, but now, we're glad.  It's better than some of the previous scenes, I think.\n",
    "LEFT HEAD: At least ours was better visually.\n",
    "DENNIS: Well, at least ours was committed.  It wasn't just a string of pussy jokes.\n",
    "OLD MAN: Get on with it.\n",
    "TIM THE ENCHANTER: Yes, get on with it!\n",
    "ARMY OF KNIGHTS: Yes, get on with it!\n",
    "DINGO: Oh, I am enjoying this scene.\n",
    "GOD: Get on with it!\n",
    "DINGO: [sigh] Oh, wicked, wicked Zoot.  Oh, she is a naughty person, and she must pay the penalty.  And here in Castle Anthrax, we have but one punishment for setting alight the grail-shaped beacon.  You must tie her down on a bed and spank her.\n",
    "GIRLS: A spanking!  A spanking!\n",
    "DINGO: You must spank her well.  And after you have spanked her, you may deal with her as you like.  And then, spank me.\n",
    "AMAZING: And spank me.\n",
    "STUNNER: And me.\n",
    "LOVELY: And me.\n",
    "DINGO: Yes, yes, you must give us all a good spanking!\n",
    "GIRLS: A spanking!  A spanking!  There is going to be a spanking tonight!\n",
    "DINGO: And after the spanking, the oral sex.\n",
    "GIRLS: The oral sex!  The oral sex!\n",
    "GALAHAD: Well, I could stay a bit longer.\n",
    "LAUNCELOT: Sir Galahad!\n",
    "GALAHAD: Oh, hello.\n",
    "LAUNCELOT: Quick!\n",
    "GALAHAD: What?\n",
    "LAUNCELOT: Quick!\n",
    "GALAHAD: Why?\n",
    "LAUNCELOT: You are in great peril!\n",
    "DINGO: No he isn't.\n",
    "LAUNCELOT: Silence, foul temptress!\n",
    "GALAHAD: You know, she's got a point.\n",
    "LAUNCELOT: Come on!  We will cover your escape!\n",
    "GALAHAD: Look, I'm fine!\n",
    "LAUNCELOT: Come on!\n",
    "GIRLS: Sir Galahad!\n",
    "GALAHAD: No.  Look, I can tackle this lot single-handed!\n",
    "DINGO: Yes!  Let him tackle us single-handed!\n",
    "GIRLS: Yes!  Let him tackle us single-handed!\n",
    "LAUNCELOT: No, Sir Galahad.  Come on!\n",
    "GALAHAD: No!  Really!  Honestly, I can cope.  I can handle this lot easily.\n",
    "DINGO: Oh, yes.  Let him handle us easily.\n",
    "GIRLS: Yes.  Let him handle us easily.\n",
    "LAUNCELOT: No.  Quick!  Quick!\n",
    "GALAHAD: Please!  I can defeat them!  There's only a hundred-and-fifty of them!\n",
    "DINGO: Yes, yes!  He will beat us easily!  We haven't a chance.\n",
    "GIRLS: We haven't a chance.  He will beat us easily... [boom] \n",
    "DINGO: Oh, shit.\n",
    "LAUNCELOT: We were in the nick of time.  You were in great peril.\n",
    "GALAHAD: I don't think I was.\n",
    "LAUNCELOT: Yes you were.  You were in terrible peril.\n",
    "GALAHAD: Look, let me go back in there and face the peril.\n",
    "LAUNCELOT: No, it's too perilous.\n",
    "GALAHAD: Look, it's my duty as a knight to sample as much peril as I can.\n",
    "LAUNCELOT: No, we've got to find the Holy Grail.  Come on!\n",
    "GALAHAD: Oh, let me have just a little bit of peril?\n",
    "LAUNCELOT: No.  It's unhealthy.\n",
    "GALAHAD: I bet you're gay.\n",
    "LAUNCELOT: No I'm not\n",
    "NARRATOR: Sir Launcelot had saved Sir Galahad from almost certain temptation, but they were still no nearer the Grail.  Meanwhile, King Arthur and Sir Bedevere, not more than a swallow's flight away, had discovered something.  Oh, that's an unladen swallow's flight, obviously.  I mean, they were more than two laden swallows' flights away-- four, really, if they had a coconut on a line between them.  I mean, if the birds were walking and dragging--\n",
    "CROWD: Get on with it!\n",
    "NARRATOR: Oh, anyway.  On to scene twenty-four, which is a smashing scene with some lovely acting, in which Arthur discovers a vital clue, and in which there aren't any swallows, although I think you can hear a starling-- oooh\n",
    "SCENE 12:\n",
    "OLD MAN: Heh, hee ha ha hee hee!  Hee hee hee ha ha ha...\n",
    "ARTHUR: And this enchanter of whom you speak, he has seen the Grail?\n",
    "OLD MAN: ... Ha ha ha ha!  Heh, hee ha ha hee!  Ha hee ha!  Ha ha ha ha...\n",
    "ARTHUR: Where does he live?\n",
    "OLD MAN: ... Heh heh heh heh...\n",
    "ARTHUR: Old man, where does he live?\n",
    "OLD MAN: ... Hee ha ha ha.  He knows of a cave, a cave which no man has entered.\n",
    "ARTHUR: And the Grail.  The Grail is there?\n",
    "OLD MAN: There is much danger, for beyond the cave lies the Gorge of Eternal Peril, which no man has ever crossed.\n",
    "ARTHUR: But the Grail!  Where is the Grail?!\n",
    "OLD MAN: Seek you the Bridge of Death.\n",
    "ARTHUR: The Bridge of Death, which leads to the Grail?\n",
    "OLD MAN: Heh, hee hee hee hee!  Ha ha ha ha ha!  Hee ha ha..\n",
    "SCENE 13: [spooky music] [music stops] \n",
    "HEAD KNIGHT OF NI: Ni!\n",
    "KNIGHTS OF NI: Ni!  Ni!  Ni!  Ni!  Ni!\n",
    "ARTHUR: Who are you?\n",
    "HEAD KNIGHT: We are the Knights Who Say...  'Ni'!\n",
    "RANDOM: Ni!\n",
    "ARTHUR: No!  Not the Knights Who Say 'Ni'!\n",
    "HEAD KNIGHT: The same!\n",
    "BEDEVERE: Who are they?\n",
    "HEAD KNIGHT: We are the keepers of the sacred words: Ni, Peng, and Neee-wom!\n",
    "RANDOM: Neee-wom!\n",
    "ARTHUR: Those who hear them seldom live to tell the tale!\n",
    "HEAD KNIGHT: The Knights Who Say 'Ni' demand a sacrifice!\n",
    "ARTHUR: Knights of Ni, we are but simple travellers who seek the enchanter who lives beyond these woods.\n",
    "HEAD KNIGHT: Ni!\n",
    "KNIGHTS OF NI: Ni!  Ni!  Ni!  Ni!  Ni! ...\n",
    "ARTHUR: Ow!  Ow!  Ow!  Agh!\n",
    "HEAD KNIGHT: We shall say 'ni' again to you if you do not appease us.\n",
    "ARTHUR: Well, what is it you want?\n",
    "HEAD KNIGHT: We want...  a shrubbery! [dramatic chord] \n",
    "ARTHUR: A what?\n",
    "KNIGHTS OF NI: Ni!  Ni!  Ni!  Ni!\n",
    "ARTHUR and PARTY: Ow!  Oh!\n",
    "ARTHUR: Please, please!  No more!  We will find you a shrubbery.\n",
    "HEAD KNIGHT: You must return here with a shrubbery or else you will never pass through this wood alive!\n",
    "ARTHUR: O Knights of Ni, you are just and fair, and we will return with a shrubbery.\n",
    "HEAD KNIGHT: One that looks nice.\n",
    "ARTHUR: Of course.\n",
    "HEAD KNIGHT: And not too expensive.\n",
    "ARTHUR: Yes.\n",
    "HEAD KNIGHT: Now...  go [trumpets] \"\"\"\n",
    "# Split the script into lines: lines\n",
    "lines = holy_grail.split('\\n')     #or here we can simple use sentence tokenizer, we will get same result\n",
    "\n",
    "print(lines)\n",
    "\n",
    "# Replace all script lines for speaker [like removing zoot,old man,arthur etc...]\n",
    "pattern = \"[A-Z]{2,}(\\s)?(#\\d)?([A-Z]{2,})?:\"\n",
    "lines = [re.sub(pattern, '', l) for l in lines]\n",
    "print(\"\\n\\nlines:\\n\",lines)\n",
    "\n",
    "# Tokenize each line: tokenized_lines\n",
    "#WE CANT USE NORMAL WORD TOKENIZER HERE BECAUSE, IT NEEDS A STRING RATHER THAN A LIST OF STRINGS...HENCE THIS OR WE .LOWER()\n",
    "tokenized_lines = [regexp_tokenize(s, \"\\w+\") for s in lines]\n",
    "print(\"\\n\\ntokenized lines:\\n\",tokenized_lines)\n",
    "\n",
    "# Make a frequency list of lengths: line_num_words\n",
    "########################## this contain length of each string in list ####################\n",
    "line_num_words = [len(t_line) for t_line in tokenized_lines]\n",
    "print(\"\\n\\ line_num_words:\\n\",line_num_words)\n",
    "# Plot a histogram of the line lengths\n",
    "plt.hist(line_num_words)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ZOOT: Oh, you must see the doctors immediately!  No, no, please!  Lie down. [clap clap] \n",
       "PIGLET: Well, what seems to be the trouble?\n",
       "GALAHAD: They're doctors?!\n",
       "ZOOT: Uh, they have a basic medical training, yes.\n",
       "GALAHAD: B-- but--\n",
       "ZOOT: Oh, come, come.  You must try to rest.  Doctor Piglet!  Doctor Winston!  Practice your art.\n",
       "WINSTON: Try to relax.\n",
       "GALAHAD: Are you sure that's absolutely necessary?\n",
       "PIGLET: We must examine you.\n",
       "GALAHAD: There's nothing wrong with that!\n",
       "PIGLET: Please.  We are doctors.\n",
       "GALAHAD: Look!  This cannot be.  I am sworn to chastity.\n",
       "PIGLET: Back to your bed!  At once!\n",
       "GALAHAD: Torment me no longer.  I have seen the Grail!\n",
       "PIGLET: There's no grail here.\n",
       "GALAHAD: I have seen it!  I have seen it! [clank] I have seen--\n",
       "GIRLS: Hello.\n",
       "GALAHAD: Oh.\n",
       "GIRLS: Hello.  Hello.  Hello.  Hello.  Hello.  Hello.  Hello.  Hello.  Hello.  Hello.  Hello.  Hello.  Hello.  Hello.  Hello.  Hello.  Hello.  Hello.  Hello.  Hello.  Hello.  Hello.  Hello.\n",
       "GALAHAD: Zoot!\n",
       "DINGO: No, I am Zoot's identical twin sister, Dingo.\n",
       "GALAHAD: Oh, well, excuse me, I--\n",
       "DINGO: Where are you going?\n",
       "GALAHAD: I seek the Grail!  I have seen it, here in this castle!\n",
       "DINGO: Oh no.  Oh, no!  Bad, bad Zoot!\n",
       "GALAHAD: Well, what is it?\n",
       "DINGO: Oh, wicked, bad, naughty Zoot!  She has been setting alight to our beacon, which, I have just remembered, is grail-shaped.  It's not the first time we've had this problem.\n",
       "GALAHAD: It's not the real Grail?\n",
       "DINGO: Oh, wicked, bad, naughty, evil Zoot!  She is a bad person and must pay the penalty.  Do you think this scene should have been cut?  We were so worried when the boys were writing it, but now, we're glad.  It's better than some of the previous scenes, I think.\n",
       "LEFT HEAD: At least ours was better visually.\n",
       "DENNIS: Well, at least ours was committed.  It wasn't just a string of pussy jokes.\n",
       "OLD MAN: Get on with it.\n",
       "TIM THE ENCHANTER: Yes, get on with it!\n",
       "ARMY OF KNIGHTS: Yes, get on with it!\n",
       "DINGO: Oh, I am enjoying this scene.\n",
       "GOD: Get on with it!\n",
       "DINGO: [sigh] Oh, wicked, wicked Zoot.  Oh, she is a naughty person, and she must pay the penalty.  And here in Castle Anthrax, we have but one punishment for setting alight the grail-shaped beacon.  You must tie her down on a bed and spank her.\n",
       "GIRLS: A spanking!  A spanking!\n",
       "DINGO: You must spank her well.  And after you have spanked her, you may deal with her as you like.  And then, spank me.\n",
       "AMAZING: And spank me.\n",
       "STUNNER: And me.\n",
       "LOVELY: And me.\n",
       "DINGO: Yes, yes, you must give us all a good spanking!\n",
       "GIRLS: A spanking!  A spanking!  There is going to be a spanking tonight!\n",
       "DINGO: And after the spanking, the oral sex.\n",
       "GIRLS: The oral sex!  The oral sex!\n",
       "GALAHAD: Well, I could stay a bit longer.\n",
       "LAUNCELOT: Sir Galahad!\n",
       "GALAHAD: Oh, hello.\n",
       "LAUNCELOT: Quick!\n",
       "GALAHAD: What?\n",
       "LAUNCELOT: Quick!\n",
       "GALAHAD: Why?\n",
       "LAUNCELOT: You are in great peril!\n",
       "DINGO: No he isn't.\n",
       "LAUNCELOT: Silence, foul temptress!\n",
       "GALAHAD: You know, she's got a point.\n",
       "LAUNCELOT: Come on!  We will cover your escape!\n",
       "GALAHAD: Look, I'm fine!\n",
       "LAUNCELOT: Come on!\n",
       "GIRLS: Sir Galahad!\n",
       "GALAHAD: No.  Look, I can tackle this lot single-handed!\n",
       "DINGO: Yes!  Let him tackle us single-handed!\n",
       "GIRLS: Yes!  Let him tackle us single-handed!\n",
       "LAUNCELOT: No, Sir Galahad.  Come on!\n",
       "GALAHAD: No!  Really!  Honestly, I can cope.  I can handle this lot easily.\n",
       "DINGO: Oh, yes.  Let him handle us easily.\n",
       "GIRLS: Yes.  Let him handle us easily.\n",
       "LAUNCELOT: No.  Quick!  Quick!\n",
       "GALAHAD: Please!  I can defeat them!  There's only a hundred-and-fifty of them!\n",
       "DINGO: Yes, yes!  He will beat us easily!  We haven't a chance.\n",
       "GIRLS: We haven't a chance.  He will beat us easily... [boom] \n",
       "DINGO: Oh, shit.\n",
       "LAUNCELOT: We were in the nick of time.  You were in great peril.\n",
       "GALAHAD: I don't think I was.\n",
       "LAUNCELOT: Yes you were.  You were in terrible peril.\n",
       "GALAHAD: Look, let me go back in there and face the peril.\n",
       "LAUNCELOT: No, it's too perilous.\n",
       "GALAHAD: Look, it's my duty as a knight to sample as much peril as I can.\n",
       "LAUNCELOT: No, we've got to find the Holy Grail.  Come on!\n",
       "GALAHAD: Oh, let me have just a little bit of peril?\n",
       "LAUNCELOT: No.  It's unhealthy.\n",
       "GALAHAD: I bet you're gay.\n",
       "LAUNCELOT: No I'm not\n",
       "NARRATOR: Sir Launcelot had saved Sir Galahad from almost certain temptation, but they were still no nearer the Grail.  Meanwhile, King Arthur and Sir Bedevere, not more than a swallow's flight away, had discovered something.  Oh, that's an unladen swallow's flight, obviously.  I mean, they were more than two laden swallows' flights away-- four, really, if they had a coconut on a line between them.  I mean, if the birds were walking and dragging--\n",
       "CROWD: Get on with it!\n",
       "NARRATOR: Oh, anyway.  On to scene twenty-four, which is a smashing scene with some lovely acting, in which Arthur discovers a vital clue, and in which there aren't any swallows, although I think you can hear a starling-- oooh\n",
       "SCENE 12:\n",
       "OLD MAN: Heh, hee ha ha hee hee!  Hee hee hee ha ha ha...\n",
       "ARTHUR: And this enchanter of whom you speak, he has seen the Grail?\n",
       "OLD MAN: ... Ha ha ha ha!  Heh, hee ha ha hee!  Ha hee ha!  Ha ha ha ha...\n",
       "ARTHUR: Where does he live?\n",
       "OLD MAN: ... Heh heh heh heh...\n",
       "ARTHUR: Old man, where does he live?\n",
       "OLD MAN: ... Hee ha ha ha.  He knows of a cave, a cave which no man has entered.\n",
       "ARTHUR: And the Grail.  The Grail is there?\n",
       "OLD MAN: There is much danger, for beyond the cave lies the Gorge of Eternal Peril, which no man has ever crossed.\n",
       "ARTHUR: But the Grail!  Where is the Grail?!\n",
       "OLD MAN: Seek you the Bridge of Death.\n",
       "ARTHUR: The Bridge of Death, which leads to the Grail?\n",
       "OLD MAN: Heh, hee hee hee hee!  Ha ha ha ha ha!  Hee ha ha..\n",
       "SCENE 13: [spooky music] [music stops] \n",
       "HEAD KNIGHT OF NI: Ni!\n",
       "KNIGHTS OF NI: Ni!  Ni!  Ni!  Ni!  Ni!\n",
       "ARTHUR: Who are you?\n",
       "HEAD KNIGHT: We are the Knights Who Say...  'Ni'!\n",
       "RANDOM: Ni!\n",
       "ARTHUR: No!  Not the Knights Who Say 'Ni'!\n",
       "HEAD KNIGHT: The same!\n",
       "BEDEVERE: Who are they?\n",
       "HEAD KNIGHT: We are the keepers of the sacred words: Ni, Peng, and Neee-wom!\n",
       "RANDOM: Neee-wom!\n",
       "ARTHUR: Those who hear them seldom live to tell the tale!\n",
       "HEAD KNIGHT: The Knights Who Say 'Ni' demand a sacrifice!\n",
       "ARTHUR: Knights of Ni, we are but simple travellers who seek the enchanter who lives beyond these woods.\n",
       "HEAD KNIGHT: Ni!\n",
       "KNIGHTS OF NI: Ni!  Ni!  Ni!  Ni!  Ni! ...\n",
       "ARTHUR: Ow!  Ow!  Ow!  Agh!\n",
       "HEAD KNIGHT: We shall say 'ni' again to you if you do not appease us.\n",
       "ARTHUR: Well, what is it you want?\n",
       "HEAD KNIGHT: We want...  a shrubbery! [dramatic chord] \n",
       "ARTHUR: A what?\n",
       "KNIGHTS OF NI: Ni!  Ni!  Ni!  Ni!\n",
       "ARTHUR and PARTY: Ow!  Oh!\n",
       "ARTHUR: Please, please!  No more!  We will find you a shrubbery.\n",
       "HEAD KNIGHT: You must return here with a shrubbery or else you will never pass through this wood alive!\n",
       "ARTHUR: O Knights of Ni, you are just and fair, and we will return with a shrubbery.\n",
       "HEAD KNIGHT: One that looks nice.\n",
       "ARTHUR: Of course.\n",
       "HEAD KNIGHT: And not too expensive.\n",
       "ARTHUR: Yes.\n",
       "HEAD KNIGHT: Now...  go [trumpets] "
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp(holy_grail)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BAG OF WORDS OR FREQUENCY OF WORDS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bag of words is a very simple and basic method to finding topics in a text. For bag of words, you need to first create tokens using tokenization, and then count up all the tokens you have. The theory is that the more frequent a word or token is, the more central or important it might be to the text. Bag of words can be a great way to determine the significant words in a text based on the number of times they are used\n",
    "\n",
    "\n",
    "\n",
    "What is Python Counter?\n",
    "Python Counter is a container that will hold the count of each of the elements present in the container. The counter is a sub-class available inside the dictionary class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "''''Debugging''' is the process of finding and resolving of defects that prevent correct operation of computer software or a system.  \n",
      "\n",
      "Numerous books have been written about debugging (see below: #Further reading|Further reading), as it involves numerous aspects, including interactive debugging, control flow, integration testing, Logfile|log files, monitoring (Application monitoring|application, System Monitoring|system), memory dumps, Profiling (computer programming)|profiling, Statistical Process Control, and special design tactics to improve detection while simplifying changes.\n",
      "\n",
      "Origin\n",
      "A computer log entry from the Mark&nbsp;II, with a moth taped to the page\n",
      "\n",
      "The terms \"bug\" and \"debugging\" are popularly attributed to Admiral Grace Hopper in the 1940s.[http://foldoc.org/Grace+Hopper Grace Hopper]  from FOLDOC While she was working on a Harvard Mark II|Mark II Computer at Harvard University, her associates discovered a moth stuck in a relay and thereby impeding operation, whereupon she remarked that they were \"debugging\" the system. However the term \"bug\" in the meaning of technical error dates back at least to 1878 and Thomas Edison (see software bug for a full discussion), and \"debugging\" seems to have been used as a term in aeronautics before entering the world of computers\n",
      "\n",
      "\n",
      "bag of words are: [(',', 15), ('the', 9), ('a', 8), (\"''\", 7), ('and', 6), ('of', 5), ('debugging', 5), ('to', 5), ('``', 5), ('computer', 4), ('.', 4), ('(', 4), (')', 4), ('in', 4), ('system', 3), ('bug', 3), (\"'\", 2), ('process', 2), ('that', 2), ('operation', 2)]\n"
     ]
    }
   ],
   "source": [
    "article=\"\"\"'\\'\\'\\'Debugging\\'\\'\\' is the process of finding and resolving of defects that prevent correct operation of computer software or a system.  \\n\\nNumerous books have been written about debugging (see below: #Further reading|Further reading), as it involves numerous aspects, including interactive debugging, control flow, integration testing, Logfile|log files, monitoring (Application monitoring|application, System Monitoring|system), memory dumps, Profiling (computer programming)|profiling, Statistical Process Control, and special design tactics to improve detection while simplifying changes.\\n\\nOrigin\\nA computer log entry from the Mark&nbsp;II, with a moth taped to the page\\n\\nThe terms \"bug\" and \"debugging\" are popularly attributed to Admiral Grace Hopper in the 1940s.[http://foldoc.org/Grace+Hopper Grace Hopper]  from FOLDOC While she was working on a Harvard Mark II|Mark II Computer at Harvard University, her associates discovered a moth stuck in a relay and thereby impeding operation, whereupon she remarked that they were \"debugging\" the system. However the term \"bug\" in the meaning of technical error dates back at least to 1878 and Thomas Edison (see software bug for a full discussion), and \"debugging\" seems to have been used as a term in aeronautics before entering the world of computers\"\"\"\n",
    "print(article)\n",
    "# Import Counter\n",
    "from collections import Counter\n",
    "\n",
    "# Tokenize the article: tokens\n",
    "tokens = word_tokenize(article)\n",
    "\n",
    "# Convert the tokens into lowercase: lower_tokens\n",
    "lower_tokens = [t.lower() for t in tokens]\n",
    "\n",
    "# Create a Counter with the lowercase tokens: bow_simple\n",
    "bow_simple = Counter(lower_tokens)\n",
    "\n",
    "# Print the 10 most common tokens\n",
    "print(\"\\n\\nbag of words are:\",bow_simple.most_common(20))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GENSIM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Gensim** is a popular open-source natural language processing library. It uses top academic models to perform complex tasks like building document or word vectors, corpora and performing topic identification and document comparisons."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gensim allows you to build corpora and dictionaries using simple classes and functions. A corpus (or if plural, corpora) is a set of texts used to help perform natural language processing tasks.\n",
    "\n",
    "Using the dictionary we built {which is actually a bag of words}, we can then create a Gensim corpus. This is a bit different than a normal corpus -- which is just a collection of documents. Gensim uses a simple bag-of-words model which transforms each document into a bag of words using the token ids and the frequency of each token in the document. {rather than having words:counts, it will be of the model of (token_id,count),where token_id is the numeric id of each word}\n",
    "\n",
    "And unlike our previous Counter-based bag of words, this Gensim model can be easily saved, updated and reused thanks to the extra tools we have available in Gensim. Our dictionary can also be updated with new texts and extract only words that meet particular thresholds. We are building a more advanced and feature-rich bag-of-words model which can then be used for future exercises.\n",
    "\n",
    "input to dictionary should be a list of list and output of the corpus will also be a list of lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence tokenizing the text: \n",
      "\n",
      "['Destination Canada, a Nations Online country profile of the second-largest country in the world.', 'Canada occupies most of the northern part of North America.', 'The country is bounded by the North Atlantic Ocean on the east, the North Pacific Ocean on the west, and the Arctic Ocean on the north.', 'It borders Alaska (USA) in the west, and twelve US states of the continental part of the United States in the south.', 'Its border with its southern neighbor runs mainly along the 45th parallel (north).', \"It is the world's longest international border between two countries (8,891 km (5,525 mi)).\"]\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "plain_text=\"\"\"Destination Canada, a Nations Online country profile of the second-largest country in the world.\n",
    "Canada occupies most of the northern part of North America. The country is bounded by the North Atlantic Ocean on the east, the North Pacific Ocean on the west, and the Arctic Ocean on the north.\n",
    "It borders Alaska (USA) in the west, and twelve US states of the continental part of the United States in the south. Its border with its southern neighbor runs mainly along the 45th parallel (north). It is the world's longest international border between two countries (8,891 km (5,525 mi)).\"\"\"\n",
    "\n",
    "sentence_tokenized = sent_tokenize(plain_text)\n",
    "print(\"Sentence tokenizing the text: \\n\")\n",
    "print(sentence_tokenized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### we can either below two methods to convert a list of strings into 2d[list of list] list of tokenized words\n",
    "\n",
    "####WE CANT USE NORMAL WORD TOKENIZER HERE BECAUSE, IT NEEDS A STRING RATHER THAN A LIST OF STRINGS...HENCE THIS OR WE .LOWER()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "tokenized lines:\n",
      " [['Destination', 'Canada', 'a', 'Nations', 'Online', 'country', 'profile', 'of', 'the', 'second', 'largest', 'country', 'in', 'the', 'world'], ['Canada', 'occupies', 'most', 'of', 'the', 'northern', 'part', 'of', 'North', 'America'], ['The', 'country', 'is', 'bounded', 'by', 'the', 'North', 'Atlantic', 'Ocean', 'on', 'the', 'east', 'the', 'North', 'Pacific', 'Ocean', 'on', 'the', 'west', 'and', 'the', 'Arctic', 'Ocean', 'on', 'the', 'north'], ['It', 'borders', 'Alaska', 'USA', 'in', 'the', 'west', 'and', 'twelve', 'US', 'states', 'of', 'the', 'continental', 'part', 'of', 'the', 'United', 'States', 'in', 'the', 'south'], ['Its', 'border', 'with', 'its', 'southern', 'neighbor', 'runs', 'mainly', 'along', 'the', '45th', 'parallel', 'north'], ['It', 'is', 'the', 'world', 's', 'longest', 'international', 'border', 'between', 'two', 'countries', '8', '891', 'km', '5', '525', 'mi']]\n"
     ]
    }
   ],
   "source": [
    "tokenized_lines = [regexp_tokenize(s, \"\\w+\") for s in sentence_tokenized]\n",
    "print(\"\\n\\ntokenized lines:\\n\",tokenized_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word tokenizing the text: \n",
      "\n",
      "[['Destination', 'Canada', ',', 'a', 'Nations', 'Online', 'country', 'profile', 'of', 'the', 'second-largest', 'country', 'in', 'the', 'world', '.'], ['Canada', 'occupies', 'most', 'of', 'the', 'northern', 'part', 'of', 'North', 'America', '.'], ['The', 'country', 'is', 'bounded', 'by', 'the', 'North', 'Atlantic', 'Ocean', 'on', 'the', 'east', ',', 'the', 'North', 'Pacific', 'Ocean', 'on', 'the', 'west', ',', 'and', 'the', 'Arctic', 'Ocean', 'on', 'the', 'north', '.'], ['It', 'borders', 'Alaska', '(', 'USA', ')', 'in', 'the', 'west', ',', 'and', 'twelve', 'US', 'states', 'of', 'the', 'continental', 'part', 'of', 'the', 'United', 'States', 'in', 'the', 'south', '.'], ['Its', 'border', 'with', 'its', 'southern', 'neighbor', 'runs', 'mainly', 'along', 'the', '45th', 'parallel', '(', 'north', ')', '.'], ['It', 'is', 'the', 'world', \"'s\", 'longest', 'international', 'border', 'between', 'two', 'countries', '(', '8,891', 'km', '(', '5,525', 'mi', ')', ')', '.']]\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "tokenized_docs = [word_tokenize(t) for t in sentence_tokenized]    # if we put word_tokenize(t.alpha()) we can remove , ! ' etc \n",
    "print(\"Word tokenizing the text: \\n\")\n",
    "print(tokenized_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nfor i in range(len(corpus )):\\n    corpus [i] = corpus [i].lower()\\n    corpus [i] = re.sub(r'\\\\W',' ',corpus [i])\\n    corpus [i] = re.sub(r'\\\\s+',' ',corpus [i]) \""
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#this is an another way to preprocess the string\n",
    "\"\"\"\n",
    "for i in range(len(corpus )):\n",
    "    corpus [i] = corpus [i].lower()\n",
    "    corpus [i] = re.sub(r'\\W',' ',corpus [i])\n",
    "    corpus [i] = re.sub(r'\\s+',' ',corpus [i]) \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(corpus )):\n",
    "    corpus [i] = corpus [i].lower()\n",
    "    corpus [i] = re.sub(r'\\W',' ',corpus [i])\n",
    "    corpus [i] = re.sub(r'\\s+',' ',corpus [i]) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['destination canada a nations online country profile of the second largest country in the world ',\n",
       " 'canada occupies most of the northern part of north america ',\n",
       " 'the country is bounded by the north atlantic ocean on the east the north pacific ocean on the west and the arctic ocean on the north ',\n",
       " 'it borders alaska usa in the west and twelve us states of the continental part of the united states in the south ',\n",
       " 'its border with its southern neighbor runs mainly along the 45th parallel north ',\n",
       " 'it is the world s longest international border between two countries 8 891 km 5 525 mi ']"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### INTO THE DICTIONARY WE SHOULD PASS ARGUMENT AS A LIST OF LIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<gensim.corpora.dictionary.Dictionary at 0x2588ba0e760>"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(66 unique tokens: ['Canada', 'Destination', 'Nations', 'Online', 'a']...)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Canada': 0,\n",
       " 'Destination': 1,\n",
       " 'Nations': 2,\n",
       " 'Online': 3,\n",
       " 'a': 4,\n",
       " 'country': 5,\n",
       " 'in': 6,\n",
       " 'largest': 7,\n",
       " 'of': 8,\n",
       " 'profile': 9,\n",
       " 'second': 10,\n",
       " 'the': 11,\n",
       " 'world': 12,\n",
       " 'America': 13,\n",
       " 'North': 14,\n",
       " 'most': 15,\n",
       " 'northern': 16,\n",
       " 'occupies': 17,\n",
       " 'part': 18,\n",
       " 'Arctic': 19,\n",
       " 'Atlantic': 20,\n",
       " 'Ocean': 21,\n",
       " 'Pacific': 22,\n",
       " 'The': 23,\n",
       " 'and': 24,\n",
       " 'bounded': 25,\n",
       " 'by': 26,\n",
       " 'east': 27,\n",
       " 'is': 28,\n",
       " 'north': 29,\n",
       " 'on': 30,\n",
       " 'west': 31,\n",
       " 'Alaska': 32,\n",
       " 'It': 33,\n",
       " 'States': 34,\n",
       " 'US': 35,\n",
       " 'USA': 36,\n",
       " 'United': 37,\n",
       " 'borders': 38,\n",
       " 'continental': 39,\n",
       " 'south': 40,\n",
       " 'states': 41,\n",
       " 'twelve': 42,\n",
       " '45th': 43,\n",
       " 'Its': 44,\n",
       " 'along': 45,\n",
       " 'border': 46,\n",
       " 'its': 47,\n",
       " 'mainly': 48,\n",
       " 'neighbor': 49,\n",
       " 'parallel': 50,\n",
       " 'runs': 51,\n",
       " 'southern': 52,\n",
       " 'with': 53,\n",
       " '5': 54,\n",
       " '525': 55,\n",
       " '8': 56,\n",
       " '891': 57,\n",
       " 'between': 58,\n",
       " 'countries': 59,\n",
       " 'international': 60,\n",
       " 'km': 61,\n",
       " 'longest': 62,\n",
       " 'mi': 63,\n",
       " 's': 64,\n",
       " 'two': 65}"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import Dictionary\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "\n",
    "# Create a Dictionary from the articles: dictionary----INPUT IS TOKENIZED LIST OF LIST\n",
    "dictionary = Dictionary(tokenized_lines)\n",
    "print(dictionary)\n",
    "dictionary.token2id   #displays the ids of each word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(0, 1),\n",
       "  (1, 1),\n",
       "  (2, 1),\n",
       "  (3, 1),\n",
       "  (4, 1),\n",
       "  (5, 2),\n",
       "  (6, 1),\n",
       "  (7, 1),\n",
       "  (8, 1),\n",
       "  (9, 1),\n",
       "  (10, 1),\n",
       "  (11, 2),\n",
       "  (12, 1)],\n",
       " [(0, 1),\n",
       "  (8, 2),\n",
       "  (11, 1),\n",
       "  (13, 1),\n",
       "  (14, 1),\n",
       "  (15, 1),\n",
       "  (16, 1),\n",
       "  (17, 1),\n",
       "  (18, 1)],\n",
       " [(5, 1),\n",
       "  (11, 6),\n",
       "  (14, 2),\n",
       "  (19, 1),\n",
       "  (20, 1),\n",
       "  (21, 3),\n",
       "  (22, 1),\n",
       "  (23, 1),\n",
       "  (24, 1),\n",
       "  (25, 1),\n",
       "  (26, 1),\n",
       "  (27, 1),\n",
       "  (28, 1),\n",
       "  (29, 1),\n",
       "  (30, 3),\n",
       "  (31, 1)],\n",
       " [(6, 2),\n",
       "  (8, 2),\n",
       "  (11, 4),\n",
       "  (18, 1),\n",
       "  (24, 1),\n",
       "  (31, 1),\n",
       "  (32, 1),\n",
       "  (33, 1),\n",
       "  (34, 1),\n",
       "  (35, 1),\n",
       "  (36, 1),\n",
       "  (37, 1),\n",
       "  (38, 1),\n",
       "  (39, 1),\n",
       "  (40, 1),\n",
       "  (41, 1),\n",
       "  (42, 1)],\n",
       " [(11, 1),\n",
       "  (29, 1),\n",
       "  (43, 1),\n",
       "  (44, 1),\n",
       "  (45, 1),\n",
       "  (46, 1),\n",
       "  (47, 1),\n",
       "  (48, 1),\n",
       "  (49, 1),\n",
       "  (50, 1),\n",
       "  (51, 1),\n",
       "  (52, 1),\n",
       "  (53, 1)],\n",
       " [(11, 1),\n",
       "  (12, 1),\n",
       "  (28, 1),\n",
       "  (33, 1),\n",
       "  (46, 1),\n",
       "  (54, 1),\n",
       "  (55, 1),\n",
       "  (56, 1),\n",
       "  (57, 1),\n",
       "  (58, 1),\n",
       "  (59, 1),\n",
       "  (60, 1),\n",
       "  (61, 1),\n",
       "  (62, 1),\n",
       "  (63, 1),\n",
       "  (64, 1),\n",
       "  (65, 1)]]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = [dictionary.doc2bow(cor) for cor in tokenized_lines]    #converts the dictionary into a corpus or bag of words\n",
    "corpus                              #corpus is a list of lists..input and output is both a list of lits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(11, 1), (29, 1), (43, 1), (44, 1), (45, 1), (46, 1), (47, 1), (48, 1), (49, 1), (50, 1), (51, 1), (52, 1), (53, 1)]\n",
      "[(11, 1), (29, 1), (43, 1), (44, 1), (45, 1), (46, 1), (47, 1), (48, 1), (49, 1), (50, 1)]\n",
      "50\n",
      "parallel\n"
     ]
    }
   ],
   "source": [
    "print(corpus[4])\n",
    "print(corpus[4][:10])\n",
    "parallel_id = dictionary.token2id.get(\"parallel\")\n",
    "print(parallel_id)\n",
    "print(dictionary.get(parallel_id))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28\n"
     ]
    }
   ],
   "source": [
    "parallel_id = dictionary.token2id.get(\"is\")\n",
    "print(parallel_id)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "defaultdict allows us to initialize a dictionary that will assign a default value to non-existent keys. By supplying the argument int, we are able to ensure that any non-existent keys are automatically assigned a default value of 0. This makes it ideal for storing the counts of words in this exercise.\n",
    "\n",
    "itertools.chain.from_iterable() allows us to iterate through a set of sequences as if they were one continuous sequence. Using this function, we can easily iterate through our corpus object (which is a list of lists)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"\\n####\\n# Sort the doc for frequency: bow_doc\\nbow_doc = sorted(doc, key=lambda w: w[1], reverse=True)\\n\\n# Print the top 5 words of the document alongside the count\\nfor word_id, word_count in bow_doc[:5]:\\n    print(dictionary.get(word_id), word_count)\\n    \\n# Create the defaultdict: total_word_count\\ntotal_word_count = defaultdict(int)\\nfor word_id, word_count in itertools.chain.from_iterable(corpus):\\n    total_word_count[word_id] += word_count\\n\\n# Create a sorted list from the defaultdict: sorted_word_count \\nsorted_word_count = sorted(total_word_count.items(), key=lambda w: w[1], reverse=True) \\n\\n# Print the top 5 words across all documents alongside the count\\nfor word_id, word_count in sorted_word_count[:5]:\\n    print(dictionary.get(word_id), word_count)\\n'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " \n",
    "# Save the fifth document: doc\n",
    "doc = corpus[4]\n",
    "\"\"\"\"\n",
    "####\n",
    "# Sort the doc for frequency: bow_doc\n",
    "bow_doc = sorted(doc, key=lambda w: w[1], reverse=True)\n",
    "\n",
    "# Print the top 5 words of the document alongside the count\n",
    "for word_id, word_count in bow_doc[:5]:\n",
    "    print(dictionary.get(word_id), word_count)\n",
    "    \n",
    "# Create the defaultdict: total_word_count\n",
    "total_word_count = defaultdict(int)\n",
    "for word_id, word_count in itertools.chain.from_iterable(corpus):\n",
    "    total_word_count[word_id] += word_count\n",
    "\n",
    "# Create a sorted list from the defaultdict: sorted_word_count \n",
    "sorted_word_count = sorted(total_word_count.items(), key=lambda w: w[1], reverse=True) \n",
    "\n",
    "# Print the top 5 words across all documents alongside the count\n",
    "for word_id, word_count in sorted_word_count[:5]:\n",
    "    print(dictionary.get(word_id), word_count)\n",
    "\"\"\"\n",
    "###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "#WORD VECTORS USING GENSIM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TFIDF----------------------\n",
    "##FIDF model with Gensim.\n",
    "\n",
    "**Tf-idf** stands for term-frequncy - inverse document frequency. It is a commonly used natural language processing model that helps you determine the most important words in each document in the corpus. The idea behind tf-idf is that each corpus might have more shared words than just stopwords. These common words are like stopwords and should be removed or at least down-weighted in importance. For example, if I am an astronomer, sky might be used often but is not important, so I want to downweight that word. TF-Idf does precisely that. It will take texts that share common language and ensure the most common words across the entire corpus don't show up as keywords. Tf-idf helps keep the document-specific frequent words weighted high and the common words across the entire corpus weighted low.\n",
    "\n",
    "HOW DOES THIS WORK??\n",
    "the weight will be low if the term doesnt appear often in the document because the tf variable will then be low. However, the weight will also be a low if the logarithm is close to zero, meaning the internal equation is low. Here we can see if the total number of documents divded by the number of documents that have the term is close to one, then our logarithm will be close to zero. So words that occur across many or all documents will have a very low tf-idf weight. On the contrary, if the word only occurs in a few documents, that logarithm will return a higher number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 2), (6, 1), (7, 1), (8, 1), (9, 1), (10, 1), (11, 2), (12, 1)], [(0, 1), (8, 2), (11, 1), (13, 1), (14, 1), (15, 1), (16, 1), (17, 1), (18, 1)], [(5, 1), (11, 6), (14, 2), (19, 1), (20, 1), (21, 3), (22, 1), (23, 1), (24, 1), (25, 1), (26, 1), (27, 1), (28, 1), (29, 1), (30, 3), (31, 1)], [(6, 2), (8, 2), (11, 4), (18, 1), (24, 1), (31, 1), (32, 1), (33, 1), (34, 1), (35, 1), (36, 1), (37, 1), (38, 1), (39, 1), (40, 1), (41, 1), (42, 1)], [(11, 1), (29, 1), (43, 1), (44, 1), (45, 1), (46, 1), (47, 1), (48, 1), (49, 1), (50, 1), (51, 1), (52, 1), (53, 1)], [(11, 1), (12, 1), (28, 1), (33, 1), (46, 1), (54, 1), (55, 1), (56, 1), (57, 1), (58, 1), (59, 1), (60, 1), (61, 1), (62, 1), (63, 1), (64, 1), (65, 1)]]\n",
      "[(29, 0.18699162553358545), (43, 0.30497020575141676), (44, 0.30497020575141676), (45, 0.30497020575141676), (46, 0.18699162553358545)]\n",
      "45th 0.30497020575141676\n",
      "Its 0.30497020575141676\n",
      "along 0.30497020575141676\n",
      "its 0.30497020575141676\n",
      "mainly 0.30497020575141676\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import TfidfModel\n",
    "print(corpus)\n",
    "doc = corpus[4]\n",
    "\n",
    "# Create a new TfidfModel using the corpus: tfidf\n",
    "tfidf = TfidfModel(corpus)\n",
    "\n",
    "# Calculate the tfidf weights of doc: tfidf_weights\n",
    "tfidf_weights = tfidf[doc]\n",
    "\n",
    "# Print the first five weights\n",
    "print(tfidf_weights[:5])\n",
    "\n",
    "# Sort the weights from highest to lowest: sorted_tfidf_weights\n",
    "sorted_tfidf_weights = sorted(tfidf_weights, key=lambda w: w[1], reverse=True)\n",
    "\n",
    "# Print the top 5 weighted words\n",
    "for term_id, weight in sorted_tfidf_weights[:5]:\n",
    "    print(dictionary.get(term_id), weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://medium.com/analytics-vidhya/tf-idf-term-frequency-technique-easiest-explanation-for-text-classification-in-nlp-with-code-8ca3912e58c3\n",
    "\n",
    "https://www.geeksforgeeks.org/tf-idf-model-for-page-ranking/\n",
    "\n",
    "https://stackoverflow.com/questions/53294482/how-to-get-tf-idf-scores-for-the-words/53294731"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NER   [i think its only nltk]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Named Entity Recognition?\n",
    "Named Entity Recognition or NER for short is a natural language processing task used to identify important named entities in the text -- such as people, places and organizations -- they can even be dates, states, works of art and other categories depending on the libraries and notation you use. NER can be used alongside topic identification, or on its own to determine important items in a text or answer basic natural language understanding questions such as who? what? when and where?\n",
    "\n",
    "\n",
    "\n",
    "NLTK allows you to interact with named entity recognition via it's own model, but also the aforementioned Stanford library. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     C:\\Users\\ANTHONY\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\ANTHONY\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Trudeau', 'added', 'that', 'there', 'will', 'be', 'exceptions', 'to', 'these', 'new', 'restrictions', ',', 'particularly', 'for', 'truckers', 'and', 'health-care', 'workers', 'travelling', 'into', 'Canada', '.'], ['â€œ', 'We', 'â€™', 're', 'not', 'trying', 'to', 'punish', 'people', ',', 'we', 'â€™', 're', 'trying', 'to', 'keep', 'people', 'safe', ',', 'â€', 'he', 'said.', 'â€œ', 'These', 'border', 'measures', 'will', 'help', 'stop', 'the', 'spread', 'of', 'COVID-19', 'and', 'new', 'variants.', 'â€', 'In', 'late', 'January', ',', 'Trudeau', 'announced', 'that', 'travellers', 'arriving', 'in', 'the', 'country', 'by', 'air', 'will', 'have', 'to', 'take', 'a', 'mandatory', 'PCR', 'coronavirus', 'test', '.'], ['While', 'they', 'await', 'the', 'results', 'of', 'that', 'test', ',', 'they', 'will', 'be', 'forced', 'to', 'quarantine', 'at', 'a', 'hotel', 'for', 'up', 'to', 'three', 'days', 'â€”', 'on', 'their', 'own', 'dime', '.']]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[('Trudeau', 'NNP'),\n",
       "  ('added', 'VBD'),\n",
       "  ('that', 'IN'),\n",
       "  ('there', 'EX'),\n",
       "  ('will', 'MD'),\n",
       "  ('be', 'VB'),\n",
       "  ('exceptions', 'NNS'),\n",
       "  ('to', 'TO'),\n",
       "  ('these', 'DT'),\n",
       "  ('new', 'JJ'),\n",
       "  ('restrictions', 'NNS'),\n",
       "  (',', ','),\n",
       "  ('particularly', 'RB'),\n",
       "  ('for', 'IN'),\n",
       "  ('truckers', 'NNS'),\n",
       "  ('and', 'CC'),\n",
       "  ('health-care', 'JJ'),\n",
       "  ('workers', 'NNS'),\n",
       "  ('travelling', 'VBG'),\n",
       "  ('into', 'IN'),\n",
       "  ('Canada', 'NNP'),\n",
       "  ('.', '.')],\n",
       " [('â€œ', 'NN'),\n",
       "  ('We', 'PRP'),\n",
       "  ('â€™', 'VBP'),\n",
       "  ('re', 'JJ'),\n",
       "  ('not', 'RB'),\n",
       "  ('trying', 'VBG'),\n",
       "  ('to', 'TO'),\n",
       "  ('punish', 'VB'),\n",
       "  ('people', 'NNS'),\n",
       "  (',', ','),\n",
       "  ('we', 'PRP'),\n",
       "  ('â€™', 'VBP'),\n",
       "  ('re', 'VB'),\n",
       "  ('trying', 'VBG'),\n",
       "  ('to', 'TO'),\n",
       "  ('keep', 'VB'),\n",
       "  ('people', 'NNS'),\n",
       "  ('safe', 'JJ'),\n",
       "  (',', ','),\n",
       "  ('â€', 'EX'),\n",
       "  ('he', 'PRP'),\n",
       "  ('said.', 'VBD'),\n",
       "  ('â€œ', 'NNP'),\n",
       "  ('These', 'DT'),\n",
       "  ('border', 'NN'),\n",
       "  ('measures', 'NNS'),\n",
       "  ('will', 'MD'),\n",
       "  ('help', 'VB'),\n",
       "  ('stop', 'VB'),\n",
       "  ('the', 'DT'),\n",
       "  ('spread', 'NN'),\n",
       "  ('of', 'IN'),\n",
       "  ('COVID-19', 'NNP'),\n",
       "  ('and', 'CC'),\n",
       "  ('new', 'JJ'),\n",
       "  ('variants.', 'NN'),\n",
       "  ('â€', 'NN'),\n",
       "  ('In', 'IN'),\n",
       "  ('late', 'JJ'),\n",
       "  ('January', 'NNP'),\n",
       "  (',', ','),\n",
       "  ('Trudeau', 'NNP'),\n",
       "  ('announced', 'VBD'),\n",
       "  ('that', 'IN'),\n",
       "  ('travellers', 'NNS'),\n",
       "  ('arriving', 'VBG'),\n",
       "  ('in', 'IN'),\n",
       "  ('the', 'DT'),\n",
       "  ('country', 'NN'),\n",
       "  ('by', 'IN'),\n",
       "  ('air', 'NN'),\n",
       "  ('will', 'MD'),\n",
       "  ('have', 'VB'),\n",
       "  ('to', 'TO'),\n",
       "  ('take', 'VB'),\n",
       "  ('a', 'DT'),\n",
       "  ('mandatory', 'JJ'),\n",
       "  ('PCR', 'NNP'),\n",
       "  ('coronavirus', 'NN'),\n",
       "  ('test', 'NN'),\n",
       "  ('.', '.')],\n",
       " [('While', 'IN'),\n",
       "  ('they', 'PRP'),\n",
       "  ('await', 'VBP'),\n",
       "  ('the', 'DT'),\n",
       "  ('results', 'NNS'),\n",
       "  ('of', 'IN'),\n",
       "  ('that', 'DT'),\n",
       "  ('test', 'NN'),\n",
       "  (',', ','),\n",
       "  ('they', 'PRP'),\n",
       "  ('will', 'MD'),\n",
       "  ('be', 'VB'),\n",
       "  ('forced', 'VBN'),\n",
       "  ('to', 'TO'),\n",
       "  ('quarantine', 'VB'),\n",
       "  ('at', 'IN'),\n",
       "  ('a', 'DT'),\n",
       "  ('hotel', 'NN'),\n",
       "  ('for', 'IN'),\n",
       "  ('up', 'IN'),\n",
       "  ('to', 'TO'),\n",
       "  ('three', 'CD'),\n",
       "  ('days', 'NNS'),\n",
       "  ('â€”', 'VBP'),\n",
       "  ('on', 'IN'),\n",
       "  ('their', 'PRP$'),\n",
       "  ('own', 'JJ'),\n",
       "  ('dime', 'NN'),\n",
       "  ('.', '.')]]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "plain_news=\"\"\"Trudeau added that there will be exceptions to these new restrictions, particularly for truckers and health-care workers travelling into Canada.\n",
    "â€œWeâ€™re not trying to punish people, weâ€™re trying to keep people safe,â€ he said.â€œThese border measures will help stop the spread of COVID-19 and new variants.â€\n",
    "In late January, Trudeau announced that travellers arriving in the country by air will have to take a mandatory PCR coronavirus test. While they await the results of that test, they will be forced to quarantine at a hotel for up to three days â€” on their own dime.\"\"\"\n",
    "\n",
    "# Tokenize the article into sentences: sentences\n",
    "sentences = nltk.sent_tokenize(plain_news)\n",
    "\n",
    "# Tokenize each sentence into words: token_sentences\n",
    "token_sentences = [nltk.word_tokenize(sent) for sent in sentences]\n",
    "print(token_sentences)\n",
    "\n",
    "\n",
    "# Tag each tokenized sentence into parts of speech: pos_sentences\n",
    "pos_sentences = [nltk.pos_tag(sent) for sent in token_sentences] \n",
    "\n",
    "pos_sentences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'VBD'"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_sentences[0][1][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (NE Trudeau/NNP)\n",
      "  added/VBD\n",
      "  that/IN\n",
      "  there/EX\n",
      "  will/MD\n",
      "  be/VB\n",
      "  exceptions/NNS\n",
      "  to/TO\n",
      "  these/DT\n",
      "  new/JJ\n",
      "  restrictions/NNS\n",
      "  ,/,\n",
      "  particularly/RB\n",
      "  for/IN\n",
      "  truckers/NNS\n",
      "  and/CC\n",
      "  health-care/JJ\n",
      "  workers/NNS\n",
      "  travelling/VBG\n",
      "  into/IN\n",
      "  (NE Canada/NNP)\n",
      "  ./.)\n",
      "(S\n",
      "  â€œ/NN\n",
      "  We/PRP\n",
      "  â€™/VBP\n",
      "  re/JJ\n",
      "  not/RB\n",
      "  trying/VBG\n",
      "  to/TO\n",
      "  punish/VB\n",
      "  people/NNS\n",
      "  ,/,\n",
      "  we/PRP\n",
      "  â€™/VBP\n",
      "  re/VB\n",
      "  trying/VBG\n",
      "  to/TO\n",
      "  keep/VB\n",
      "  people/NNS\n",
      "  safe/JJ\n",
      "  ,/,\n",
      "  â€/EX\n",
      "  he/PRP\n",
      "  said./VBD\n",
      "  â€œ/NNP\n",
      "  These/DT\n",
      "  border/NN\n",
      "  measures/NNS\n",
      "  will/MD\n",
      "  help/VB\n",
      "  stop/VB\n",
      "  the/DT\n",
      "  spread/NN\n",
      "  of/IN\n",
      "  COVID-19/NNP\n",
      "  and/CC\n",
      "  new/JJ\n",
      "  variants./NN\n",
      "  â€/NN\n",
      "  In/IN\n",
      "  late/JJ\n",
      "  January/NNP\n",
      "  ,/,\n",
      "  (NE Trudeau/NNP)\n",
      "  announced/VBD\n",
      "  that/IN\n",
      "  travellers/NNS\n",
      "  arriving/VBG\n",
      "  in/IN\n",
      "  the/DT\n",
      "  country/NN\n",
      "  by/IN\n",
      "  air/NN\n",
      "  will/MD\n",
      "  have/VB\n",
      "  to/TO\n",
      "  take/VB\n",
      "  a/DT\n",
      "  mandatory/JJ\n",
      "  (NE PCR/NNP)\n",
      "  coronavirus/NN\n",
      "  test/NN\n",
      "  ./.)\n",
      "(S\n",
      "  While/IN\n",
      "  they/PRP\n",
      "  await/VBP\n",
      "  the/DT\n",
      "  results/NNS\n",
      "  of/IN\n",
      "  that/DT\n",
      "  test/NN\n",
      "  ,/,\n",
      "  they/PRP\n",
      "  will/MD\n",
      "  be/VB\n",
      "  forced/VBN\n",
      "  to/TO\n",
      "  quarantine/VB\n",
      "  at/IN\n",
      "  a/DT\n",
      "  hotel/NN\n",
      "  for/IN\n",
      "  up/IN\n",
      "  to/TO\n",
      "  three/CD\n",
      "  days/NNS\n",
      "  â€”/VBP\n",
      "  on/IN\n",
      "  their/PRP$\n",
      "  own/JJ\n",
      "  dime/NN\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "# Create the named entity chunks: chunked_sentences\n",
    "#we pass this tagged sentence into the ne_chunk function, or named entity chunk, which will return the sentence as a tree.\n",
    "#NLTK Tree's might look a bit different than trees you might use in other libraries, but they do have leaves and subtrees representing more complex grammar.\n",
    "\n",
    "chunked_sentences = nltk.ne_chunk_sents(pos_sentences, binary=True)\n",
    "\n",
    "chunked_sentences    #cant print normally cause its a generator object\n",
    "\n",
    "for i in chunked_sentences: \n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test for stems of the tree with 'NE' tags\n",
    "for sent in chunked_sentences:\n",
    "    for chunk in sent:\n",
    "        if hasattr(chunk, \"label\") and chunk.label() == \"NNP\":\n",
    "            print(chunk)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'ï»¿The taxi-hailing company Uber brings into very sharp focus the question of whether corporations can be said to have a moral character. If any human being were to behave with the single-minded and ruthless greed of the company, we would consider them sociopathic. Uber wanted to know as much as possible about the people who use its service, and those who donâ€™t. It has an arrangement with unroll.me, a company which offered a free service for unsubscribing from junk mail, to buy the contacts unroll.me customers had had with rival taxi companies. Even if their email was notionally anonymised, this use of it was not something the users had bargained for. Beyond that, it keeps track of the phones that have been used to summon its services even after the original owner has sold them, attempting this with Appleâ€™s phones even thought it is forbidden by the company.\r\n",
      "\r\n",
      "\r\n",
      "Uber has also tweaked its software so that regulatory agencies that the company regarded as hostile would, when they tried to hire a driver, be given false reports about the location of its cars. Uber management booked and then cancelled rides with a rival taxi-hailing company which took their vehicles out of circulation. Uber deny this was the intention. The punishment for this behaviour was negligible. Uber promised not to use this â€œgreyballâ€ software against law enforcement â€“ one wonders what would happen to someone carrying a knife who promised never to stab a policeman with it. Travis Kalanick of Uber got a personal dressing down from Tim Cook, who runs Apple, but the company did not prohibit the use of the app. Too much money was at stake for that.\r\n",
      "\r\n",
      "\r\n",
      "Millions of people around the world value the cheapness and convenience of Uberâ€™s rides too much to care about the lack of driversâ€™ rights or pay. Many of the users themselves are not much richer than the drivers. The â€œsharing economyâ€ encourages the insecure and exploited to exploit others equally insecure to the profit of a tiny clique of billionaires. Silicon Valleyâ€™s culture seems hostile to humane and democratic values. The outgoing CEO of Yahoo, Marissa Mayer, who is widely judged to have been a failure, is likely to get a $186m payout. This may not be a cause for panic, any more than the previous hero worship should have been a cause for euphoria. Yet thereâ€™s an urgent political task to tame these companies, to ensure they are punished when they break the law, that they pay their taxes fairly and that they behave responsibly.'\n",
      "\n",
      "[[\"'\\ufeffThe\", 'taxi-hailing', 'company', 'Uber', 'brings', 'into', 'very', 'sharp', 'focus', 'the', 'question', 'of', 'whether', 'corporations', 'can', 'be', 'said', 'to', 'have', 'a', 'moral', 'character', '.'], ['If', 'any', 'human', 'being', 'were', 'to', 'behave', 'with', 'the', 'single-minded', 'and', 'ruthless', 'greed', 'of', 'the', 'company', ',', 'we', 'would', 'consider', 'them', 'sociopathic', '.'], ['Uber', 'wanted', 'to', 'know', 'as', 'much', 'as', 'possible', 'about', 'the', 'people', 'who', 'use', 'its', 'service', ',', 'and', 'those', 'who', 'don', 'â€™', 't', '.'], ['It', 'has', 'an', 'arrangement', 'with', 'unroll.me', ',', 'a', 'company', 'which', 'offered', 'a', 'free', 'service', 'for', 'unsubscribing', 'from', 'junk', 'mail', ',', 'to', 'buy', 'the', 'contacts', 'unroll.me', 'customers', 'had', 'had', 'with', 'rival', 'taxi', 'companies', '.'], ['Even', 'if', 'their', 'email', 'was', 'notionally', 'anonymised', ',', 'this', 'use', 'of', 'it', 'was', 'not', 'something', 'the', 'users', 'had', 'bargained', 'for', '.'], ['Beyond', 'that', ',', 'it', 'keeps', 'track', 'of', 'the', 'phones', 'that', 'have', 'been', 'used', 'to', 'summon', 'its', 'services', 'even', 'after', 'the', 'original', 'owner', 'has', 'sold', 'them', ',', 'attempting', 'this', 'with', 'Apple', 'â€™', 's', 'phones', 'even', 'thought', 'it', 'is', 'forbidden', 'by', 'the', 'company', '.'], ['Uber', 'has', 'also', 'tweaked', 'its', 'software', 'so', 'that', 'regulatory', 'agencies', 'that', 'the', 'company', 'regarded', 'as', 'hostile', 'would', ',', 'when', 'they', 'tried', 'to', 'hire', 'a', 'driver', ',', 'be', 'given', 'false', 'reports', 'about', 'the', 'location', 'of', 'its', 'cars', '.'], ['Uber', 'management', 'booked', 'and', 'then', 'cancelled', 'rides', 'with', 'a', 'rival', 'taxi-hailing', 'company', 'which', 'took', 'their', 'vehicles', 'out', 'of', 'circulation', '.'], ['Uber', 'deny', 'this', 'was', 'the', 'intention', '.'], ['The', 'punishment', 'for', 'this', 'behaviour', 'was', 'negligible', '.'], ['Uber', 'promised', 'not', 'to', 'use', 'this', 'â€œ', 'greyball', 'â€', 'software', 'against', 'law', 'enforcement', 'â€“', 'one', 'wonders', 'what', 'would', 'happen', 'to', 'someone', 'carrying', 'a', 'knife', 'who', 'promised', 'never', 'to', 'stab', 'a', 'policeman', 'with', 'it', '.'], ['Travis', 'Kalanick', 'of', 'Uber', 'got', 'a', 'personal', 'dressing', 'down', 'from', 'Tim', 'Cook', ',', 'who', 'runs', 'Apple', ',', 'but', 'the', 'company', 'did', 'not', 'prohibit', 'the', 'use', 'of', 'the', 'app', '.'], ['Too', 'much', 'money', 'was', 'at', 'stake', 'for', 'that', '.'], ['Millions', 'of', 'people', 'around', 'the', 'world', 'value', 'the', 'cheapness', 'and', 'convenience', 'of', 'Uber', 'â€™', 's', 'rides', 'too', 'much', 'to', 'care', 'about', 'the', 'lack', 'of', 'drivers', 'â€™', 'rights', 'or', 'pay', '.'], ['Many', 'of', 'the', 'users', 'themselves', 'are', 'not', 'much', 'richer', 'than', 'the', 'drivers', '.'], ['The', 'â€œ', 'sharing', 'economy', 'â€', 'encourages', 'the', 'insecure', 'and', 'exploited', 'to', 'exploit', 'others', 'equally', 'insecure', 'to', 'the', 'profit', 'of', 'a', 'tiny', 'clique', 'of', 'billionaires', '.'], ['Silicon', 'Valley', 'â€™', 's', 'culture', 'seems', 'hostile', 'to', 'humane', 'and', 'democratic', 'values', '.'], ['The', 'outgoing', 'CEO', 'of', 'Yahoo', ',', 'Marissa', 'Mayer', ',', 'who', 'is', 'widely', 'judged', 'to', 'have', 'been', 'a', 'failure', ',', 'is', 'likely', 'to', 'get', 'a', '$', '186m', 'payout', '.'], ['This', 'may', 'not', 'be', 'a', 'cause', 'for', 'panic', ',', 'any', 'more', 'than', 'the', 'previous', 'hero', 'worship', 'should', 'have', 'been', 'a', 'cause', 'for', 'euphoria', '.'], ['Yet', 'there', 'â€™', 's', 'an', 'urgent', 'political', 'task', 'to', 'tame', 'these', 'companies', ',', 'to', 'ensure', 'they', 'are', 'punished', 'when', 'they', 'break', 'the', 'law', ',', 'that', 'they', 'pay', 'their', 'taxes', 'fairly', 'and', 'that', 'they', 'behave', 'responsibly', '.', \"'\"]]\n",
      "(NE Uber/NNP)\n",
      "(NE Beyond/NN)\n",
      "(NE Apple/NNP)\n",
      "(NE Uber/NNP)\n",
      "(NE Uber/NNP)\n",
      "(NE Travis/NNP Kalanick/NNP)\n",
      "(NE Tim/NNP Cook/NNP)\n",
      "(NE Apple/NNP)\n",
      "(NE Silicon/NNP Valley/NNP)\n",
      "(NE CEO/NNP)\n",
      "(NE Yahoo/NNP)\n",
      "(NE Marissa/NNP Mayer/NNP)\n"
     ]
    }
   ],
   "source": [
    "articl=\"\"\"'\\ufeffThe taxi-hailing company Uber brings into very sharp focus the question of whether corporations can be said to have a moral character. If any human being were to behave with the single-minded and ruthless greed of the company, we would consider them sociopathic. Uber wanted to know as much as possible about the people who use its service, and those who donâ€™t. It has an arrangement with unroll.me, a company which offered a free service for unsubscribing from junk mail, to buy the contacts unroll.me customers had had with rival taxi companies. Even if their email was notionally anonymised, this use of it was not something the users had bargained for. Beyond that, it keeps track of the phones that have been used to summon its services even after the original owner has sold them, attempting this with Appleâ€™s phones even thought it is forbidden by the company.\\r\\n\\r\\n\\r\\nUber has also tweaked its software so that regulatory agencies that the company regarded as hostile would, when they tried to hire a driver, be given false reports about the location of its cars. Uber management booked and then cancelled rides with a rival taxi-hailing company which took their vehicles out of circulation. Uber deny this was the intention. The punishment for this behaviour was negligible. Uber promised not to use this â€œgreyballâ€ software against law enforcement â€“ one wonders what would happen to someone carrying a knife who promised never to stab a policeman with it. Travis Kalanick of Uber got a personal dressing down from Tim Cook, who runs Apple, but the company did not prohibit the use of the app. Too much money was at stake for that.\\r\\n\\r\\n\\r\\nMillions of people around the world value the cheapness and convenience of Uberâ€™s rides too much to care about the lack of driversâ€™ rights or pay. Many of the users themselves are not much richer than the drivers. The â€œsharing economyâ€ encourages the insecure and exploited to exploit others equally insecure to the profit of a tiny clique of billionaires. Silicon Valleyâ€™s culture seems hostile to humane and democratic values. The outgoing CEO of Yahoo, Marissa Mayer, who is widely judged to have been a failure, is likely to get a $186m payout. This may not be a cause for panic, any more than the previous hero worship should have been a cause for euphoria. Yet thereâ€™s an urgent political task to tame these companies, to ensure they are punished when they break the law, that they pay their taxes fairly and that they behave responsibly.'\n",
    "\"\"\"\n",
    "# Tokenize the article into sentences: sentences\n",
    "sentences = nltk.sent_tokenize(articl)\n",
    "print(articl)\n",
    "# Tokenize each sentence into words: token_sentences\n",
    "token_sentences = [nltk.word_tokenize(sent) for sent in sentences]\n",
    "print(token_sentences)\n",
    "\n",
    "# Tag each tokenized sentence into parts of speech: pos_sentences\n",
    "pos_sentences = [nltk.pos_tag(sent) for sent in token_sentences] \n",
    "\n",
    "# Create the named entity chunks: chunked_sentences\n",
    "chunked_sentences = nltk.ne_chunk_sents(pos_sentences, binary=True)\n",
    "\n",
    "# Test for stems of the tree with 'NE' tags\n",
    "for sent in chunked_sentences:\n",
    "    for chunk in sent:\n",
    "        if hasattr(chunk, \"label\") and chunk.label() == \"NE\":\n",
    "            print(chunk)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# below CHARTING IS NOT CORRECT....DO LOOK INTO IT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "articl=\"\"\"'\\ufeffThe taxi-hailing company Uber brings into very sharp focus the question of whether corporations can be said to have a moral character. If any human being were to behave with the single-minded and ruthless greed of the company, we would consider them sociopathic. Uber wanted to know as much as possible about the people who use its service, and those who donâ€™t. It has an arrangement with unroll.me, a company which offered a free service for unsubscribing from junk mail, to buy the contacts unroll.me customers had had with rival taxi companies. Even if their email was notionally anonymised, this use of it was not something the users had bargained for. Beyond that, it keeps track of the phones that have been used to summon its services even after the original owner has sold them, attempting this with Appleâ€™s phones even thought it is forbidden by the company.\\r\\n\\r\\n\\r\\nUber has also tweaked its software so that regulatory agencies that the company regarded as hostile would, when they tried to hire a driver, be given false reports about the location of its cars. Uber management booked and then cancelled rides with a rival taxi-hailing company which took their vehicles out of circulation. Uber deny this was the intention. The punishment for this behaviour was negligible. Uber promised not to use this â€œgreyballâ€ software against law enforcement â€“ one wonders what would happen to someone carrying a knife who promised never to stab a policeman with it. Travis Kalanick of Uber got a personal dressing down from Tim Cook, who runs Apple, but the company did not prohibit the use of the app. Too much money was at stake for that.\\r\\n\\r\\n\\r\\nMillions of people around the world value the cheapness and convenience of Uberâ€™s rides too much to care about the lack of driversâ€™ rights or pay. Many of the users themselves are not much richer than the drivers. The â€œsharing economyâ€ encourages the insecure and exploited to exploit others equally insecure to the profit of a tiny clique of billionaires. Silicon Valleyâ€™s culture seems hostile to humane and democratic values. The outgoing CEO of Yahoo, Marissa Mayer, who is widely judged to have been a failure, is likely to get a $186m payout. This may not be a cause for panic, any more than the previous hero worship should have been a cause for euphoria. Yet thereâ€™s an urgent political task to tame these companies, to ensure they are punished when they break the law, that they pay their taxes fairly and that they behave responsibly.'\n",
    "\"\"\"\n",
    "# Tokenize the article into sentences: sentences\n",
    "sentences = nltk.sent_tokenize(articl)\n",
    "\n",
    "# Tokenize each sentence into words: token_sentences\n",
    "token_sentences = [nltk.word_tokenize(sent) for sent in sentences]\n",
    "\n",
    "# Tag each tokenized sentence into parts of speech: pos_sentences\n",
    "pos_sentences = [nltk.pos_tag(sent) for sent in token_sentences] \n",
    "\n",
    "# Create the named entity chunks: chunked_sentences\n",
    "chunked_sentences = nltk.ne_chunk_sents(pos_sentences, binary=True)\n",
    "\n",
    "import collections\n",
    "# Create the defaultdict: ner_categories\n",
    "ner_categories = collections.defaultdict(int)\n",
    "\n",
    "# Test for stems of the tree with 'NE' tags\n",
    "for sent in chunked_sentences:\n",
    "    for chunk in sent:\n",
    "        if hasattr(chunk, 'label'):\n",
    "            ner_categories[chunk.label()] += 1\n",
    "            \n",
    "# Create a list from the dictionary keys for the chart labels: labels\n",
    "labels = list(ner_categories.keys())\n",
    "\n",
    "# Create a list of the values: values\n",
    "values = [ner_categories.get(v) for v in labels]\n",
    "\n",
    "# Create the pie chart\n",
    "plt.pie(values, labels=labels, autopct='%1.1f%%', startangle=140)\n",
    "\n",
    "# Display the chart\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PLOYGLOT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "polyglot? for ner and many more\n",
    "\n",
    "Polyglot is yet another natural language processing library which uses ................WORD VECTORS.............. to perform simple tasks such as entity recognition. You might be wondering: why do I need to learn another library which uses word vectors? Don't I already have Gensim and Spacy? And you would be correct. The main benefit and difference of using Polyglot, however, is the wide variety of languages it supports. Polyglot has word embeddings for more than 130 languages!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polyglot\n",
    "from polyglot.text import Text, Word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-3795737b065a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mpolyglot\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdownloader\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdownloader\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdownloader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msupported_languages_table\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"ner2\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\polyglot\\downloader.py\u001b[0m in \u001b[0;36msupported_languages_table\u001b[1;34m(self, task, cols)\u001b[0m\n\u001b[0;32m    976\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    977\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0msupported_languages_table\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtask\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcols\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 978\u001b[1;33m     \u001b[0mlanguages\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msupported_languages\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtask\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    979\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mpretty_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlanguages\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    980\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\polyglot\\downloader.py\u001b[0m in \u001b[0;36msupported_languages\u001b[1;34m(self, task)\u001b[0m\n\u001b[0;32m    968\u001b[0m     \"\"\"\n\u001b[0;32m    969\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mtask\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 970\u001b[1;33m       \u001b[0mcollection\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_collection\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtask\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    971\u001b[0m       return [isoLangs[x.id.split('.')[1]][\"name\"]\n\u001b[0;32m    972\u001b[0m                                          for x in collection.packages]\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\polyglot\\downloader.py\u001b[0m in \u001b[0;36mget_collection\u001b[1;34m(self, lang, task)\u001b[0m\n\u001b[0;32m    944\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"You should pass either the task or the lang\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    945\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 946\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    947\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    948\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mlang\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;32mraise\u001b[0m \u001b[0mLanguageNotSupported\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Language {} is not supported\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\polyglot\\downloader.py\u001b[0m in \u001b[0;36minfo\u001b[1;34m(self, id)\u001b[0m\n\u001b[0;32m    927\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mid\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_packages\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_packages\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mid\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    928\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mid\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_collections\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_collections\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mid\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 929\u001b[1;33m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_update_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# If package is not found, most probably we did not\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    930\u001b[0m                          \u001b[1;31m# warm up the cache\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    931\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mid\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_packages\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_packages\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mid\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\polyglot\\downloader.py\u001b[0m in \u001b[0;36m_update_index\u001b[1;34m(self, url)\u001b[0m\n\u001b[0;32m    841\u001b[0m     \u001b[0mpackages\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    842\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mobjs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 843\u001b[1;33m       \u001b[0mP\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPackage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfromcsobj\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    844\u001b[0m       \u001b[0mpackages\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mP\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    845\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_packages\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpackages\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\polyglot\\downloader.py\u001b[0m in \u001b[0;36mfromcsobj\u001b[1;34m(csobj)\u001b[0m\n\u001b[0;32m    214\u001b[0m     \u001b[0mfilename\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mattrs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"name\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    215\u001b[0m     \u001b[0mtask\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msubdir\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 216\u001b[1;33m     \u001b[0mlanguage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msubdir\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    217\u001b[0m     \u001b[0mattrs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mattrs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    218\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mPackage\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mlocals\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "from polyglot.downloader import downloader\n",
    "print(downloader.supported_languages_table(\"ner2\", 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-e8998db66c26>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mblob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"\"\"The Israeli Prime Minister Benjamin Netanyahu has warned that Iran poses a \"threat to the entire world\".\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mtext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mText\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mblob\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mtext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mentities\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\polyglot\\decorators.py\u001b[0m in \u001b[0;36m__get__\u001b[1;34m(self, obj, cls)\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mobj\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m     \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__dict__\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\polyglot\\text.py\u001b[0m in \u001b[0;36mentities\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    130\u001b[0m     \u001b[0mprev_tag\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34mu'O'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    131\u001b[0m     \u001b[0mchunks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 132\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtag\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mne_chunker\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mannotate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    133\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mtag\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mprev_tag\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    134\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mprev_tag\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34mu'O'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\polyglot\\decorators.py\u001b[0m in \u001b[0;36m__get__\u001b[1;34m(self, obj, cls)\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mobj\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m     \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__dict__\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\polyglot\\text.py\u001b[0m in \u001b[0;36mne_chunker\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     98\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mcached_property\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     99\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mne_chunker\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 100\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mget_ner_tagger\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlang\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    101\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    102\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mcached_property\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\polyglot\\decorators.py\u001b[0m in \u001b[0;36mmemoizer\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     28\u001b[0m     \u001b[0mkey\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0msorted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mkey\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcache\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m       \u001b[0mcache\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mcache\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0mmemoizer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\polyglot\\tag\\base.py\u001b[0m in \u001b[0;36mget_ner_tagger\u001b[1;34m(lang)\u001b[0m\n\u001b[0;32m    189\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mget_ner_tagger\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlang\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'en'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    190\u001b[0m   \u001b[1;34m\"\"\"Return a NER tagger from the models cache.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 191\u001b[1;33m   \u001b[1;32mreturn\u001b[0m \u001b[0mNEChunker\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlang\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlang\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\polyglot\\tag\\base.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, lang)\u001b[0m\n\u001b[0;32m    102\u001b[0m       \u001b[0mlang\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mlanguage\u001b[0m \u001b[0mcode\u001b[0m \u001b[0mto\u001b[0m \u001b[0mdecide\u001b[0m \u001b[0mwhich\u001b[0m \u001b[0mchunker\u001b[0m \u001b[0mto\u001b[0m \u001b[0muse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    103\u001b[0m     \"\"\"\n\u001b[1;32m--> 104\u001b[1;33m     \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mNEChunker\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlang\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlang\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    105\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mID_TAG\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mNER_ID_TAG\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    106\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\polyglot\\tag\\base.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, lang)\u001b[0m\n\u001b[0;32m     38\u001b[0m     \"\"\"\n\u001b[0;32m     39\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlang\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlang\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 40\u001b[1;33m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredictor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_load_network\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     41\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mID_TAG\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_bias\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\polyglot\\tag\\base.py\u001b[0m in \u001b[0;36m_load_network\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    107\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_load_network\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    108\u001b[0m     \u001b[1;34m\"\"\" Building the predictor out of the model.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 109\u001b[1;33m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0membeddings\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_embeddings\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlang\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'cw'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnormalize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    110\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_ner_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlang\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlang\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mversion\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    111\u001b[0m     \u001b[0mfirst_layer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msecond_layer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\polyglot\\decorators.py\u001b[0m in \u001b[0;36mmemoizer\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     28\u001b[0m     \u001b[0mkey\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0msorted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mkey\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcache\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m       \u001b[0mcache\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mcache\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0mmemoizer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\polyglot\\load.py\u001b[0m in \u001b[0;36mload_embeddings\u001b[1;34m(lang, task, type, normalize)\u001b[0m\n\u001b[0;32m     59\u001b[0m   \"\"\"\n\u001b[0;32m     60\u001b[0m   \u001b[0msrc_dir\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"_\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtask\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mtype\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mtask\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m   \u001b[0mp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlocate_resource\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc_dir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlang\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     62\u001b[0m   \u001b[0me\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mEmbedding\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mtype\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"cw\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\polyglot\\load.py\u001b[0m in \u001b[0;36mlocate_resource\u001b[1;34m(name, lang, filter)\u001b[0m\n\u001b[0;32m     41\u001b[0m   \u001b[0mp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpolyglot_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtask_dir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlang\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 43\u001b[1;33m     \u001b[1;32mif\u001b[0m \u001b[0mdownloader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpackage_id\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mdownloader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mINSTALLED\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     44\u001b[0m       raise ValueError(\"This resource is available in the index \"\n\u001b[0;32m     45\u001b[0m                        \u001b[1;34m\"but not downloaded, yet. Try to run\\n\\n\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\polyglot\\downloader.py\u001b[0m in \u001b[0;36mstatus\u001b[1;34m(self, info_or_id, download_dir)\u001b[0m\n\u001b[0;32m    735\u001b[0m     \"\"\"\n\u001b[0;32m    736\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mdownload_dir\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mdownload_dir\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_download_dir\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 737\u001b[1;33m     \u001b[0minfo\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_info_or_id\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minfo_or_id\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    738\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    739\u001b[0m     \u001b[1;31m# Handle collections:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\polyglot\\downloader.py\u001b[0m in \u001b[0;36m_info_or_id\u001b[1;34m(self, info_or_id)\u001b[0m\n\u001b[0;32m    505\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_info_or_id\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minfo_or_id\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    506\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minfo_or_id\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0municode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 507\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minfo_or_id\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    508\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    509\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0minfo_or_id\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\polyglot\\downloader.py\u001b[0m in \u001b[0;36minfo\u001b[1;34m(self, id)\u001b[0m\n\u001b[0;32m    927\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mid\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_packages\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_packages\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mid\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    928\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mid\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_collections\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_collections\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mid\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 929\u001b[1;33m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_update_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# If package is not found, most probably we did not\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    930\u001b[0m                          \u001b[1;31m# warm up the cache\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    931\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mid\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_packages\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_packages\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mid\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\polyglot\\downloader.py\u001b[0m in \u001b[0;36m_update_index\u001b[1;34m(self, url)\u001b[0m\n\u001b[0;32m    841\u001b[0m     \u001b[0mpackages\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    842\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mobjs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 843\u001b[1;33m       \u001b[0mP\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPackage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfromcsobj\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    844\u001b[0m       \u001b[0mpackages\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mP\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    845\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_packages\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpackages\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\polyglot\\downloader.py\u001b[0m in \u001b[0;36mfromcsobj\u001b[1;34m(csobj)\u001b[0m\n\u001b[0;32m    214\u001b[0m     \u001b[0mfilename\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mattrs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"name\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    215\u001b[0m     \u001b[0mtask\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msubdir\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 216\u001b[1;33m     \u001b[0mlanguage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msubdir\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    217\u001b[0m     \u001b[0mattrs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mattrs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    218\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mPackage\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mlocals\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "blob = \"\"\"The Israeli Prime Minister Benjamin Netanyahu has warned that Iran poses a \"threat to the entire world\".\"\"\"\n",
    "text = Text(blob)\n",
    "text.entities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import regex as re\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "holy_grail = \"\"\"ZOOT: Oh, you must see the doctors immediately!  No, no, please!  Lie down. [clap clap] \n",
    "PIGLET: Well, what seems to be the trouble?\n",
    "GALAHAD: They're doctors?!\n",
    "ZOOT: Uh, they have a basic medical training, yes.\n",
    "GALAHAD: B-- but--\n",
    "ZOOT: Oh, come, come.  You must try to rest.  Doctor Piglet!  Doctor Winston!  Practice your art.\n",
    "WINSTON: Try to relax.\n",
    "GALAHAD: Are you sure that's absolutely necessary?\n",
    "PIGLET: We must examine you.\n",
    "GALAHAD: There's nothing wrong with that!\n",
    "PIGLET: Please.  We are doctors.\n",
    "GALAHAD: Look!  This cannot be.  I am sworn to chastity.\n",
    "PIGLET: Back to your bed!  At once!\n",
    "GALAHAD: Torment me no longer.  I have seen the Grail!\n",
    "PIGLET: There's no grail here.\n",
    "GALAHAD: I have seen it!  I have seen it! [clank] I have seen--\n",
    "GIRLS: Hello.\n",
    "GALAHAD: Oh.\n",
    "GIRLS: Hello.  Hello.  Hello.  Hello.  Hello.  Hello.  Hello.  Hello.  Hello.  Hello.  Hello.  Hello.  Hello.  Hello.  Hello.  Hello.  Hello.  Hello.  Hello.  Hello.  Hello.  Hello.  Hello.\n",
    "GALAHAD: Zoot!\n",
    "DINGO: No, I am Zoot's identical twin sister, Dingo.\n",
    "GALAHAD: Oh, well, excuse me, I--\n",
    "DINGO: Where are you going?\n",
    "GALAHAD: I seek the Grail!  I have seen it, here in this castle!\n",
    "DINGO: Oh no.  Oh, no!  Bad, bad Zoot!\n",
    "GALAHAD: Well, what is it?\n",
    "DINGO: Oh, wicked, bad, naughty Zoot!  She has been setting alight to our beacon, which, I have just remembered, is grail-shaped.  It's not the first time we've had this problem.\n",
    "GALAHAD: It's not the real Grail?\n",
    "DINGO: Oh, wicked, bad, naughty, evil Zoot!  She is a bad person and must pay the penalty.  Do you think this scene should have been cut?  We were so worried when the boys were writing it, but now, we're glad.  It's better than some of the previous scenes, I think.\n",
    "LEFT HEAD: At least ours was better visually.\n",
    "DENNIS: Well, at least ours was committed.  It wasn't just a string of pussy jokes.\n",
    "OLD MAN: Get on with it.\n",
    "TIM THE ENCHANTER: Yes, get on with it!\n",
    "ARMY OF KNIGHTS: Yes, get on with it!\n",
    "DINGO: Oh, I am enjoying this scene.\n",
    "GOD: Get on with it!\n",
    "DINGO: [sigh] Oh, wicked, wicked Zoot.  Oh, she is a naughty person, and she must pay the penalty.  And here in Castle Anthrax, we have but one punishment for setting alight the grail-shaped beacon.  You must tie her down on a bed and spank her.\n",
    "GIRLS: A spanking!  A spanking!\n",
    "DINGO: You must spank her well.  And after you have spanked her, you may deal with her as you like.  And then, spank me.\n",
    "AMAZING: And spank me.\n",
    "STUNNER: And me.\n",
    "LOVELY: And me.\n",
    "DINGO: Yes, yes, you must give us all a good spanking!\n",
    "GIRLS: A spanking!  A spanking!  There is going to be a spanking tonight!\n",
    "DINGO: And after the spanking, the oral sex.\n",
    "GIRLS: The oral sex!  The oral sex!\n",
    "GALAHAD: Well, I could stay a bit longer.\n",
    "LAUNCELOT: Sir Galahad!\n",
    "GALAHAD: Oh, hello.\n",
    "LAUNCELOT: Quick!\n",
    "GALAHAD: What?\n",
    "LAUNCELOT: Quick!\n",
    "GALAHAD: Why?\n",
    "LAUNCELOT: You are in great peril!\n",
    "DINGO: No he isn't.\n",
    "LAUNCELOT: Silence, foul temptress!\n",
    "GALAHAD: You know, she's got a point.\n",
    "LAUNCELOT: Come on!  We will cover your escape!\n",
    "GALAHAD: Look, I'm fine!\n",
    "LAUNCELOT: Come on!\n",
    "GIRLS: Sir Galahad!\n",
    "GALAHAD: No.  Look, I can tackle this lot single-handed!\n",
    "DINGO: Yes!  Let him tackle us single-handed!\n",
    "GIRLS: Yes!  Let him tackle us single-handed!\n",
    "LAUNCELOT: No, Sir Galahad.  Come on!\n",
    "GALAHAD: No!  Really!  Honestly, I can cope.  I can handle this lot easily.\n",
    "DINGO: Oh, yes.  Let him handle us easily.\n",
    "GIRLS: Yes.  Let him handle us easily.\n",
    "LAUNCELOT: No.  Quick!  Quick!\n",
    "GALAHAD: Please!  I can defeat them!  There's only a hundred-and-fifty of them!\n",
    "DINGO: Yes, yes!  He will beat us easily!  We haven't a chance.\n",
    "GIRLS: We haven't a chance.  He will beat us easily... [boom] \n",
    "DINGO: Oh, shit.\n",
    "LAUNCELOT: We were in the nick of time.  You were in great peril.\n",
    "GALAHAD: I don't think I was.\n",
    "LAUNCELOT: Yes you were.  You were in terrible peril.\n",
    "GALAHAD: Look, let me go back in there and face the peril.\n",
    "LAUNCELOT: No, it's too perilous.\n",
    "GALAHAD: Look, it's my duty as a knight to sample as much peril as I can.\n",
    "LAUNCELOT: No, we've got to find the Holy Grail.  Come on!\n",
    "GALAHAD: Oh, let me have just a little bit of peril?\n",
    "LAUNCELOT: No.  It's unhealthy.\n",
    "GALAHAD: I bet you're gay.\n",
    "LAUNCELOT: No I'm not\n",
    "NARRATOR: Sir Launcelot had saved Sir Galahad from almost certain temptation, but they were still no nearer the Grail.  Meanwhile, King Arthur and Sir Bedevere, not more than a swallow's flight away, had discovered something.  Oh, that's an unladen swallow's flight, obviously.  I mean, they were more than two laden swallows' flights away-- four, really, if they had a coconut on a line between them.  I mean, if the birds were walking and dragging--\n",
    "CROWD: Get on with it!\n",
    "NARRATOR: Oh, anyway.  On to scene twenty-four, which is a smashing scene with some lovely acting, in which Arthur discovers a vital clue, and in which there aren't any swallows, although I think you can hear a starling-- oooh\n",
    "SCENE 12:\n",
    "OLD MAN: Heh, hee ha ha hee hee!  Hee hee hee ha ha ha...\n",
    "ARTHUR: And this enchanter of whom you speak, he has seen the Grail?\n",
    "OLD MAN: ... Ha ha ha ha!  Heh, hee ha ha hee!  Ha hee ha!  Ha ha ha ha...\n",
    "ARTHUR: Where does he live?\n",
    "OLD MAN: ... Heh heh heh heh...\n",
    "ARTHUR: Old man, where does he live?\n",
    "OLD MAN: ... Hee ha ha ha.  He knows of a cave, a cave which no man has entered.\n",
    "ARTHUR: And the Grail.  The Grail is there?\n",
    "OLD MAN: There is much danger, for beyond the cave lies the Gorge of Eternal Peril, which no man has ever crossed.\n",
    "ARTHUR: But the Grail!  Where is the Grail?!\n",
    "OLD MAN: Seek you the Bridge of Death.\n",
    "ARTHUR: The Bridge of Death, which leads to the Grail?\n",
    "OLD MAN: Heh, hee hee hee hee!  Ha ha ha ha ha!  Hee ha ha..\n",
    "SCENE 13: [spooky music] [music stops] \n",
    "HEAD KNIGHT OF NI: Ni!\n",
    "KNIGHTS OF NI: Ni!  Ni!  Ni!  Ni!  Ni!\n",
    "ARTHUR: Who are you?\n",
    "HEAD KNIGHT: We are the Knights Who Say...  'Ni'!\n",
    "RANDOM: Ni!\n",
    "ARTHUR: No!  Not the Knights Who Say 'Ni'!\n",
    "HEAD KNIGHT: The same!\n",
    "BEDEVERE: Who are they?\n",
    "HEAD KNIGHT: We are the keepers of the sacred words: Ni, Peng, and Neee-wom!\n",
    "RANDOM: Neee-wom!\n",
    "ARTHUR: Those who hear them seldom live to tell the tale!\n",
    "HEAD KNIGHT: The Knights Who Say 'Ni' demand a sacrifice!\n",
    "ARTHUR: Knights of Ni, we are but simple travellers who seek the enchanter who lives beyond these woods.\n",
    "HEAD KNIGHT: Ni!\n",
    "KNIGHTS OF NI: Ni!  Ni!  Ni!  Ni!  Ni! ...\n",
    "ARTHUR: Ow!  Ow!  Ow!  Agh!\n",
    "HEAD KNIGHT: We shall say 'ni' again to you if you do not appease us.\n",
    "ARTHUR: Well, what is it you want?\n",
    "HEAD KNIGHT: We want...  a shrubbery! [dramatic chord] \n",
    "ARTHUR: A what?\n",
    "KNIGHTS OF NI: Ni!  Ni!  Ni!  Ni!\n",
    "ARTHUR and PARTY: Ow!  Oh!\n",
    "ARTHUR: Please, please!  No more!  We will find you a shrubbery.\n",
    "HEAD KNIGHT: You must return here with a shrubbery or else you will never pass through this wood alive!\n",
    "ARTHUR: O Knights of Ni, you are just and fair, and we will return with a shrubbery.\n",
    "HEAD KNIGHT: One that looks nice.\n",
    "ARTHUR: Of course.\n",
    "HEAD KNIGHT: And not too expensive.\n",
    "ARTHUR: Yes.\n",
    "HEAD KNIGHT: Now...  go [trumpets] \"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['immediately!',\n",
       " 'please!',\n",
       " '!',\n",
       " 'Piglet!',\n",
       " 'Winston!',\n",
       " 'that!',\n",
       " 'Look!',\n",
       " 'bed!',\n",
       " 'once!',\n",
       " 'Grail!',\n",
       " 'it!',\n",
       " 'it!',\n",
       " 'Zoot!',\n",
       " 'Grail!',\n",
       " 'castle!',\n",
       " 'no!',\n",
       " 'Zoot!',\n",
       " 'Zoot!',\n",
       " 'Zoot!',\n",
       " 'it!',\n",
       " 'it!',\n",
       " 'it!',\n",
       " 'spanking!',\n",
       " 'spanking!',\n",
       " 'spanking!',\n",
       " 'spanking!',\n",
       " 'spanking!',\n",
       " 'tonight!',\n",
       " 'sex!',\n",
       " 'sex!',\n",
       " 'Galahad!',\n",
       " 'Quick!',\n",
       " 'Quick!',\n",
       " 'peril!',\n",
       " 'temptress!',\n",
       " 'on!',\n",
       " 'escape!',\n",
       " 'fine!',\n",
       " 'on!',\n",
       " 'Galahad!',\n",
       " 'handed!',\n",
       " 'Yes!',\n",
       " 'handed!',\n",
       " 'Yes!',\n",
       " 'handed!',\n",
       " 'on!',\n",
       " 'No!',\n",
       " 'Really!',\n",
       " 'Quick!',\n",
       " 'Quick!',\n",
       " 'Please!',\n",
       " 'them!',\n",
       " 'them!',\n",
       " 'yes!',\n",
       " 'easily!',\n",
       " 'on!',\n",
       " 'it!',\n",
       " 'hee!',\n",
       " 'ha!',\n",
       " 'hee!',\n",
       " 'ha!',\n",
       " 'Grail!',\n",
       " '!',\n",
       " 'hee!',\n",
       " 'ha!',\n",
       " 'Ni!',\n",
       " 'Ni!',\n",
       " 'Ni!',\n",
       " 'Ni!',\n",
       " 'Ni!',\n",
       " 'Ni!',\n",
       " '!',\n",
       " 'Ni!',\n",
       " 'No!',\n",
       " '!',\n",
       " 'same!',\n",
       " 'wom!',\n",
       " 'wom!',\n",
       " 'tale!',\n",
       " 'sacrifice!',\n",
       " 'Ni!',\n",
       " 'Ni!',\n",
       " 'Ni!',\n",
       " 'Ni!',\n",
       " 'Ni!',\n",
       " 'Ni!',\n",
       " 'Ow!',\n",
       " 'Ow!',\n",
       " 'Ow!',\n",
       " 'Agh!',\n",
       " 'shrubbery!',\n",
       " 'Ni!',\n",
       " 'Ni!',\n",
       " 'Ni!',\n",
       " 'Ni!',\n",
       " 'Ow!',\n",
       " 'Oh!',\n",
       " 'please!',\n",
       " 'more!',\n",
       " 'alive!']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "res = re.findall(r'\\w*!', holy_grail)\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ZOOT:',\n",
       " 'PIGLET:',\n",
       " 'GALAHAD:',\n",
       " 'ZOOT:',\n",
       " 'GALAHAD:',\n",
       " 'ZOOT:',\n",
       " 'WINSTON:',\n",
       " 'GALAHAD:',\n",
       " 'PIGLET:',\n",
       " 'GALAHAD:',\n",
       " 'PIGLET:',\n",
       " 'GALAHAD:',\n",
       " 'PIGLET:',\n",
       " 'GALAHAD:',\n",
       " 'PIGLET:',\n",
       " 'GALAHAD:',\n",
       " 'GIRLS:',\n",
       " 'GALAHAD:',\n",
       " 'GIRLS:',\n",
       " 'GALAHAD:',\n",
       " 'DINGO:',\n",
       " 'GALAHAD:',\n",
       " 'DINGO:',\n",
       " 'GALAHAD:',\n",
       " 'DINGO:',\n",
       " 'GALAHAD:',\n",
       " 'DINGO:',\n",
       " 'GALAHAD:',\n",
       " 'DINGO:',\n",
       " 'HEAD:',\n",
       " 'DENNIS:',\n",
       " 'MAN:',\n",
       " 'ENCHANTER:',\n",
       " 'KNIGHTS:',\n",
       " 'DINGO:',\n",
       " 'GOD:',\n",
       " 'DINGO:',\n",
       " 'GIRLS:',\n",
       " 'DINGO:',\n",
       " 'AMAZING:',\n",
       " 'STUNNER:',\n",
       " 'LOVELY:',\n",
       " 'DINGO:',\n",
       " 'GIRLS:',\n",
       " 'DINGO:',\n",
       " 'GIRLS:',\n",
       " 'GALAHAD:',\n",
       " 'LAUNCELOT:',\n",
       " 'GALAHAD:',\n",
       " 'LAUNCELOT:',\n",
       " 'GALAHAD:',\n",
       " 'LAUNCELOT:',\n",
       " 'GALAHAD:',\n",
       " 'LAUNCELOT:',\n",
       " 'DINGO:',\n",
       " 'LAUNCELOT:',\n",
       " 'GALAHAD:',\n",
       " 'LAUNCELOT:',\n",
       " 'GALAHAD:',\n",
       " 'LAUNCELOT:',\n",
       " 'GIRLS:',\n",
       " 'GALAHAD:',\n",
       " 'DINGO:',\n",
       " 'GIRLS:',\n",
       " 'LAUNCELOT:',\n",
       " 'GALAHAD:',\n",
       " 'DINGO:',\n",
       " 'GIRLS:',\n",
       " 'LAUNCELOT:',\n",
       " 'GALAHAD:',\n",
       " 'DINGO:',\n",
       " 'GIRLS:',\n",
       " 'DINGO:',\n",
       " 'LAUNCELOT:',\n",
       " 'GALAHAD:',\n",
       " 'LAUNCELOT:',\n",
       " 'GALAHAD:',\n",
       " 'LAUNCELOT:',\n",
       " 'GALAHAD:',\n",
       " 'LAUNCELOT:',\n",
       " 'GALAHAD:',\n",
       " 'LAUNCELOT:',\n",
       " 'GALAHAD:',\n",
       " 'LAUNCELOT:',\n",
       " 'NARRATOR:',\n",
       " 'CROWD:',\n",
       " 'NARRATOR:',\n",
       " '12:',\n",
       " 'MAN:',\n",
       " 'ARTHUR:',\n",
       " 'MAN:',\n",
       " 'ARTHUR:',\n",
       " 'MAN:',\n",
       " 'ARTHUR:',\n",
       " 'MAN:',\n",
       " 'ARTHUR:',\n",
       " 'MAN:',\n",
       " 'ARTHUR:',\n",
       " 'MAN:',\n",
       " 'ARTHUR:',\n",
       " 'MAN:',\n",
       " '13:',\n",
       " 'NI:',\n",
       " 'NI:',\n",
       " 'ARTHUR:',\n",
       " 'KNIGHT:',\n",
       " 'RANDOM:',\n",
       " 'ARTHUR:',\n",
       " 'KNIGHT:',\n",
       " 'BEDEVERE:',\n",
       " 'KNIGHT:',\n",
       " 'words:',\n",
       " 'RANDOM:',\n",
       " 'ARTHUR:',\n",
       " 'KNIGHT:',\n",
       " 'ARTHUR:',\n",
       " 'KNIGHT:',\n",
       " 'NI:',\n",
       " 'ARTHUR:',\n",
       " 'KNIGHT:',\n",
       " 'ARTHUR:',\n",
       " 'KNIGHT:',\n",
       " 'ARTHUR:',\n",
       " 'NI:',\n",
       " 'PARTY:',\n",
       " 'ARTHUR:',\n",
       " 'KNIGHT:',\n",
       " 'ARTHUR:',\n",
       " 'KNIGHT:',\n",
       " 'ARTHUR:',\n",
       " 'KNIGHT:',\n",
       " 'ARTHUR:',\n",
       " 'KNIGHT:']"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "      \n",
    "# using regex( findall() )\n",
    "# to extract words from string\n",
    "res = re.findall(r'\\w*:', holy_grail)\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['12:' '13:' 'AMAZING:' 'ARTHUR:' 'BEDEVERE:' 'CROWD:' 'DENNIS:' 'DINGO:'\n",
      " 'ENCHANTER:' 'GALAHAD:' 'GIRLS:' 'GOD:' 'HEAD:' 'KNIGHT:' 'KNIGHTS:'\n",
      " 'LAUNCELOT:' 'LOVELY:' 'MAN:' 'NARRATOR:' 'NI:' 'PARTY:' 'PIGLET:'\n",
      " 'RANDOM:' 'STUNNER:' 'WINSTON:' 'ZOOT:' 'words:']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GALAHAD:      27\n",
       "ARTHUR:       17\n",
       "LAUNCELOT:    15\n",
       "DINGO:        15\n",
       "KNIGHT:       11\n",
       "GIRLS:         9\n",
       "MAN:           8\n",
       "PIGLET:        5\n",
       "NI:            4\n",
       "ZOOT:          3\n",
       "RANDOM:        2\n",
       "NARRATOR:      2\n",
       "CROWD:         1\n",
       "PARTY:         1\n",
       "ENCHANTER:     1\n",
       "12:            1\n",
       "AMAZING:       1\n",
       "words:         1\n",
       "BEDEVERE:      1\n",
       "STUNNER:       1\n",
       "HEAD:          1\n",
       "KNIGHTS:       1\n",
       "WINSTON:       1\n",
       "LOVELY:        1\n",
       "13:            1\n",
       "GOD:           1\n",
       "DENNIS:        1\n",
       "dtype: int64"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    " \n",
    "# function to get unique values\n",
    "\n",
    "x = np.array(res)\n",
    "print(np.unique(x))\n",
    "\n",
    "pd.value_counts(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
