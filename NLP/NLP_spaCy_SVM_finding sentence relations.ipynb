{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "spaCy is a free, open-source library for NLP in Python. It's written in Cython and is designed to build information extraction or natural language understanding systems. It's built for production use and provides a concise and user-friendly API."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A core difference between NLTK and spaCy stems from the way in which these libraries were built. NLTK is essentially a string processing library, where each function takes strings as input and returns a processed string. Though this seems like a simple way to use the library, in practice, you’ll often find yourself going back to the documentation to discover new functions.\n",
    "\n",
    "In contrast, spaCy takes an object-oriented approach. Each function returns objects instead of strings or arrays. This allows for easy exploration of the tool. Developers don’t need to constantly check with documentation to understand context because the object itself provides it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-lg==3.0.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-3.0.0/en_core_web_lg-3.0.0-py3-none-any.whl (778.8 MB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Exception:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\ANTHONY\\anaconda3\\lib\\site-packages\\pip\\_vendor\\urllib3\\response.py\", line 438, in _error_catcher\n",
      "    yield\n",
      "  File \"C:\\Users\\ANTHONY\\anaconda3\\lib\\site-packages\\pip\\_vendor\\urllib3\\response.py\", line 519, in read\n",
      "    data = self._fp.read(amt) if not fp_closed else b\"\"\n",
      "  File \"C:\\Users\\ANTHONY\\anaconda3\\lib\\site-packages\\pip\\_vendor\\cachecontrol\\filewrapper.py\", line 62, in read\n",
      "    data = self.__fp.read(amt)\n",
      "  File \"C:\\Users\\ANTHONY\\anaconda3\\lib\\http\\client.py\", line 458, in read\n",
      "    n = self.readinto(b)\n",
      "  File \"C:\\Users\\ANTHONY\\anaconda3\\lib\\http\\client.py\", line 502, in readinto\n",
      "    n = self.fp.readinto(b)\n",
      "  File \"C:\\Users\\ANTHONY\\anaconda3\\lib\\socket.py\", line 669, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "  File \"C:\\Users\\ANTHONY\\anaconda3\\lib\\ssl.py\", line 1241, in recv_into\n",
      "    return self.read(nbytes, buffer)\n",
      "  File \"C:\\Users\\ANTHONY\\anaconda3\\lib\\ssl.py\", line 1099, in read\n",
      "    return self._sslobj.read(len, buffer)\n",
      "socket.timeout: The read operation timed out\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\ANTHONY\\anaconda3\\lib\\site-packages\\pip\\_internal\\cli\\base_command.py\", line 189, in _main\n",
      "    status = self.run(options, args)\n",
      "  File \"C:\\Users\\ANTHONY\\anaconda3\\lib\\site-packages\\pip\\_internal\\cli\\req_command.py\", line 178, in wrapper\n",
      "    return func(self, options, args)\n",
      "  File \"C:\\Users\\ANTHONY\\anaconda3\\lib\\site-packages\\pip\\_internal\\commands\\install.py\", line 316, in run\n",
      "    requirement_set = resolver.resolve(\n",
      "  File \"C:\\Users\\ANTHONY\\anaconda3\\lib\\site-packages\\pip\\_internal\\resolution\\resolvelib\\resolver.py\", line 100, in resolve\n",
      "    r = self.factory.make_requirement_from_install_req(\n",
      "  File \"C:\\Users\\ANTHONY\\anaconda3\\lib\\site-packages\\pip\\_internal\\resolution\\resolvelib\\factory.py\", line 301, in make_requirement_from_install_req\n",
      "    cand = self._make_candidate_from_link(\n",
      "  File \"C:\\Users\\ANTHONY\\anaconda3\\lib\\site-packages\\pip\\_internal\\resolution\\resolvelib\\factory.py\", line 167, in _make_candidate_from_link\n",
      "    self._link_candidate_cache[link] = LinkCandidate(\n",
      "  File \"C:\\Users\\ANTHONY\\anaconda3\\lib\\site-packages\\pip\\_internal\\resolution\\resolvelib\\candidates.py\", line 300, in __init__\n",
      "    super().__init__(\n",
      "  File \"C:\\Users\\ANTHONY\\anaconda3\\lib\\site-packages\\pip\\_internal\\resolution\\resolvelib\\candidates.py\", line 144, in __init__\n",
      "    self.dist = self._prepare()\n",
      "  File \"C:\\Users\\ANTHONY\\anaconda3\\lib\\site-packages\\pip\\_internal\\resolution\\resolvelib\\candidates.py\", line 226, in _prepare\n",
      "    dist = self._prepare_distribution()\n",
      "  File \"C:\\Users\\ANTHONY\\anaconda3\\lib\\site-packages\\pip\\_internal\\resolution\\resolvelib\\candidates.py\", line 311, in _prepare_distribution\n",
      "    return self._factory.preparer.prepare_linked_requirement(\n",
      "  File \"C:\\Users\\ANTHONY\\anaconda3\\lib\\site-packages\\pip\\_internal\\operations\\prepare.py\", line 457, in prepare_linked_requirement\n",
      "    return self._prepare_linked_requirement(req, parallel_builds)\n",
      "  File \"C:\\Users\\ANTHONY\\anaconda3\\lib\\site-packages\\pip\\_internal\\operations\\prepare.py\", line 480, in _prepare_linked_requirement\n",
      "    local_file = unpack_url(\n",
      "  File \"C:\\Users\\ANTHONY\\anaconda3\\lib\\site-packages\\pip\\_internal\\operations\\prepare.py\", line 230, in unpack_url\n",
      "    file = get_http_url(\n",
      "  File \"C:\\Users\\ANTHONY\\anaconda3\\lib\\site-packages\\pip\\_internal\\operations\\prepare.py\", line 108, in get_http_url\n",
      "    from_path, content_type = download(link, temp_dir.path)\n",
      "  File \"C:\\Users\\ANTHONY\\anaconda3\\lib\\site-packages\\pip\\_internal\\network\\download.py\", line 163, in __call__\n",
      "    for chunk in chunks:\n",
      "  File \"C:\\Users\\ANTHONY\\anaconda3\\lib\\site-packages\\pip\\_internal\\cli\\progress_bars.py\", line 159, in iter\n",
      "    for x in it:\n",
      "  File \"C:\\Users\\ANTHONY\\anaconda3\\lib\\site-packages\\pip\\_internal\\network\\utils.py\", line 64, in response_chunks\n",
      "    for chunk in response.raw.stream(\n",
      "  File \"C:\\Users\\ANTHONY\\anaconda3\\lib\\site-packages\\pip\\_vendor\\urllib3\\response.py\", line 576, in stream\n",
      "    data = self.read(amt=amt, decode_content=decode_content)\n",
      "  File \"C:\\Users\\ANTHONY\\anaconda3\\lib\\site-packages\\pip\\_vendor\\urllib3\\response.py\", line 541, in read\n",
      "    raise IncompleteRead(self._fp_bytes_read, self.length_remaining)\n",
      "  File \"C:\\Users\\ANTHONY\\anaconda3\\lib\\contextlib.py\", line 131, in __exit__\n",
      "    self.gen.throw(type, value, traceback)\n",
      "  File \"C:\\Users\\ANTHONY\\anaconda3\\lib\\site-packages\\pip\\_vendor\\urllib3\\response.py\", line 443, in _error_catcher\n",
      "    raise ReadTimeoutError(self._pool, None, \"Read timed out.\")\n",
      "pip._vendor.urllib3.exceptions.ReadTimeoutError: HTTPSConnectionPool(host='github-releases.githubusercontent.com', port=443): Read timed out.\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en_core_web_lg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "MyVectorizer = spacy.load('en_core_web_lg') #creates an object of spacey\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(kernel='linear')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence1 = \"I love the book\"\n",
    "sentence2 = \"This is a great book ?\"\n",
    "sentence3 = \"The fit is great\"\n",
    "sentence4 = \"I love the shoes\"\n",
    "sentence5 = \"Do you hate driving\"\n",
    "sentence6 = \"I love boxing\"\n",
    "sentence7 = \"Beggars can't be chooser\"\n",
    "sentence8 = \"Early to bed and early to rise, makes a man healthy, wealthy and wise\"\n",
    "sentence9 = \"Playing sports makes you stronger\"\n",
    "sentence10 = \"Any competitive physical activity providing a sense of enjoyment is Sport\"\n",
    "sentence11 = \"play a major part in improving our physical and mental fitness\" \n",
    "sentence12 = \" It helps in developing Self Confidence, Team Spirit, and Mental & Physical toughness.There are two types of sports, Indoor and Outdoor\"\n",
    "s13=\"Collisions between trains and vehicles standing foul of the line .\"\n",
    "s14=\"People used it to go to one place to another.\"\n",
    "s15=\"Its is a means of transport.It is a four-wheeled vehicle.It has five comfortable seats.It also has windows and AC.Its used to carry things to drop from one place to another.\"\n",
    "s16=\"She should be looking for a replacement vehicle, but having another car in the garage would only be a reminder that there was no one left to drive it.\"\n",
    "\n",
    "train_x = [sentence1,sentence2,sentence3,sentence4,sentence5,sentence6,sentence7,sentence8,sentence9,sentence10,sentence11,sentence12,s13,s14,s15,s16]\n",
    "train_y = [\"books\", \"book\", \"clothes\", \"clothes\",\"vehicle\",\"games\",\"proverb\",\"proverb\",\"games\",\"games\",\"games\",\"games\",\"vehicle\",\"vehicle\",\"vehicle\",\"vehicle\"]\n",
    "\n",
    "\n",
    "# Hey Spacy please read and understand the training data for me! can you? SPACEY understands the sentences to be convereted into numeric format\n",
    "MyTextVectors = [MyVectorizer(text) for text in train_x]\n",
    "\n",
    "# Hey Spacy please converrt your knowledge into numerical values[only then it can predict] so that I can analyze them with mathematical methods I know already\n",
    "MyNumericalVectors = [x.vector for x in MyTextVectors]\n",
    "\n",
    "\n",
    "# SVM is a classifier that learns and later decides if a text is about the clothes or books  or anything else\n",
    "from sklearn import svm\n",
    "MySVM = svm.SVC(kernel = 'linear')\n",
    "MySVM.fit(MyNumericalVectors, train_y)  #we will fit traning set into the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['books'], dtype='<U7')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_x = [\"I love the books and the stories\"]\n",
    "MyTextVectors = [MyVectorizer(text) for text in test_x]\n",
    "MyNumericalVectors = [x.vector for x in MyTextVectors]\n",
    "MySVM.predict(MyNumericalVectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['vehicle'], dtype='<U7')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_x = [\"I dont have fuel\"]\n",
    "MyTextVectors = [MyVectorizer(text) for text in test_x]\n",
    "MyNumericalVectors = [x.vector for x in MyTextVectors]\n",
    "MySVM.predict(MyNumericalVectors)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenization in spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_lg') #creates an object of spacey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'tutorial', 'is', 'about', 'Natural', 'Language', 'Processing', 'in', 'Spacy', '.']\n"
     ]
    }
   ],
   "source": [
    "introduction_text = ('This tutorial is about Natural'\n",
    " ' Language Processing in Spacy.')\n",
    "introduction_doc = nlp(introduction_text)\n",
    " # Extract tokens for the given doc\n",
    "print ([token.text for token in introduction_doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
